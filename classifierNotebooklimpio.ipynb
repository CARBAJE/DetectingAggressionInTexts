{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# Modelo de Clasificación Jerárquica con Aumento de Datos v4.0\n",
    "\n",
    "Este notebook implementa un pipeline avanzado de clasificación jerárquica con características combinadas:\n",
    "\n",
    "1.  **Carga y Preprocesamiento**: Usa `hate_speech_twitter` y realiza limpieza de texto (tokenización, stemming, etc.).\n",
    "2.  **Generación de Características Dual**: Crea embeddings de BERT y vectores TF-IDF.\n",
    "3.  **Aumento de Datos Sintéticos**: Utiliza **CTGAN** de la librería `sdv` para generar datos sintéticos y **balancear las sub-categorías** de discurso de odio en el conjunto de entrenamiento, mejorando la robustez del modelo.\n",
    "4.  **Entrenamiento de Clasificador Principal (Nivel 1)**: Entrena y optimiza un **ensemble extendido de seis modelos**. Incluye modelos que usan solo embeddings, solo TF-IDF, y una **combinación de ambos** para una detección más robusta de `odio` vs. `no-odio`.\n",
    "5.  **Entrenamiento de Clasificador de Sub-categorías (Nivel 2)**: Entrena un ensemble de tres modelos XGBoost, MLP y Regresión Logística para clasificar el **tipo de odio** (ej. sexismo, racismo), utilizando también **características combinadas de embeddings y TF-IDF**.\n",
    "6.  **Evaluación Jerárquica**: Evalúa el rendimiento del pipeline completo en dos niveles, reportando la precisión tanto en la detección de odio como en la clasificación de su tipo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_md",
   "metadata": {},
   "source": [
    "## 1. Instalación y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch datasets scikit-learn xgboost pandas seaborn matplotlib tqdm optuna nltk scipy sdv nltk ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "\n",
      "ID de trabajo: hierarchical-job-1750954743\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import xgboost as xgb\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix, vstack\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_SAMPLES = 10000 # Aumentar para un mejor entrenamiento de sub-categorías\n",
    "MAX_TOKEN_LENGTH = 128\n",
    "\n",
    "# --- Configuración de Dispositivo (GPU o CPU) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# --- Definición de Rutas Locales ---\n",
    "job_id = f\"hierarchical-job-{int(time.time())}\"\n",
    "BASE_DIR = \"./datos_locales\"\n",
    "# PROCESSED_DIR = os.path.join(BASE_DIR, \"processed\", job_id)\n",
    "MODEL_OUTPUT_DIR = os.path.join(BASE_DIR, \"model_output\", job_id)\n",
    "# os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "# PROCESSED_DATA_PATH = os.path.join(PROCESSED_DIR, \"processed_data_with_embeddings.csv\")\n",
    "print(f\"\\nID de trabajo: {job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_load_md",
   "metadata": {},
   "source": [
    "## 2. Carga, Análisis y Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_load_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cargando dataset 'thefrankhsu/hate_speech_twitter'...\")\n",
    "dataset = load_dataset(\"thefrankhsu/hate_speech_twitter\")\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Renombrar columnas y manejar nulos en 'categories'\n",
    "df = df.rename(columns={'tweet': 'text_raw', 'label': 'main_label', 'categories': 'sub_label_str'})\n",
    "df['sub_label_str'] = df['sub_label_str'].fillna('not-hate')\n",
    "\n",
    "\n",
    "if MAX_SAMPLES is not None:\n",
    "    # Asegurarse de que el tamaño de la muestra no sea mayor que la población\n",
    "    sample_size = min(MAX_SAMPLES, len(df))\n",
    "    print(f\"Tomando una muestra aleatoria de {sample_size} registros (de un total de {len(df)}).\")\n",
    "    df = df.sample(n=sample_size, random_state=42, replace=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Distribución de etiquetas principales:\")\n",
    "print(df['main_label'].value_counts())\n",
    "\n",
    "print(\"\\nDistribución de sub-etiquetas (solo para 'odio'):\")\n",
    "print(df[df['main_label'] == 1]['sub_label_str'].value_counts())\n",
    "\n",
    "# Codificar sub-etiquetas\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "sub_label_encoder = LabelEncoder()\n",
    "df['sub_label_encoded'] = sub_label_encoder.fit_transform(df['sub_label_str'])\n",
    "sub_label_mapping = dict(zip(sub_label_encoder.classes_, sub_label_encoder.transform(sub_label_encoder.classes_)))\n",
    "print(\"\\nMapeo de Sub-etiquetas:\", sub_label_mapping)\n",
    "\n",
    "# Preprocesamiento de texto\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "def clean_text(text, apply_stemming=False):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|#','', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    words = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    if apply_stemming: words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "tqdm.pandas(desc=\"Limpiando Texto para Embeddings\")\n",
    "df['text_cleaned'] = df['text_raw'].progress_apply(lambda x: clean_text(x, apply_stemming=False))\n",
    "tqdm.pandas(desc=\"Aplicando Stemming para TF-IDF\")\n",
    "df['text_stemmed'] = df['text_cleaned'].progress_apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding_md",
   "metadata": {},
   "source": [
    "## 3. Generación de Embeddings y División de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedding_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cargando modelo y tokenizador BERT: {BERT_MODEL_NAME}\")\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "model_bert = AutoModel.from_pretrained(BERT_MODEL_NAME).to(device)\n",
    "model_bert.eval()\n",
    "\n",
    "def get_bert_embeddings(batch_text):\n",
    "    inputs = tokenizer_bert(batch_text, padding=True, truncation=True, max_length=MAX_TOKEN_LENGTH, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "print(\"Generando embeddings...\")\n",
    "batch_size = 32\n",
    "all_embeddings = np.vstack([get_bert_embeddings(df.iloc[i:i+batch_size]['text_cleaned'].tolist()) for i in tqdm(range(0, len(df), batch_size))])\n",
    "\n",
    "embedding_cols = [f'dim_{i}' for i in range(all_embeddings.shape[1])]\n",
    "df_embeddings = pd.DataFrame(all_embeddings, columns=embedding_cols, index=df.index)\n",
    "df_processed = pd.concat([df, df_embeddings], axis=1)\n",
    "\n",
    "print(\"\\n--- Dividiendo Datos ---\")\n",
    "y_main = df_processed['main_label'].values\n",
    "df_trainval, df_test = train_test_split(df_processed, test_size=0.2, random_state=42, stratify=y_main)\n",
    "y_trainval_main = df_trainval['main_label'].values\n",
    "df_train, df_val = train_test_split(df_trainval, test_size=0.25, random_state=42, stratify=y_trainval_main)\n",
    "\n",
    "print(f\"Tamaño Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "augmentation_md",
   "metadata": {},
   "source": [
    "## 4. Aumento de Datos Sintéticos para Sub-categorías (CTGAN)\n",
    "Nos enfocamos en el desbalance de las sub-categorías de 'odio'. Usaremos CTGAN para generar nuevos datos de entrenamiento para las clases minoritarias, basándonos en sus embeddings. **Importante**: CTGAN solo genera embeddings sintéticos; no puede generar texto. La parte de TF-IDF para estos datos se manejará más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "augmentation_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"--- Preparando datos para aumento ---\")\n",
    "# 1. Aislar los datos de entrenamiento que son 'odio'\n",
    "df_train_hate = df_train[df_train['main_label'] == 1].copy()\n",
    "features_to_augment = ['sub_label_str'] + embedding_cols\n",
    "df_to_augment = df_train_hate[features_to_augment]\n",
    "\n",
    "print(\"Distribución de sub-categorías ANTES del aumento:\")\n",
    "hate_counts = df_to_augment['sub_label_str'].value_counts()\n",
    "print(hate_counts)\n",
    "\n",
    "# Crear un nuevo encoder dedicado SOLO para las sub-categorías de odio.\n",
    "sub_hate_only_encoder = LabelEncoder()\n",
    "df_synthetic = pd.DataFrame() # Inicializar como dataframe vacío\n",
    "\n",
    "if len(hate_counts) > 1 and not df_to_augment.empty:\n",
    "    # Ajustar el nuevo encoder solo con las etiquetas de odio\n",
    "    sub_hate_only_encoder.fit(df_to_augment['sub_label_str'])\n",
    "    print(\"\\nNuevo encoder para Nivel 2 creado. Clases:\", sub_hate_only_encoder.classes_)\n",
    "\n",
    "    # 2. Configurar metadatos\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(data=df_to_augment)\n",
    "\n",
    "    print(\"\\nActualizando metadatos para tratar embeddings como numéricos continuos...\")\n",
    "    for col in embedding_cols:\n",
    "        metadata.update_column(column_name=col, sdtype='numerical')\n",
    "    metadata.update_column(column_name='sub_label_str', sdtype='categorical')\n",
    "\n",
    "    # 3. Configurar y entrenar el sintetizador CTGAN\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    print(f\"Usando GPU: {use_gpu}\")\n",
    "    synthesizer = CTGANSynthesizer(\n",
    "        metadata,\n",
    "        epochs=150,\n",
    "        embedding_dim=64,\n",
    "        verbose=False,\n",
    "        cuda=use_gpu\n",
    "    )\n",
    "\n",
    "    print(f\"\\nEntrenando CTGAN para generar datos sintéticos... (Usando GPU: {use_gpu})\")\n",
    "    synthesizer.fit(df_to_augment)\n",
    "\n",
    "    # 4. Determinar cuántas muestras generar\n",
    "    max_class_size = hate_counts.max()\n",
    "    num_to_generate = max_class_size * len(hate_counts) - hate_counts.sum()\n",
    "\n",
    "    # 5. Generar y combinar datos\n",
    "    if num_to_generate > 0:\n",
    "        print(f\"\\nGenerando {num_to_generate} muestras sintéticas...\")\n",
    "        df_synthetic = synthesizer.sample(num_rows=num_to_generate)\n",
    "        df_train_hate_balanced = pd.concat([df_to_augment, df_synthetic], ignore_index=True)\n",
    "    else:\n",
    "        df_train_hate_balanced = df_to_augment\n",
    "else:\n",
    "    print(\"\\nSolo una sub-categoría presente o no hay datos de odio, no se requiere aumento.\")\n",
    "    df_train_hate_balanced = df_to_augment\n",
    "    if not df_to_augment.empty:\n",
    "        # Ajustar el encoder si solo hay una clase\n",
    "        sub_hate_only_encoder.fit(df_to_augment['sub_label_str'])\n",
    "\n",
    "print(\"\\nDistribución de sub-categorías DESPUÉS del aumento:\")\n",
    "all_sub_labels = df_to_augment['sub_label_str'].unique()\n",
    "print(df_train_hate_balanced['sub_label_str'].value_counts().reindex(all_sub_labels, fill_value=0))\n",
    "\n",
    "# Preparar datos de entrenamiento para el clasificador de sub-categorías\n",
    "if not df_train_hate_balanced.empty:\n",
    "    X_train_sub_emb = df_train_hate_balanced[embedding_cols].values\n",
    "    # Usar el NUEVO encoder para transformar las etiquetas\n",
    "    y_train_sub = sub_hate_only_encoder.transform(df_train_hate_balanced['sub_label_str'])\n",
    "else:\n",
    "    # Crear arrays vacíos si no hay datos para evitar errores posteriores\n",
    "    X_train_sub_emb = np.array([]).reshape(0, len(embedding_cols))\n",
    "    y_train_sub = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main_classifier_md",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento del Clasificador Principal (Nivel 1) con Optuna y Ensemble Extendido\n",
    "\n",
    "Aquí es donde integramos el pipeline de entrenamiento robusto. Entrenaremos y optimizaremos un **ensemble de seis modelos** (XGBoost y MLP usando solo embeddings, los mismos dos usando embeddings + TF-IDF, y dos Regresiones Logísticas usando cada tipo de característica por separado). Este proceso no utiliza los datos aumentados, solo el conjunto de entrenamiento original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main_classifier_train",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"--- Preparando datos para el entrenamiento del Clasificador Principal ---\")\n",
    "\n",
    "# Usar las variables correctas del split jerárquico\n",
    "y_train = df_train['main_label'].values\n",
    "y_val = df_val['main_label'].values\n",
    "num_classes = len(np.unique(y_train)) # Será 2 en este caso\n",
    "\n",
    "# 1. Escalar características de embeddings\n",
    "scaler_L1_emb = StandardScaler()\n",
    "X_train_emb = df_train[embedding_cols].values\n",
    "X_train_emb_scaled = scaler_L1_emb.fit_transform(X_train_emb)\n",
    "X_val_emb = df_val[embedding_cols].values\n",
    "X_val_emb_scaled = scaler_L1_emb.transform(X_val_emb)\n",
    "\n",
    "# 2. Vectorizar características de texto con TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "X_train_text = df_train['text_stemmed'].values\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_val_text = df_val['text_stemmed'].values\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val_text)\n",
    "print(f\"TF-IDF: {X_train_tfidf.shape[1]} características generadas.\")\n",
    "\n",
    "# 3. Crear características combinadas (Embeddings + TF-IDF)\n",
    "# Para XGBoost y modelos de Scikit-learn, usamos la matriz sparse combinada\n",
    "X_train_combined = hstack([X_train_emb, X_train_tfidf]).tocsr()\n",
    "X_val_combined = hstack([X_val_emb, X_val_tfidf]).tocsr()\n",
    "\n",
    "# Para MLP, necesitamos una matriz densa. Combinamos embeddings escalados y TF-IDF denso.\n",
    "X_train_combined_dense = np.hstack([X_train_emb_scaled, X_train_tfidf.toarray()])\n",
    "X_val_combined_dense = np.hstack([X_val_emb_scaled, X_val_tfidf.toarray()])\n",
    "\n",
    "# 4. Convertir datos a tensores para PyTorch\n",
    "X_val_torch_emb = torch.tensor(X_val_emb_scaled, dtype=torch.float32).to(device)\n",
    "X_val_torch_combined = torch.tensor(X_val_combined_dense, dtype=torch.float32).to(device)\n",
    "y_val_torch = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "print(\"\\n✓ Datos escalados, vectorizados y tensores de PyTorch listos para el Nivel 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5533809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clase genérica para MLP ---\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size, activation_fn, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(current_size, hidden_size))\n",
    "            layers.append(activation_fn())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            current_size = hidden_size\n",
    "        layers.append(nn.Linear(current_size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Función genérica para entrenar y evaluar MLP en Optuna ---\n",
    "def train_eval_mlp_objective(trial, X_train_data, y_train_data, X_val_tensor, y_val_tensor, input_dim):\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    hidden_layers = [trial.suggest_int(f'n_units_l{i}', 32, 256) for i in range(n_layers)]\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop'])\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    activation_fn = getattr(nn, trial.suggest_categorical('activation', ['ReLU', 'Tanh']))\n",
    "\n",
    "    model = MLP(input_dim, hidden_layers, num_classes, activation_fn, dropout_rate).to(device)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train_data, dtype=torch.float32), torch.tensor(y_train_data, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    for epoch in range(25):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(data), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = criterion(model(X_val_tensor), y_val_tensor).item()\n",
    "\n",
    "    trial.report(val_loss, epoch)\n",
    "    if trial.should_prune(): raise optuna.exceptions.TrialPruned()\n",
    "    return val_loss\n",
    "\n",
    "# --- 1. Objetivo para MLP (Usa solo Embeddings) ---\n",
    "def objective_mlp_embeddings(trial):\n",
    "    return train_eval_mlp_objective(trial, X_train_emb_scaled, y_train, X_val_torch_emb, y_val_torch, X_train_emb_scaled.shape[1])\n",
    "\n",
    "# --- 2. Objetivo para MLP (Usa Embeddings + TF-IDF) ---\n",
    "def objective_mlp_combined(trial):\n",
    "    return train_eval_mlp_objective(trial, X_train_combined_dense, y_train, X_val_torch_combined, y_val_torch, X_train_combined_dense.shape[1])\n",
    "\n",
    "# --- 3. Objetivo para XGBoost (Usa solo Embeddings) ---\n",
    "def objective_xgboost_embeddings(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic', 'eval_metric': 'logloss',\n",
    "        'device': 'cuda' if device.type == 'cuda' else 'cpu',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**params, early_stopping_rounds=10)\n",
    "    model.fit(X_train_emb, y_train, eval_set=[(X_val_emb, y_val)], verbose=False)\n",
    "    return log_loss(y_val, model.predict_proba(X_val_emb))\n",
    "\n",
    "# --- 4. Objetivo para XGBoost (Usa Embeddings + TF-IDF) ---\n",
    "def objective_xgboost_combined(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic', 'eval_metric': 'logloss',\n",
    "        'device': 'cuda' if device.type == 'cuda' else 'cpu',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**params, early_stopping_rounds=10)\n",
    "    model.fit(X_train_combined, y_train, eval_set=[(X_val_combined, y_val)], verbose=False)\n",
    "    return log_loss(y_val, model.predict_proba(X_val_combined))\n",
    "\n",
    "# --- 5. Objetivo para Regresión Logística (Usa solo Embeddings) ---\n",
    "def objective_logistic_embeddings(trial):\n",
    "    params = {'C': trial.suggest_float('C', 1e-4, 1e2, log=True), 'solver': 'liblinear', 'max_iter': 1000}\n",
    "    model = LogisticRegression(**params, random_state=42)\n",
    "    model.fit(X_train_emb_scaled, y_train)\n",
    "    return log_loss(y_val, model.predict_proba(X_val_emb_scaled))\n",
    "\n",
    "# --- 6. Objetivo para Regresión Logística (Usa solo TF-IDF) ---\n",
    "def objective_logistic_tfidf(trial):\n",
    "    params = {'C': trial.suggest_float('C', 1e-2, 1e2, log=True), 'solver': 'liblinear', 'max_iter': 1000}\n",
    "    model = LogisticRegression(**params, random_state=42)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    return log_loss(y_val, model.predict_proba(X_val_tfidf))\n",
    "\n",
    "print(f\"Funciones objetivo de Optuna para el Nivel 1 definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b520e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    'XGBoost_Embeddings': {'objective_func': objective_xgboost_embeddings, 'n_trials': 25},\n",
    "    'MLP_PyTorch_Embeddings': {'objective_func': objective_mlp_embeddings, 'n_trials': 30},\n",
    "    'LogisticRegression_Embeddings': {'objective_func': objective_logistic_embeddings, 'n_trials': 20},\n",
    "    'LogisticRegression_TFIDF': {'objective_func': objective_logistic_tfidf, 'n_trials': 20},\n",
    "    'XGBoost_Combined': {'objective_func': objective_xgboost_combined, 'n_trials': 25},\n",
    "    'MLP_PyTorch_Combined': {'objective_func': objective_mlp_combined, 'n_trials': 30}\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model_name, config in models_config.items():\n",
    "    print(f\"\\n--- Optimizando {model_name} (Nivel 1) ---\")\n",
    "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(config['objective_func'], n_trials=config['n_trials'], show_progress_bar=True)\n",
    "\n",
    "    model_results[model_name] = {\n",
    "        'best_params': study.best_params,\n",
    "        'best_score': study.best_value\n",
    "    }\n",
    "    print(f\"✓ {model_name} completado. Mejor LogLoss: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classifier_models = {}\n",
    "print(\"--- Entrenando modelos finales con los mejores hiperparámetros ---\\n\")\n",
    "\n",
    "# 1. XGBoost (Embeddings)\n",
    "params = model_results['XGBoost_Embeddings']['best_params']\n",
    "final_xgb_emb = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss',\n",
    "                                  device='cuda' if device.type == 'cuda' else 'cpu', **params)\n",
    "final_xgb_emb.fit(X_train_emb, y_train)\n",
    "main_classifier_models['XGBoost_Embeddings'] = final_xgb_emb\n",
    "print(\"✓ Modelo XGBoost (Embeddings) final entrenado.\")\n",
    "\n",
    "# 2. XGBoost (Combined)\n",
    "params = model_results['XGBoost_Combined']['best_params']\n",
    "final_xgb_comb = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss',\n",
    "                                   device='cuda' if device.type == 'cuda' else 'cpu', **params)\n",
    "final_xgb_comb.fit(X_train_combined, y_train)\n",
    "main_classifier_models['XGBoost_Combined'] = final_xgb_comb\n",
    "print(\"✓ Modelo XGBoost (Combined) final entrenado.\")\n",
    "\n",
    "# 3. Regresión Logística (Embeddings)\n",
    "params = model_results['LogisticRegression_Embeddings']['best_params']\n",
    "final_log_emb = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000, **params)\n",
    "final_log_emb.fit(X_train_emb_scaled, y_train)\n",
    "main_classifier_models['LogisticRegression_Embeddings'] = final_log_emb\n",
    "print(\"✓ Modelo Regresión Logística (Embeddings) final entrenado.\")\n",
    "\n",
    "# 4. Regresión Logística (TF-IDF)\n",
    "params = model_results['LogisticRegression_TFIDF']['best_params']\n",
    "final_log_tfidf = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000, **params)\n",
    "final_log_tfidf.fit(X_train_tfidf, y_train)\n",
    "main_classifier_models['LogisticRegression_TFIDF'] = final_log_tfidf\n",
    "print(\"✓ Modelo Regresión Logística (TF-IDF) final entrenado.\")\n",
    "\n",
    "# 5. MLP (Embeddings)\n",
    "params = model_results['MLP_PyTorch_Embeddings']['best_params']\n",
    "hidden_layers = [params[f'n_units_l{i}'] for i in range(params['n_layers'])]\n",
    "final_mlp_emb = MLP(X_train_emb_scaled.shape[1], hidden_layers, num_classes,\n",
    "                    getattr(nn, params['activation']), params['dropout_rate']).to(device)\n",
    "optimizer = getattr(optim, params['optimizer'])(final_mlp_emb.parameters(), lr=params['lr'])\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_train_emb_scaled, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)), batch_size=128, shuffle=True)\n",
    "for epoch in tqdm(range(30), desc=\"Epochs MLP (Embeddings) final\"):\n",
    "    final_mlp_emb.train()\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = nn.CrossEntropyLoss()(final_mlp_emb(data), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "main_classifier_models['MLP_PyTorch_Embeddings'] = final_mlp_emb.eval()\n",
    "print(\"✓ Modelo MLP (Embeddings) final entrenado.\")\n",
    "\n",
    "# 6. MLP (Combined)\n",
    "params = model_results['MLP_PyTorch_Combined']['best_params']\n",
    "hidden_layers = [params[f'n_units_l{i}'] for i in range(params['n_layers'])]\n",
    "final_mlp_comb = MLP(X_train_combined_dense.shape[1], hidden_layers, num_classes,\n",
    "                     getattr(nn, params['activation']), params['dropout_rate']).to(device)\n",
    "optimizer = getattr(optim, params['optimizer'])(final_mlp_comb.parameters(), lr=params['lr'])\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_train_combined_dense, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)), batch_size=128, shuffle=True)\n",
    "for epoch in tqdm(range(30), desc=\"Epochs MLP (Combined) final\"):\n",
    "    final_mlp_comb.train()\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = nn.CrossEntropyLoss()(final_mlp_comb(data), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "main_classifier_models['MLP_PyTorch_Combined'] = final_mlp_comb.eval()\n",
    "print(\"✓ Modelo MLP (Combined) final entrenado.\")\n",
    "\n",
    "print(\"\\n✓ Todos los modelos finales han sido entrenados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Calculando pesos para el Ensemble del Clasificador Principal (Nivel 1) ---\")\n",
    "val_probas = {}\n",
    "\n",
    "# Obtener predicciones de cada modelo en el set de validación\n",
    "val_probas['XGBoost_Embeddings'] = main_classifier_models['XGBoost_Embeddings'].predict_proba(X_val_emb)\n",
    "val_probas['XGBoost_Combined'] = main_classifier_models['XGBoost_Combined'].predict_proba(X_val_combined)\n",
    "val_probas['LogisticRegression_Embeddings'] = main_classifier_models['LogisticRegression_Embeddings'].predict_proba(X_val_emb_scaled)\n",
    "val_probas['LogisticRegression_TFIDF'] = main_classifier_models['LogisticRegression_TFIDF'].predict_proba(X_val_tfidf)\n",
    "with torch.no_grad():\n",
    "    # MLP Embeddings\n",
    "    mlp_outputs_emb = main_classifier_models['MLP_PyTorch_Embeddings'](X_val_torch_emb)\n",
    "    val_probas['MLP_PyTorch_Embeddings'] = torch.softmax(mlp_outputs_emb, dim=1).cpu().numpy()\n",
    "    # MLP Combined\n",
    "    mlp_outputs_comb = main_classifier_models['MLP_PyTorch_Combined'](X_val_torch_combined)\n",
    "    val_probas['MLP_PyTorch_Combined'] = torch.softmax(mlp_outputs_comb, dim=1).cpu().numpy()\n",
    "\n",
    "# Calcular métricas y pesos del ensemble (mayor peso a menor log_loss)\n",
    "losses = {name: log_loss(y_val, proba) for name, proba in val_probas.items()}\n",
    "scores = {name: 1.0 / (loss + 1e-9) for name, loss in losses.items()}\n",
    "total_score = sum(scores.values())\n",
    "ensemble_weights = {name: score / total_score for name, score in scores.items()}\n",
    "\n",
    "print(\"\\n--- Pesos del Ensemble de Nivel 1 Calculados ---\")\n",
    "for name, w in sorted(ensemble_weights.items(), key=lambda item: item[1], reverse=True):\n",
    "    print(f\"{name:<30} | Peso: {w:.3f} | LogLoss (Val): {losses[name]:.4f}\")\n",
    "\n",
    "# Evaluar el rendimiento del ensemble en el set de validación\n",
    "ensemble_proba_val = np.zeros_like(val_probas['XGBoost_Embeddings'])\n",
    "for name, proba in val_probas.items():\n",
    "    ensemble_proba_val += proba * ensemble_weights[name]\n",
    "\n",
    "ensemble_log_loss_val = log_loss(y_val, ensemble_proba_val)\n",
    "print(f\"\\nLogLoss del Ensemble L1 en Validación: {ensemble_log_loss_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sub_classifier_md",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento del Clasificador de Sub-categorías (Nivel 2) con Optuna y Ensemble\n",
    "\n",
    "Ahora, aplicamos la misma metodología robusta al clasificador de Nivel 2. Este se entrenará **únicamente con los datos de 'odio' balanceados sintéticamente**. Crearemos un ensemble de tres modelos (XGBoost, MLP, Regresión Logística) usando **características combinadas de embeddings y TF-IDF**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sub_classifier_train",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Preparando datos y definiendo objetivos para el Clasificador de Sub-categorías (Nivel 2) ---\")\n",
    "\n",
    "if X_train_sub_emb.shape[0] > 0:\n",
    "    # 1. Preparar características TF-IDF para datos de Nivel 2\n",
    "    # Datos reales de 'odio'\n",
    "    real_hate_texts_train = df_train_hate['text_stemmed']\n",
    "    X_train_sub_tfidf_real = tfidf_vectorizer.transform(real_hate_texts_train)\n",
    "    # Datos sintéticos (vector de ceros, ya que no tienen texto)\n",
    "    num_synthetic = len(df_synthetic)\n",
    "    X_train_sub_tfidf_synthetic = csr_matrix((num_synthetic, X_train_sub_tfidf_real.shape[1]), dtype=np.float64)\n",
    "    # Combinar TF-IDF de datos reales y sintéticos\n",
    "    X_train_sub_tfidf = vstack([X_train_sub_tfidf_real, X_train_sub_tfidf_synthetic])\n",
    "\n",
    "    # 2. Combinar Embeddings y TF-IDF para Nivel 2\n",
    "    X_train_sub_combined = hstack([X_train_sub_emb, X_train_sub_tfidf]).tocsr()\n",
    "    num_sub_classes = len(np.unique(y_train_sub))\n",
    "    print(f\"Datos combinados para Nivel 2 listos. Shape: {X_train_sub_combined.shape}, {num_sub_classes} sub-clases detectadas.\")\n",
    "\n",
    "    # 3. Dividir los datos COMBINADOS para HPO\n",
    "    X_sub_train_comb, X_sub_val_comb, y_sub_train, y_sub_val = train_test_split(\n",
    "        X_train_sub_combined, y_train_sub, test_size=0.25, random_state=42, stratify=y_train_sub\n",
    "    )\n",
    "\n",
    "    # 4. Escalar la parte de embeddings de los datos combinados para MLP y LogReg\n",
    "    scaler_L2_emb = StandardScaler()\n",
    "    # Extraer y escalar la parte de embeddings\n",
    "    X_sub_train_emb_part = X_sub_train_comb[:, :X_train_sub_emb.shape[1]].toarray()\n",
    "    X_sub_train_emb_part_scaled = scaler_L2_emb.fit_transform(X_sub_train_emb_part)\n",
    "    X_sub_val_emb_part = X_sub_val_comb[:, :X_train_sub_emb.shape[1]].toarray()\n",
    "    X_sub_val_emb_part_scaled = scaler_L2_emb.transform(X_sub_val_emb_part)\n",
    "    # Re-combinar con la parte de TF-IDF (que no se escala)\n",
    "    X_sub_train_scaled_comb_dense = np.hstack([X_sub_train_emb_part_scaled, X_sub_train_comb[:, X_train_sub_emb.shape[1]:].toarray()])\n",
    "    X_sub_val_scaled_comb_dense = np.hstack([X_sub_val_emb_part_scaled, X_sub_val_comb[:, X_train_sub_emb.shape[1]:].toarray()])\n",
    "\n",
    "    # 5. Convertir a tensores de PyTorch\n",
    "    X_sub_val_torch = torch.tensor(X_sub_val_scaled_comb_dense, dtype=torch.float32).to(device)\n",
    "    y_sub_val_torch = torch.tensor(y_sub_val, dtype=torch.long).to(device)\n",
    "\n",
    "    # --- Funciones Objetivo para Optuna (Nivel 2, con datos combinados) ---\n",
    "    def objective_xgboost_L2(trial):\n",
    "        params = {'objective': 'multi:softprob', 'num_class': num_sub_classes, 'eval_metric': 'mlogloss', 'device': 'cuda',\n",
    "                  'n_estimators': trial.suggest_int('n_estimators', 100, 800), 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True), 'max_depth': trial.suggest_int('max_depth', 3, 8)}\n",
    "        model = xgb.XGBClassifier(**params, early_stopping_rounds=10)\n",
    "        model.fit(X_sub_train_comb, y_sub_train, eval_set=[(X_sub_val_comb, y_sub_val)], verbose=False)\n",
    "        return log_loss(y_sub_val, model.predict_proba(X_sub_val_comb))\n",
    "\n",
    "    def objective_logistic_L2(trial):\n",
    "        params = {'C': trial.suggest_float('C', 1e-3, 1e2, log=True), 'solver': 'liblinear', 'max_iter': 1000, 'multi_class': 'ovr'}\n",
    "        model = LogisticRegression(**params, random_state=42)\n",
    "        # Se entrena con los datos densos (escalados en parte)\n",
    "        model.fit(X_sub_train_scaled_comb_dense, y_sub_train)\n",
    "        return log_loss(y_sub_val, model.predict_proba(X_sub_val_scaled_comb_dense))\n",
    "\n",
    "    def objective_mlp_L2(trial):\n",
    "        n_layers = trial.suggest_int('n_layers', 1, 2)\n",
    "        hidden_layers = [trial.suggest_int(f'n_units_l{i}', 32, 128) for i in range(n_layers)]\n",
    "        lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "\n",
    "        model = MLP(X_sub_train_scaled_comb_dense.shape[1], hidden_layers, num_sub_classes, nn.ReLU, 0.3).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        train_dataset = TensorDataset(torch.tensor(X_sub_train_scaled_comb_dense, dtype=torch.float32), torch.tensor(y_sub_train, dtype=torch.long))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        for epoch in range(20):\n",
    "            for data, target in train_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(data), target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = criterion(model(X_sub_val_torch), y_sub_val_torch).item()\n",
    "        return val_loss\n",
    "    print(\"Funciones objetivo para Nivel 2 definidas.\")\n",
    "else:\n",
    "    print(\"No hay datos para preparar el Nivel 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train_sub_emb.shape[0] > 0:\n",
    "    # --- 1. Búsqueda de Hiperparámetros (HPO) para Nivel 2 ---\n",
    "    models_config_L2 = {\n",
    "        'XGBoost_L2': {'objective_func': objective_xgboost_L2, 'n_trials': 20},\n",
    "        'MLP_PyTorch_L2': {'objective_func': objective_mlp_L2, 'n_trials': 25},\n",
    "        'LogisticRegression_L2': {'objective_func': objective_logistic_L2, 'n_trials': 15}\n",
    "    }\n",
    "    model_results_L2 = {}\n",
    "    for model_name, config in models_config_L2.items():\n",
    "        print(f\"\\n--- Optimizando {model_name} (Nivel 2) ---\")\n",
    "        study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(config['objective_func'], n_trials=config['n_trials'], show_progress_bar=True)\n",
    "        model_results_L2[model_name] = {'best_params': study.best_params}\n",
    "\n",
    "    # --- 2. Entrenamiento de los modelos finales del ensemble de Nivel 2 ---\n",
    "    print(\"\\n--- Entrenando modelos finales del Ensemble (Nivel 2) ---\")\n",
    "    sub_classifier_models = {}\n",
    "\n",
    "    # Preparar datos completos de entrenamiento L2 (escalados y densos para MLP/LogReg)\n",
    "    X_train_sub_emb_part_full = X_train_sub_combined[:, :X_train_sub_emb.shape[1]].toarray()\n",
    "    X_train_sub_emb_part_full_scaled = scaler_L2_emb.transform(X_train_sub_emb_part_full)\n",
    "    X_train_sub_full_scaled_dense = np.hstack([X_train_sub_emb_part_full_scaled, X_train_sub_combined[:, X_train_sub_emb.shape[1]:].toarray()])\n",
    "\n",
    "    # XGBoost L2\n",
    "    params = model_results_L2['XGBoost_L2']['best_params']\n",
    "    final_xgb_L2 = xgb.XGBClassifier(objective='multi:softprob', num_class=num_sub_classes, eval_metric='mlogloss',\n",
    "                                     device='cuda' if device.type == 'cuda' else 'cpu', **params)\n",
    "    final_xgb_L2.fit(X_train_sub_combined, y_train_sub) # Entrenar con sparse\n",
    "    sub_classifier_models['XGBoost_L2'] = final_xgb_L2\n",
    "\n",
    "    # Logistic Regression L2\n",
    "    params = model_results_L2['LogisticRegression_L2']['best_params']\n",
    "    final_log_L2 = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000, **params)\n",
    "    final_log_L2.fit(X_train_sub_full_scaled_dense, y_train_sub) # Entrenar con denso escalado\n",
    "    sub_classifier_models['LogisticRegression_L2'] = final_log_L2\n",
    "\n",
    "    # MLP L2\n",
    "    params = model_results_L2['MLP_PyTorch_L2']['best_params']\n",
    "    hidden_layers = [params[f'n_units_l{i}'] for i in range(params['n_layers'])]\n",
    "    final_mlp_L2 = MLP(X_train_sub_full_scaled_dense.shape[1], hidden_layers, num_sub_classes, nn.ReLU, 0.3).to(device)\n",
    "    optimizer = optim.Adam(final_mlp_L2.parameters(), lr=params['lr'])\n",
    "    train_dataset_L2 = TensorDataset(torch.tensor(X_train_sub_full_scaled_dense, dtype=torch.float32), torch.tensor(y_train_sub, dtype=torch.long))\n",
    "    train_loader_L2 = DataLoader(train_dataset_L2, batch_size=64, shuffle=True)\n",
    "    for epoch in tqdm(range(30), desc=\"Epochs MLP L2 final\"):\n",
    "        for data, target in train_loader_L2:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = nn.CrossEntropyLoss()(final_mlp_L2(data), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    sub_classifier_models['MLP_PyTorch_L2'] = final_mlp_L2.eval()\n",
    "    print(\"✓ Todos los modelos del ensemble de Nivel 2 han sido entrenados.\")\n",
    "\n",
    "    # --- 3. Cálculo de pesos para el ensemble de Nivel 2 ---\n",
    "    print(\"\\n--- Calculando pesos para el Ensemble de Nivel 2 ---\")\n",
    "    val_probas_L2 = {}\n",
    "    val_probas_L2['XGBoost_L2'] = sub_classifier_models['XGBoost_L2'].predict_proba(X_sub_val_comb)\n",
    "    val_probas_L2['LogisticRegression_L2'] = sub_classifier_models['LogisticRegression_L2'].predict_proba(X_sub_val_scaled_comb_dense)\n",
    "    with torch.no_grad():\n",
    "        mlp_outputs = sub_classifier_models['MLP_PyTorch_L2'](X_sub_val_torch)\n",
    "        val_probas_L2['MLP_PyTorch_L2'] = torch.softmax(mlp_outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    losses_L2 = {name: log_loss(y_sub_val, proba, labels=np.unique(y_train_sub)) for name, proba in val_probas_L2.items()}\n",
    "    scores_L2 = {name: 1.0 / (loss + 1e-9) for name, loss in losses_L2.items()}\n",
    "    total_score_L2 = sum(scores_L2.values())\n",
    "    ensemble_weights_L2 = {name: score / total_score_L2 for name, score in scores_L2.items()}\n",
    "\n",
    "    print(\"\\n--- Pesos del Ensemble de Nivel 2 Calculados ---\")\n",
    "    for name, w in sorted(ensemble_weights_L2.items(), key=lambda item: item[1], reverse=True):\n",
    "        print(f\"{name:<25} | Peso: {w:.3f} | LogLoss (Val): {losses_L2[name]:.4f}\")\n",
    "else:\n",
    "    print(\"No hay datos para entrenar el clasificador de Nivel 2.\")\n",
    "    sub_classifier_models = None\n",
    "    ensemble_weights_L2 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_eval_md",
   "metadata": {},
   "source": [
    "## 7. Evaluación Final del Pipeline Jerárquico Robusto\n",
    "\n",
    "Evaluamos el pipeline completo. Primero, usamos el **ensemble ponderado de Nivel 1** para la predicción de \"Odio vs. No-Odio\". Luego, para las predicciones de \"odio\", usamos el **ensemble ponderado de Nivel 2** para predecir la sub-categoría, ambos usando las configuraciones de características correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_eval_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Evaluación del pipeline jerárquico en el conjunto de prueba ---\")\n",
    "\n",
    "# 1. Preparar todas las características de prueba\n",
    "X_test_emb_eval = df_test[embedding_cols].values\n",
    "X_test_emb_scaled_eval = scaler_L1_emb.transform(X_test_emb_eval)\n",
    "X_test_text_eval = df_test['text_stemmed'].values\n",
    "X_test_tfidf_eval = tfidf_vectorizer.transform(X_test_text_eval)\n",
    "X_test_combined_eval = hstack([X_test_emb_eval, X_test_tfidf_eval]).tocsr()\n",
    "X_test_combined_dense_eval = np.hstack([X_test_emb_scaled_eval, X_test_tfidf_eval.toarray()])\n",
    "X_test_torch_emb_eval = torch.tensor(X_test_emb_scaled_eval, dtype=torch.float32).to(device)\n",
    "X_test_torch_combined_eval = torch.tensor(X_test_combined_dense_eval, dtype=torch.float32).to(device)\n",
    "y_main_true = df_test['main_label'].values\n",
    "\n",
    "# 2. Obtener predicciones del ENSEMBLE de Nivel 1\n",
    "test_probas_L1 = {}\n",
    "test_probas_L1['XGBoost_Embeddings'] = main_classifier_models['XGBoost_Embeddings'].predict_proba(X_test_emb_eval)\n",
    "test_probas_L1['XGBoost_Combined'] = main_classifier_models['XGBoost_Combined'].predict_proba(X_test_combined_eval)\n",
    "test_probas_L1['LogisticRegression_Embeddings'] = main_classifier_models['LogisticRegression_Embeddings'].predict_proba(X_test_emb_scaled_eval)\n",
    "test_probas_L1['LogisticRegression_TFIDF'] = main_classifier_models['LogisticRegression_TFIDF'].predict_proba(X_test_tfidf_eval)\n",
    "with torch.no_grad():\n",
    "    test_probas_L1['MLP_PyTorch_Embeddings'] = torch.softmax(main_classifier_models['MLP_PyTorch_Embeddings'](X_test_torch_emb_eval), dim=1).cpu().numpy()\n",
    "    test_probas_L1['MLP_PyTorch_Combined'] = torch.softmax(main_classifier_models['MLP_PyTorch_Combined'](X_test_torch_combined_eval), dim=1).cpu().numpy()\n",
    "\n",
    "final_ensemble_proba_L1 = np.zeros_like(test_probas_L1['XGBoost_Embeddings'])\n",
    "for name, proba in test_probas_L1.items():\n",
    "    final_ensemble_proba_L1 += proba * ensemble_weights[name]\n",
    "y_main_pred = np.argmax(final_ensemble_proba_L1, axis=1)\n",
    "\n",
    "# 3. Evaluar Nivel 1\n",
    "print(\"\\n--- [Nivel 1] Rendimiento del Ensemble Principal (Prueba) ---\")\n",
    "print(classification_report(y_main_true, y_main_pred, target_names=['not-hate', 'hate']))\n",
    "\n",
    "# 4. Obtener y evaluar predicciones del ENSEMBLE de Nivel 2\n",
    "if sub_classifier_models is not None:\n",
    "    df_test_true_hate = df_test[df_test['main_label'] == 1].copy()\n",
    "    if not df_test_true_hate.empty:\n",
    "        y_sub_true = sub_hate_only_encoder.transform(df_test_true_hate['sub_label_str'])\n",
    "\n",
    "        # Preparar datos combinados para L2 en el conjunto de prueba\n",
    "        X_test_hate_emb = df_test_true_hate[embedding_cols].values\n",
    "        X_test_hate_tfidf = tfidf_vectorizer.transform(df_test_true_hate['text_stemmed'])\n",
    "        X_test_hate_combined = hstack([X_test_hate_emb, X_test_hate_tfidf]).tocsr()\n",
    "\n",
    "        # Preparar versión densa y escalada para MLP/LogReg\n",
    "        X_test_hate_emb_scaled = scaler_L2_emb.transform(X_test_hate_emb)\n",
    "        X_test_hate_combined_dense = np.hstack([X_test_hate_emb_scaled, X_test_hate_tfidf.toarray()])\n",
    "        X_test_hate_torch = torch.tensor(X_test_hate_combined_dense, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Obtener y combinar probabilidades L2\n",
    "        true_hate_probas_L2 = {}\n",
    "        true_hate_probas_L2['XGBoost_L2'] = sub_classifier_models['XGBoost_L2'].predict_proba(X_test_hate_combined)\n",
    "        true_hate_probas_L2['LogisticRegression_L2'] = sub_classifier_models['LogisticRegression_L2'].predict_proba(X_test_hate_combined_dense)\n",
    "        with torch.no_grad():\n",
    "            mlp_outputs_L2 = sub_classifier_models['MLP_PyTorch_L2'](X_test_hate_torch)\n",
    "            true_hate_probas_L2['MLP_PyTorch_L2'] = torch.softmax(mlp_outputs_L2, dim=1).cpu().numpy()\n",
    "\n",
    "        final_true_hate_proba_L2 = np.zeros_like(true_hate_probas_L2['XGBoost_L2'])\n",
    "        for name, proba in true_hate_probas_L2.items():\n",
    "            final_true_hate_proba_L2 += proba * ensemble_weights_L2[name]\n",
    "        y_sub_pred_for_eval = np.argmax(final_true_hate_proba_L2, axis=1)\n",
    "\n",
    "        print(\"\\n--- [Nivel 2] Rendimiento del Ensemble de Sub-categorías (Prueba) ---\")\n",
    "        print(classification_report(y_sub_true, y_sub_pred_for_eval, target_names=sub_hate_only_encoder.classes_, zero_division=0))\n",
    "else:\n",
    "    print(\"\\nEl clasificador de sub-categorías no fue entrenado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_artifacts_md",
   "metadata": {},
   "source": [
    "## 8. Guardado de Artefactos\n",
    "\n",
    "Guardamos todos los componentes del pipeline jerárquico: los modelos de ambos ensembles, sus respectivos pesos, transformadores y codificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_artifacts_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Guardando artefactos en {MODEL_OUTPUT_DIR} ---\")\n",
    "\n",
    "# 1. Guardar modelos y pesos del ensemble de Nivel 1\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, \"main_classifier_models_L1.pkl\"), 'wb') as f: pickle.dump(main_classifier_models, f)\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, \"ensemble_weights_L1.pkl\"), 'wb') as f: pickle.dump(ensemble_weights, f)\n",
    "print(\"✓ Modelos y pesos de Nivel 1 guardados.\")\n",
    "\n",
    "# 2. Guardar modelos y pesos del ensemble de Nivel 2\n",
    "if sub_classifier_models is not None:\n",
    "    with open(os.path.join(MODEL_OUTPUT_DIR, \"sub_classifier_models_L2.pkl\"), 'wb') as f: pickle.dump(sub_classifier_models, f)\n",
    "    with open(os.path.join(MODEL_OUTPUT_DIR, \"ensemble_weights_L2.pkl\"), 'wb') as f: pickle.dump(ensemble_weights_L2, f)\n",
    "    print(\"✓ Modelos y pesos de Nivel 2 guardados.\")\n",
    "\n",
    "# 3. Guardar transformadores y codificadores\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, \"scaler_L1_emb.pkl\"), 'wb') as f: pickle.dump(scaler_L1_emb, f)\n",
    "if 'scaler_L2_emb' in locals():\n",
    "    with open(os.path.join(MODEL_OUTPUT_DIR, \"scaler_L2_emb.pkl\"), 'wb') as f: pickle.dump(scaler_L2_emb, f)\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, \"tfidf_vectorizer.pkl\"), 'wb') as f: pickle.dump(tfidf_vectorizer, f)\n",
    "if 'sub_hate_only_encoder' in locals():\n",
    "    with open(os.path.join(MODEL_OUTPUT_DIR, \"sub_hate_only_encoder.pkl\"), 'wb') as f: pickle.dump(sub_hate_only_encoder, f)\n",
    "print(\"✓ Scalers, TF-IDF Vectorizer y codificador de sub-etiquetas guardados.\")\n",
    "\n",
    "# 4. Guardar resultados de Optuna\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, \"optuna_results.pkl\"), 'wb') as f:\n",
    "    pickle.dump({'L1': model_results, 'L2': model_results_L2 if 'model_results_L2' in locals() else {}}, f)\n",
    "print(\"✓ Resultados de Optuna guardados.\")\n",
    "\n",
    "print(\"\\n🎉 Pipeline jerárquico robusto completado y todos los artefactos han sido guardados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
