{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# Modelo de Clasificación Local de Extremo a Extremo\n",
    "\n",
    "Este notebook implementa un pipeline completo de Machine Learning de forma local:\n",
    "\n",
    "1.  **Carga de Datos**: Usa un dataset de texto real (`ag_news`) de la librería `datasets`.\n",
    "2.  **Análisis y Tokenización**: Analiza la longitud de los textos usando un tokenizador de BERT.\n",
    "3.  **Generación de Embeddings**: Convierte el texto en vectores numéricos (embeddings) usando un modelo BERT pre-entrenado.\n",
    "4.  **Entrenamiento**: Entrena un clasificador XGBoost con los embeddings generados.\n",
    "5.  **Evaluación**: Evalúa el rendimiento del modelo final.\n",
    "\n",
    "Todo el proceso se ejecuta localmente sin dependencias de la nube."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_md",
   "metadata": {},
   "source": [
    "## 1. Instalación y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch datasets scikit-learn xgboost pandas seaborn matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURACIÓN GENERAL ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import unicodedata\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# --- Parámetros de Configuración ---\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_SAMPLES = 2500 # Limitar el número de muestras para que la ejecución sea más rápida. Poner a None para usar el dataset completo.\n",
    "MAX_TOKEN_LENGTH = 128 # Max longitud para truncar/rellenar tokens.\n",
    "\n",
    "# --- Configuración de Dispositivo (GPU o CPU) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# --- Definición de Rutas Locales ---\n",
    "job_id = f\"local-bert-job-{int(time.time())}\"\n",
    "BASE_DIR = \"datos_locales\"\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"input\")\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, \"processed\", job_id)\n",
    "MODEL_OUTPUT_DIR = os.path.join(BASE_DIR, \"model_output\", job_id)\n",
    "\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "INPUT_EMBEDDINGS_FILENAME = \"text_embeddings.csv\"\n",
    "LOCAL_EMBEDDINGS_PATH = os.path.join(INPUT_DIR, INPUT_EMBEDDINGS_FILENAME)\n",
    "\n",
    "print(f\"\\nID de trabajo para esta ejecución: {job_id}\")\n",
    "print(f\"Ruta para embeddings generados: {LOCAL_EMBEDDINGS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_load_md",
   "metadata": {},
   "source": [
    "## 2. Carga, Análisis y Tokenización de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_load_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Cargando dataset '20 Newsgroups' desde Scikit-learn (100% offline)...\")\n",
    "# NOTA: La primera vez que se ejecute, scikit-learn puede descargar y guardar los datos en caché.\n",
    "# Después de eso, siempre se cargará localmente.\n",
    "\n",
    "# Cargamos tanto el conjunto de entrenamiento como el de prueba para tener más datos\n",
    "try:\n",
    "    # El parámetro 'remove' limpia los metadatos para que el modelo se centre en el contenido del texto.\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "    newsgroups_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: No se pudo cargar el dataset '20 Newsgroups'. Causa: {e}\")\n",
    "    print(\"Si es un error de red, ejecuta este notebook una vez en una máquina con internet para que se descargue y guarde en caché.\")\n",
    "    raise\n",
    "\n",
    "# Combinamos los datos en un solo DataFrame de pandas\n",
    "all_text = newsgroups_train.data + newsgroups_test.data\n",
    "all_targets = list(newsgroups_train.target) + list(newsgroups_test.target)\n",
    "label_names = newsgroups_train.target_names\n",
    "all_label_names = [label_names[i] for i in all_targets]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'text': all_text,\n",
    "    'label_name': all_label_names\n",
    "})\n",
    "\n",
    "# Tomar una muestra si se especificó para acelerar el proceso\n",
    "if 'MAX_SAMPLES' in locals() and MAX_SAMPLES is not None:\n",
    "    print(f\"Tomando una muestra aleatoria de {MAX_SAMPLES} registros.\")\n",
    "    df = df.sample(n=MAX_SAMPLES, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset cargado con {len(df)} filas.\")\n",
    "print(\"\\nPrimeras 5 filas:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDistribución de clases (en la muestra):\")\n",
    "print(df['label_name'].value_counts())\n",
    "\n",
    "# Cargar tokenizador de BERT (esto todavía necesita internet la primera vez que se ejecuta)\n",
    "print(f\"\\nCargando tokenizador: {BERT_MODEL_NAME}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "except (ConnectionError, OSError) as e:\n",
    "    print(f\"\\nERROR: No se pudo descargar el tokenizador. Causa: {e}\")\n",
    "    print(\"Si el problema es de red, descarga la carpeta del modelo 'bert-base-uncased' manualmente desde el Hub y cárgalo desde la ruta local.\")\n",
    "    raise\n",
    "\n",
    "# Medir longitud de tokens\n",
    "print(\"Analizando longitud de los textos en tokens...\")\n",
    "# Usamos tqdm para ver una barra de progreso, ya que puede tardar un poco\n",
    "df['token_length'] = [len(tokenizer.encode(text, max_length=512, truncation=True)) for text in tqdm(df['text'])]\n",
    "\n",
    "# Visualizar la distribución de la longitud de tokens\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['token_length'], bins=50, kde=True)\n",
    "plt.title('Distribución de la Longitud de Tokens por Texto')\n",
    "plt.xlabel('Longitud de Tokens')\n",
    "plt.ylabel('Frecuencia')\n",
    "# Usamos MAX_TOKEN_LENGTH definido en la primera celda\n",
    "plt.axvline(x=MAX_TOKEN_LENGTH, color='r', linestyle='--', label=f'Max Length = {MAX_TOKEN_LENGTH}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Longitud promedio de tokens: {df['token_length'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding_md",
   "metadata": {},
   "source": [
    "## 3. Generación de Embeddings con BERT\n",
    "Este es el paso más intensivo computacionalmente. Se recomienda usar una GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedding_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cargando modelo pre-entrenado: {BERT_MODEL_NAME}\")\n",
    "model = AutoModel.from_pretrained(BERT_MODEL_NAME).to(device)\n",
    "model.eval() # Poner el modelo en modo de evaluación\n",
    "\n",
    "def get_bert_embeddings(batch_text):\n",
    "    \"\"\"Tokeniza un lote de texto y obtiene el embedding [CLS] de BERT.\"\"\"\n",
    "    inputs = tokenizer(batch_text, padding=True, truncation=True, \n",
    "                       max_length=MAX_TOKEN_LENGTH, return_tensors='pt')\n",
    "    \n",
    "    # Mover tensores al dispositivo (GPU/CPU)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Usamos el embedding del token [CLS] (índice 0)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return cls_embeddings\n",
    "\n",
    "print(\"\\nGenerando embeddings... Esto puede tardar varios minutos.\")\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "\n",
    "# tqdm ofrece una barra de progreso\n",
    "for i in tqdm(range(0, len(df), batch_size)):\n",
    "    batch_df = df.iloc[i:i+batch_size]\n",
    "    batch_text = batch_df['text'].tolist()\n",
    "    embeddings = get_bert_embeddings(batch_text)\n",
    "    all_embeddings.append(embeddings)\n",
    "\n",
    "# Combinar todos los embeddings de los lotes\n",
    "final_embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "# Crear un DataFrame con los embeddings\n",
    "embedding_cols = [f'dim_{i}' for i in range(final_embeddings.shape[1])]\n",
    "df_embeddings = pd.DataFrame(final_embeddings, columns=embedding_cols)\n",
    "\n",
    "# Unir los embeddings con las etiquetas originales\n",
    "# Usamos 'label_name' como la etiqueta de texto que queremos predecir\n",
    "df_final = pd.concat([df[['label_name']].rename(columns={'label_name': 'label'}), df_embeddings], axis=1)\n",
    "\n",
    "# Guardar el resultado para los siguientes pasos\n",
    "df_final.to_csv(LOCAL_EMBEDDINGS_PATH, index=False)\n",
    "\n",
    "print(f\"\\n✓ Embeddings generados y guardados en: {LOCAL_EMBEDDINGS_PATH}\")\n",
    "print(\"Dimensiones del DataFrame final:\", df_final.shape)\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split_md",
   "metadata": {},
   "source": [
    "## 4. División y Codificación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Ejecutando Lógica de División y Codificación ---\")\n",
    "\n",
    "# 1. Cargar los datos de entrada (embeddings generados)\n",
    "print(f\"Cargando datos desde {LOCAL_EMBEDDINGS_PATH}\")\n",
    "df = pd.read_csv(LOCAL_EMBEDDINGS_PATH)\n",
    "print(f\"Datos cargados. {len(df)} filas.\")\n",
    "\n",
    "# 2. Codificar las etiquetas\n",
    "label_col_name = 'label'\n",
    "encoded_label_col = f\"{label_col_name}_encoded\"\n",
    "embedding_cols = [col for col in df.columns if col.startswith('dim_')]\n",
    "\n",
    "print(f\"Columnas de embedding detectadas: {len(embedding_cols)}\")\n",
    "print(f\"Columna de etiquetas: {label_col_name}\")\n",
    "\n",
    "# La lógica de filtrar clases con <10 instancias se mantiene por robustez\n",
    "label_counts = df[label_col_name].value_counts()\n",
    "valid_labels = label_counts[label_counts > 9].index\n",
    "df = df[df[label_col_name].isin(valid_labels)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Filtrado completado. Se conservaron {len(valid_labels)} clases con más de 9 muestras.\")\n",
    "\n",
    "print(\"Codificando etiquetas...\")\n",
    "label_encoder = LabelEncoder()\n",
    "df[encoded_label_col] = label_encoder.fit_transform(df[label_col_name])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Se detectaron y codificaron {num_classes} clases.\")\n",
    "print(\"Mapeo de etiquetas:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "X = df[embedding_cols].values\n",
    "y = df[encoded_label_col].values\n",
    "\n",
    "# 3. Dividir los datos\n",
    "print(\"Dividiendo los datos en conjuntos de entrenamiento, validación y prueba...\")\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.25, random_state=42, stratify=y_trainval)\n",
    "\n",
    "# 4. Guardar los conjuntos de datos en archivos locales\n",
    "test_features_path = os.path.join(PROCESSED_DIR, \"test_features.csv\")\n",
    "test_labels_path = os.path.join(PROCESSED_DIR, \"test_labels.csv\")\n",
    "pd.DataFrame(X_test, columns=embedding_cols).to_csv(test_features_path, index=False)\n",
    "pd.DataFrame(y_test, columns=[encoded_label_col]).to_csv(test_labels_path, index=False)\n",
    "\n",
    "# Guardar el codificador de etiquetas\n",
    "label_encoder_path = os.path.join(PROCESSED_DIR, \"label_encoder.pkl\")\n",
    "with open(label_encoder_path, \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(f\"\\n✓ Procesamiento completado exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_md",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento del Modelo (Local con XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e03378",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Ejecutando Entrenamiento del Modelo Local con XGBoost (GPU) ---\\n\")\n",
    "print(\"--- PASO 1: Búsqueda de Hiperparámetros con GridSearchCV (SIN Parada Temprana) ---\")\n",
    "\n",
    "# Modelo XGBoost configurado para usar GPU\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=num_classes,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    seed=42,\n",
    "    use_label_encoder=False,\n",
    "    tree_method='hist',  # Método de construcción de árboles compatible con GPU\n",
    "    device='cuda'        # Especifica usar GPU (XGBoost 2.0+)\n",
    "    # Para versiones anteriores usar: gpu_id=0, tree_method='gpu_hist'\n",
    ")\n",
    "\n",
    "# La parrilla de hiperparámetros\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [4, 6],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'n_estimators': [100, 150]\n",
    "}\n",
    "\n",
    "# Configuración de GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_log_loss',\n",
    "    n_jobs=1,  # Cambiado a 1 para GPU (evita conflictos de paralelización)\n",
    "    cv=3,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Lanzamos la búsqueda\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBúsqueda de hiperparámetros completada.\")\n",
    "print(f\"Mejores parámetros encontrados: {grid_search.best_params_}\")\n",
    "print(f\"Mejor score (neg_log_loss) en validación cruzada: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\n--- PASO 2: Entrenamiento del Modelo Final con los Mejores Parámetros y Parada Temprana ---\")\n",
    "\n",
    "# Obtenemos los mejores parámetros\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Modelo final con GPU\n",
    "final_model = xgb.XGBClassifier(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=num_classes,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    seed=42,\n",
    "    use_label_encoder=False,\n",
    "    tree_method='hist',  # Método compatible con GPU\n",
    "    device='cuda',       # GPU habilitada\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "# Entrenamiento con parada temprana\n",
    "final_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenamiento del modelo final completado.\")\n",
    "\n",
    "# Guardamos el modelo\n",
    "best_model = final_model\n",
    "LOCAL_MODEL_PATH = os.path.join(MODEL_OUTPUT_DIR, \"modelo_xgboost_gpu.pkl\")\n",
    "\n",
    "with open(LOCAL_MODEL_PATH, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(f\"\\n✓ Mejor modelo guardado en: {LOCAL_MODEL_PATH}\")\n",
    "\n",
    "# Opcional: Verificar si GPU está siendo utilizada\n",
    "import subprocess\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n--- Estado de GPU ---\")\n",
    "        print(\"GPU disponible y siendo utilizada por XGBoost\")\n",
    "    else:\n",
    "        print(\"\\nAdvertencia: No se pudo verificar el estado de GPU\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nAdvertencia: nvidia-smi no encontrado, verifica la instalación de CUDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_md",
   "metadata": {},
   "source": [
    "## 6. Evaluación del Modelo (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Ejecutando Evaluación del Modelo en el Conjunto de Prueba ---\")\n",
    "\n",
    "# 1. Cargar los artefactos de prueba\n",
    "X_test_eval = pd.read_csv(test_features_path).values\n",
    "y_test_eval = pd.read_csv(test_labels_path).values.flatten()\n",
    "\n",
    "with open(label_encoder_path, 'rb') as f:\n",
    "    label_encoder_eval = pickle.load(f)\n",
    "\n",
    "with open(LOCAL_MODEL_PATH, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "print(f\"Modelo y datos de prueba cargados: {X_test_eval.shape[0]} muestras.\")\n",
    "\n",
    "# 2. Obtener predicciones\n",
    "y_pred_eval = loaded_model.predict(X_test_eval)\n",
    "\n",
    "# 3. Calcular y mostrar métricas\n",
    "print(\"\\n--- Resultados de la Evaluación ---\")\n",
    "acc = accuracy_score(y_test_eval, y_pred_eval)\n",
    "report = classification_report(y_test_eval, y_pred_eval, target_names=label_encoder_eval.classes_)\n",
    "cm = confusion_matrix(y_test_eval, y_pred_eval)\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\\n\")\n",
    "print(\"Reporte de Clasificación:\")\n",
    "print(report)\n",
    "\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder_eval.classes_, yticklabels=label_encoder_eval.classes_)\n",
    "plt.ylabel('Etiqueta Real')\n",
    "plt.xlabel('Etiqueta Predicha')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Evaluación Completada ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
