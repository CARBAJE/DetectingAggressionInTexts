{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# Modelo de Clasificación Local de Extremo a Extremo\n",
    "\n",
    "Este notebook implementa un pipeline completo de Machine Learning de forma local:\n",
    "\n",
    "1.  **Carga de Datos**: Usa un dataset de texto real (`20 Newsgroups`) de forma local.\n",
    "2.  **Análisis y Tokenización**: Analiza la longitud de los textos usando un tokenizador de BERT.\n",
    "3.  **Generación de Embeddings**: Convierte el texto en vectores numéricos (embeddings) usando un modelo BERT pre-entrenado.\n",
    "4.  **Entrenamiento Multi-Modelo**: Entrena y optimiza tres modelos (XGBoost, MLP con PyTorch, Regresión Logística) usando **Optuna**.\n",
    "5.  **Creación de Ensemble**: Combina los tres modelos en un **ensemble ponderado** para mejorar la precisión.\n",
    "6.  **Evaluación**: Evalúa el rendimiento del modelo ensemble final.\n",
    "\n",
    "Todo el proceso se ejecuta localmente sin dependencias de la nube, aprovechando la GPU si está disponible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_md",
   "metadata": {},
   "source": [
    "## 1. Instalación y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch datasets scikit-learn xgboost pandas seaborn matplotlib tqdm optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa9ab72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64bit', 'WindowsPE')\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.architecture())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURACIÓN GENERAL ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import unicodedata\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss, f1_score\n",
    "\n",
    "# --- Parámetros de Configuración ---\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_SAMPLES = 2500 # Limitar el número de muestras para que la ejecución sea más rápida. Poner a None para usar el dataset completo.\n",
    "MAX_TOKEN_LENGTH = 128 # Max longitud para truncar/rellenar tokens.\n",
    "\n",
    "# --- Configuración de Dispositivo (GPU o CPU) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# --- Definición de Rutas Locales ---\n",
    "job_id = f\"local-ensemble-job-{int(time.time())}\"\n",
    "BASE_DIR = \"datos_locales\"\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"input\")\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, \"processed\", job_id)\n",
    "MODEL_OUTPUT_DIR = os.path.join(BASE_DIR, \"model_output\", job_id)\n",
    "\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "INPUT_EMBEDDINGS_FILENAME = \"text_embeddings.csv\"\n",
    "LOCAL_EMBEDDINGS_PATH = os.path.join(INPUT_DIR, INPUT_EMBEDDINGS_FILENAME)\n",
    "\n",
    "print(f\"\\nID de trabajo para esta ejecución: {job_id}\")\n",
    "print(f\"Ruta para embeddings generados: {LOCAL_EMBEDDINGS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_load_md",
   "metadata": {},
   "source": [
    "## 2. Carga, Análisis y Tokenización de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_load_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Cargando dataset '20 Newsgroups' desde Scikit-learn (100% offline)...\")\n",
    "# NOTA: La primera vez que se ejecute, scikit-learn puede descargar y guardar los datos en caché.\n",
    "# Después de eso, siempre se cargará localmente.\n",
    "\n",
    "# Cargamos tanto el conjunto de entrenamiento como el de prueba para tener más datos\n",
    "try:\n",
    "    # El parámetro 'remove' limpia los metadatos para que el modelo se centre en el contenido del texto.\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "    newsgroups_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: No se pudo cargar el dataset '20 Newsgroups'. Causa: {e}\")\n",
    "    print(\"Si es un error de red, ejecuta este notebook una vez en una máquina con internet para que se descargue y guarde en caché.\")\n",
    "    raise\n",
    "\n",
    "# Combinamos los datos en un solo DataFrame de pandas\n",
    "all_text = newsgroups_train.data + newsgroups_test.data\n",
    "all_targets = list(newsgroups_train.target) + list(newsgroups_test.target)\n",
    "label_names = newsgroups_train.target_names\n",
    "all_label_names = [label_names[i] for i in all_targets]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'text': all_text,\n",
    "    'label_name': all_label_names\n",
    "})\n",
    "\n",
    "# Tomar una muestra si se especificó para acelerar el proceso\n",
    "if 'MAX_SAMPLES' in locals() and MAX_SAMPLES is not None:\n",
    "    print(f\"Tomando una muestra aleatoria de {MAX_SAMPLES} registros.\")\n",
    "    df = df.sample(n=MAX_SAMPLES, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset cargado con {len(df)} filas.\")\n",
    "print(\"\\nPrimeras 5 filas:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDistribución de clases (en la muestra):\")\n",
    "print(df['label_name'].value_counts())\n",
    "\n",
    "# Cargar tokenizador de BERT (esto todavía necesita internet la primera vez que se ejecuta)\n",
    "print(f\"\\nCargando tokenizador: {BERT_MODEL_NAME}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "except (ConnectionError, OSError) as e:\n",
    "    print(f\"\\nERROR: No se pudo descargar el tokenizador. Causa: {e}\")\n",
    "    print(\"Si el problema es de red, descarga la carpeta del modelo 'bert-base-uncased' manualmente desde el Hub y cárgalo desde la ruta local.\")\n",
    "    raise\n",
    "\n",
    "# Medir longitud de tokens\n",
    "print(\"Analizando longitud de los textos en tokens...\")\n",
    "# Usamos tqdm para ver una barra de progreso, ya que puede tardar un poco\n",
    "df['token_length'] = [len(tokenizer.encode(text, max_length=512, truncation=True)) for text in tqdm(df['text'])]\n",
    "\n",
    "# Visualizar la distribución de la longitud de tokens\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['token_length'], bins=50, kde=True)\n",
    "plt.title('Distribución de la Longitud de Tokens por Texto')\n",
    "plt.xlabel('Longitud de Tokens')\n",
    "plt.ylabel('Frecuencia')\n",
    "# Usamos MAX_TOKEN_LENGTH definido en la primera celda\n",
    "plt.axvline(x=MAX_TOKEN_LENGTH, color='r', linestyle='--', label=f'Max Length = {MAX_TOKEN_LENGTH}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Longitud promedio de tokens: {df['token_length'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding_md",
   "metadata": {},
   "source": [
    "## 3. Generación de Embeddings con BERT\n",
    "Este es el paso más intensivo computacionalmente. Se recomienda usar una GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedding_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cargando modelo pre-entrenado: {BERT_MODEL_NAME}\")\n",
    "model = AutoModel.from_pretrained(BERT_MODEL_NAME).to(device)\n",
    "model.eval() # Poner el modelo en modo de evaluación\n",
    "\n",
    "def get_bert_embeddings(batch_text):\n",
    "    \"\"\"Tokeniza un lote de texto y obtiene el embedding [CLS] de BERT.\"\"\"\n",
    "    inputs = tokenizer(batch_text, padding=True, truncation=True, \n",
    "                       max_length=MAX_TOKEN_LENGTH, return_tensors='pt')\n",
    "    \n",
    "    # Mover tensores al dispositivo (GPU/CPU)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Usamos el embedding del token [CLS] (índice 0)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return cls_embeddings\n",
    "\n",
    "print(\"\\nGenerando embeddings... Esto puede tardar varios minutos.\")\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "\n",
    "# tqdm ofrece una barra de progreso\n",
    "for i in tqdm(range(0, len(df), batch_size)):\n",
    "    batch_df = df.iloc[i:i+batch_size]\n",
    "    batch_text = batch_df['text'].tolist()\n",
    "    embeddings = get_bert_embeddings(batch_text)\n",
    "    all_embeddings.append(embeddings)\n",
    "\n",
    "# Combinar todos los embeddings de los lotes\n",
    "final_embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "# Crear un DataFrame con los embeddings\n",
    "embedding_cols = [f'dim_{i}' for i in range(final_embeddings.shape[1])]\n",
    "df_embeddings = pd.DataFrame(final_embeddings, columns=embedding_cols)\n",
    "\n",
    "# Unir los embeddings con las etiquetas originales\n",
    "# Usamos 'label_name' como la etiqueta de texto que queremos predecir\n",
    "df_final = pd.concat([df[['label_name']].rename(columns={'label_name': 'label'}), df_embeddings], axis=1)\n",
    "\n",
    "# Guardar el resultado para los siguientes pasos\n",
    "df_final.to_csv(LOCAL_EMBEDDINGS_PATH, index=False)\n",
    "\n",
    "print(f\"\\n✓ Embeddings generados y guardados en: {LOCAL_EMBEDDINGS_PATH}\")\n",
    "print(\"Dimensiones del DataFrame final:\", df_final.shape)\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split_md",
   "metadata": {},
   "source": [
    "## 4. División y Codificación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Ejecutando Lógica de División y Codificación ---\")\n",
    "\n",
    "# 1. Cargar los datos de entrada (embeddings generados)\n",
    "print(f\"Cargando datos desde {LOCAL_EMBEDDINGS_PATH}\")\n",
    "df = pd.read_csv(LOCAL_EMBEDDINGS_PATH)\n",
    "print(f\"Datos cargados. {len(df)} filas.\")\n",
    "\n",
    "# 2. Codificar las etiquetas\n",
    "label_col_name = 'label'\n",
    "encoded_label_col = f\"{label_col_name}_encoded\"\n",
    "embedding_cols = [col for col in df.columns if col.startswith('dim_')]\n",
    "\n",
    "print(f\"Columnas de embedding detectadas: {len(embedding_cols)}\")\n",
    "print(f\"Columna de etiquetas: {label_col_name}\")\n",
    "\n",
    "# La lógica de filtrar clases con <10 instancias se mantiene por robustez\n",
    "label_counts = df[label_col_name].value_counts()\n",
    "valid_labels = label_counts[label_counts > 9].index\n",
    "df = df[df[label_col_name].isin(valid_labels)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Filtrado completado. Se conservaron {len(valid_labels)} clases con más de 9 muestras.\")\n",
    "\n",
    "print(\"Codificando etiquetas...\")\n",
    "label_encoder = LabelEncoder()\n",
    "df[encoded_label_col] = label_encoder.fit_transform(df[label_col_name])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Se detectaron y codificaron {num_classes} clases.\")\n",
    "print(\"Mapeo de etiquetas:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "X = df[embedding_cols].values\n",
    "y = df[encoded_label_col].values\n",
    "\n",
    "# 3. Dividir los datos\n",
    "print(\"Dividiendo los datos en conjuntos de entrenamiento, validación y prueba...\")\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.25, random_state=42, stratify=y_trainval)\n",
    "print(f\"Tamaño Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "\n",
    "# Guardar el codificador y los datos de prueba para la evaluación final\n",
    "label_encoder_path = os.path.join(MODEL_OUTPUT_DIR, \"label_encoder.pkl\")\n",
    "with open(label_encoder_path, \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "test_features_path = os.path.join(PROCESSED_DIR, \"test_features.pkl\")\n",
    "test_labels_path = os.path.join(PROCESSED_DIR, \"test_labels.pkl\")\n",
    "with open(test_features_path, 'wb') as f: pickle.dump(X_test, f)\n",
    "with open(test_labels_path, 'wb') as f: pickle.dump(y_test, f)\n",
    "\n",
    "print(f\"\\n✓ Procesamiento completado exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_train_md_1",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento Multi-Modelo con Optuna\n",
    "\n",
    "En esta sección, optimizaremos tres modelos diferentes usando Optuna para encontrar los mejores hiperparámetros. Luego, los combinaremos en un ensemble ponderado para maximizar el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_train_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"--- Preparando datos para el entrenamiento ---\")\n",
    "# Algunos modelos (MLP, Regresión Logística) se benefician del escalado de características.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Convertimos los datos de validación a tensores para PyTorch una sola vez\n",
    "X_val_torch = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "y_val_torch = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "print(\"Datos escalados y tensores de PyTorch listos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_train_md_2",
   "metadata": {},
   "source": [
    "### 5.1 Definición de las Funciones Objetivo para Optuna\n",
    "\n",
    "Cada función `objective` define cómo Optuna debe entrenar y evaluar un modelo para un conjunto de hiperparámetros (`trial`). El objetivo es minimizar la métrica `log_loss` en el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_train_objectives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Objetivo para MLP con PyTorch ---\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size, activation_fn, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(current_size, hidden_size))\n",
    "            layers.append(activation_fn())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            current_size = hidden_size\n",
    "        layers.append(nn.Linear(current_size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def objective_mlp(trial):\n",
    "    # Hiperparámetros a optimizar\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    hidden_layers = [trial.suggest_int(f'n_units_l{i}', 64, 512) for i in range(n_layers)]\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    activation_name = trial.suggest_categorical('activation', ['ReLU', 'Tanh'])\n",
    "    activation_fn = getattr(nn, activation_name)\n",
    "    \n",
    "    # Modelo, datos y lógica se mueven al dispositivo (GPU/CPU)\n",
    "    model = MLP(X_train_scaled.shape[1], hidden_layers, num_classes, activation_fn, dropout_rate).to(device)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience, patience_counter = 5, 0\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            # Mover cada lote al dispositivo\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # El tensor de validación ya está en el dispositivo\n",
    "            val_outputs = model(X_val_torch)\n",
    "            val_loss = criterion(val_outputs, y_val_torch).item()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience: break\n",
    "        \n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# --- 2. Objetivo para XGBoost (GPU Habilitado) ---\n",
    "def objective_xgboost(trial):\n",
    "    params = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': num_classes,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'verbosity': 0,\n",
    "        'seed': 42,\n",
    "        # Configuración explícita para usar GPU si está disponible\n",
    "        'tree_method': 'hist', \n",
    "        'device': 'cuda' if device.type == 'cuda' else 'cpu',\n",
    "        # Hiperparámetros\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10, step=1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params, early_stopping_rounds=15, use_label_encoder=False)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, y_pred_proba)\n",
    "\n",
    "# --- 3. Objetivo para Regresión Logística (CPU Paralelizado) ---\n",
    "def objective_logistic(trial):\n",
    "    # Nota: Este modelo se ejecuta en CPU. 'n_jobs=-1' usa todos los núcleos.\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 1e-4, 1e2, log=True),\n",
    "        'solver': 'saga',\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet']),\n",
    "        'max_iter': 1000,\n",
    "        'random_state': 42,\n",
    "        'multi_class': 'multinomial',\n",
    "        'n_jobs': -1 # Usar todos los núcleos de CPU disponibles\n",
    "    }\n",
    "    if params['penalty'] == 'elasticnet':\n",
    "        params['l1_ratio'] = trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "    \n",
    "    model = LogisticRegression(**params)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_val_scaled)\n",
    "    return log_loss(y_val, y_pred_proba)\n",
    "\n",
    "print(f\"Funciones objetivo de Optuna definidas. Dispositivo de cómputo principal: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_train_md_3",
   "metadata": {},
   "source": [
    "### 5.2 Ejecución de la Búsqueda de Hiperparámetros\n",
    "Lanzamos los estudios de Optuna para cada modelo. Esto puede tardar un tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_train_hpo_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    'XGBoost': {'objective_func': objective_xgboost, 'n_trials': 40},\n",
    "    'MLP_PyTorch': {'objective_func': objective_mlp, 'n_trials': 50},\n",
    "    'LogisticRegression': {'objective_func': objective_logistic, 'n_trials': 30}\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model_name, config in models_config.items():\n",
    "    print(f\"\\n--- Optimizando {model_name} ---\")\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "    )\n",
    "    study.optimize(config['objective_func'], n_trials=config['n_trials'], show_progress_bar=True)\n",
    "    \n",
    "    model_results[model_name] = {\n",
    "        'best_params': study.best_params,\n",
    "        'best_score': study.best_value\n",
    "    }\n",
    "    print(f\"✓ {model_name} completado. Mejor LogLoss: {study.best_value:.4f}\")\n",
    "    print(f\"  Mejores parámetros: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_train_md_4",
   "metadata": {},
   "source": [
    "### 5.3 Entrenamiento de los Modelos Finales\n",
    "\n",
    "Ahora entrenamos cada modelo una última vez usando el conjunto completo de entrenamiento (`X_train`, `y_train`) y los mejores hiperparámetros encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_train_final_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = {}\n",
    "\n",
    "print(f\"--- Entrenando modelos finales con los mejores hiperparámetros en el dispositivo: {device} ---\")\n",
    "\n",
    "# 1. XGBoost (Entrenamiento final en GPU)\n",
    "print(\"Entrenando XGBoost final en GPU...\")\n",
    "xgb_params = model_results['XGBoost']['best_params']\n",
    "final_xgb = xgb.XGBClassifier(\n",
    "    objective='multi:softprob', num_class=num_classes, eval_metric='mlogloss',\n",
    "    seed=42, use_label_encoder=False,\n",
    "    # Configuración explícita para GPU\n",
    "    tree_method='hist',\n",
    "    device='cuda' if device.type == 'cuda' else 'cpu',\n",
    "    early_stopping_rounds=15,\n",
    "    **xgb_params\n",
    ")\n",
    "final_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "trained_models['XGBoost'] = final_xgb\n",
    "print(\"✓ Modelo XGBoost final entrenado.\")\n",
    "\n",
    "# 2. Regresión Logística (Entrenamiento final en CPU)\n",
    "print(\"\\nEntrenando Regresión Logística final en CPU (paralelizado)...\")\n",
    "log_params = model_results['LogisticRegression']['best_params']\n",
    "if 'l1_ratio' not in log_params and log_params.get('penalty') == 'elasticnet':\n",
    "    log_params['l1_ratio'] = 0.5\n",
    "final_log = LogisticRegression(\n",
    "    solver='saga', max_iter=2000, random_state=42, multi_class='multinomial', \n",
    "    n_jobs=-1, # Usar todos los núcleos de CPU\n",
    "    **log_params\n",
    ")\n",
    "final_log.fit(X_train_scaled, y_train)\n",
    "trained_models['LogisticRegression'] = final_log\n",
    "print(\"✓ Modelo de Regresión Logística final entrenado.\")\n",
    "\n",
    "\n",
    "# 3. MLP con PyTorch (Entrenamiento final en GPU)\n",
    "print(\"\\nEntrenando MLP (PyTorch) final en GPU...\")\n",
    "mlp_params = model_results['MLP_PyTorch']['best_params']\n",
    "hidden_layers = [mlp_params[f'n_units_l{i}'] for i in range(mlp_params['n_layers'])]\n",
    "# Mover el modelo al dispositivo (GPU/CPU)\n",
    "final_mlp = MLP(\n",
    "    input_size=X_train_scaled.shape[1],\n",
    "    hidden_layers=hidden_layers,\n",
    "    output_size=num_classes,\n",
    "    activation_fn=getattr(nn, mlp_params['activation']),\n",
    "    dropout_rate=mlp_params['dropout_rate']\n",
    ").to(device)\n",
    "\n",
    "optimizer = getattr(optim, mlp_params['optimizer'])(final_mlp.parameters(), lr=mlp_params['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "for epoch in tqdm(range(60), desc=\"Epochs MLP final\"):\n",
    "    final_mlp.train()\n",
    "    for data, target in train_loader:\n",
    "        # Mover cada lote a la GPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = final_mlp(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "trained_models['MLP_PyTorch'] = final_mlp\n",
    "print(\"✓ Modelo MLP (PyTorch) final entrenado.\")\n",
    "\n",
    "print(\"\\n✓ Todos los modelos finales han sido entrenados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_train_md_5",
   "metadata": {},
   "source": [
    "### 5.4 Creación y Evaluación del Ensemble Ponderado\n",
    "\n",
    "Definimos un clasificador `Ensemble` que combina las predicciones de los modelos. Los pesos se calculan en base al rendimiento de cada modelo en el conjunto de validación: un menor `log_loss` resulta en un mayor peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_train_ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedEnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, models, weights, scaler=None, device_str='cpu'):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "        self.scaler = scaler\n",
    "        self.device = torch.device(device_str)\n",
    "        self.classes_ = None\n",
    "        \n",
    "        # Verificar si CuPy está disponible para el uso de la GPU en XGBoost\n",
    "        self.gpu_available = False\n",
    "        if self.device.type == 'cuda':\n",
    "            try:\n",
    "                import cupy as cp\n",
    "                self.gpu_available = True\n",
    "            except ImportError:\n",
    "                self.gpu_available = False\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        ensemble_proba = np.zeros((X.shape[0], len(self.classes_)))\n",
    "        X_scaled = self.scaler.transform(X) if self.scaler else X\n",
    "\n",
    "        for name, model in self.models.items():\n",
    "            if name == 'XGBoost':\n",
    "                if self.gpu_available:\n",
    "                    import cupy as cp\n",
    "                    X_gpu = cp.asarray(X)\n",
    "                    proba = cp.asnumpy(model.predict_proba(X_gpu))\n",
    "                else:\n",
    "                    proba = model.predict_proba(X)\n",
    "            elif name in ['MLP_Scikit', 'LogisticRegression']:\n",
    "                proba = model.predict_proba(X_scaled)\n",
    "            \n",
    "            ensemble_proba += proba * self.weights[name]\n",
    "        return ensemble_proba\n",
    "\n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probas, axis=1)]\n",
    "\n",
    "# --- CÓDIGO CORREGIDO ---\n",
    "\n",
    "# CORRECCIÓN: Volver a verificar la disponibilidad de GPU al inicio de la celda\n",
    "gpu_available = False\n",
    "if device.type == 'cuda':\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        gpu_available = True\n",
    "    except ImportError:\n",
    "        gpu_available = False\n",
    "\n",
    "\n",
    "print(\"--- Evaluando modelos individuales en el conjunto de validación ---\")\n",
    "individual_metrics = {}\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    if name == 'XGBoost':\n",
    "        if gpu_available:\n",
    "            import cupy as cp\n",
    "            X_val_gpu = cp.asarray(X_val)\n",
    "            proba = cp.asnumpy(model.predict_proba(X_val_gpu))\n",
    "            pred = cp.asnumpy(model.predict(X_val_gpu))\n",
    "        else:\n",
    "            proba = model.predict_proba(X_val)\n",
    "            pred = model.predict(X_val)\n",
    "    elif name in ['MLP_Scikit', 'LogisticRegression']:\n",
    "        proba = model.predict_proba(X_val_scaled)\n",
    "        pred = model.predict(X_val_scaled)\n",
    "    \n",
    "    loss = log_loss(y_val, proba)\n",
    "    acc = accuracy_score(y_val, pred)\n",
    "    f1 = f1_score(y_val, pred, average='weighted')\n",
    "    \n",
    "    individual_metrics[name] = {'log_loss': loss, 'accuracy': acc, 'f1_score': f1}\n",
    "    print(f\"Modelo: {name:<20} | LogLoss: {loss:.4f} | Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Calcular pesos\n",
    "losses = {name: metrics['log_loss'] for name, metrics in individual_metrics.items()}\n",
    "scores = {name: 1.0 / (loss + 1e-9) for name, loss in losses.items()}\n",
    "total_score = sum(scores.values())\n",
    "weights = {name: score / total_score for name, score in scores.items()}\n",
    "\n",
    "print(\"\\n--- Pesos del Ensemble Calculados ---\")\n",
    "for name, w in weights.items(): print(f\"{name:<20} | Peso: {w:.3f} | LogLoss (Val): {losses[name]:.4f}\")\n",
    "\n",
    "# Crear el ensemble final\n",
    "ensemble_model = WeightedEnsembleClassifier(trained_models, weights, scaler, device.type)\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el ensemble\n",
    "ensemble_proba = ensemble_model.predict_proba(X_val)\n",
    "ensemble_pred = ensemble_model.predict(X_val)\n",
    "ensemble_log_loss = log_loss(y_val, ensemble_proba)\n",
    "ensemble_accuracy = accuracy_score(y_val, ensemble_pred)\n",
    "ensemble_f1 = f1_score(y_val, ensemble_pred, average='weighted')\n",
    "\n",
    "print(\"\\n--- Comparación de Rendimiento (Validación) ---\")\n",
    "print(f\"{'Modelo':<20} | {'Log Loss':<10} | {'Accuracy':<10} | {'F1-Score':<10}\")\n",
    "print(\"=\"*60)\n",
    "for name, metrics in individual_metrics.items():\n",
    "    print(f\"{name:<20} | {metrics['log_loss']:<10.4f} | {metrics['accuracy']:<10.4f} | {metrics['f1_score']:<10.4f}\")\n",
    "print(\"—\"*60)\n",
    "print(f\"{'🏆 ENSEMBLE':<20} | {ensemble_log_loss:<10.4f} | {ensemble_accuracy:<10.4f} | {ensemble_f1:<10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_md",
   "metadata": {},
   "source": [
    "## 6. Evaluación Final del Modelo Ensemble (en Conjunto de Prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Ejecutando Evaluación Final del Ensemble en el Conjunto de Prueba ---\")\n",
    "\n",
    "# 1. Cargar los datos de prueba y el codificador de etiquetas\n",
    "with open(test_features_path, 'rb') as f: X_test_eval = pickle.load(f)\n",
    "with open(test_labels_path, 'rb') as f: y_test_eval = pickle.load(f)\n",
    "with open(label_encoder_path, 'rb') as f: label_encoder_eval = pickle.load(f)\n",
    "print(f\"Datos de prueba cargados: {X_test_eval.shape[0]} muestras.\")\n",
    "\n",
    "# 2. Obtener predicciones del modelo ensemble\n",
    "y_pred_eval = ensemble_model.predict(X_test_eval)\n",
    "\n",
    "# 3. Calcular y mostrar métricas\n",
    "print(\"\\n--- Resultados de la Evaluación Final ---\")\n",
    "acc = accuracy_score(y_test_eval, y_pred_eval)\n",
    "report = classification_report(y_test_eval, y_pred_eval, target_names=label_encoder_eval.classes_)\n",
    "cm = confusion_matrix(y_test_eval, y_pred_eval)\n",
    "\n",
    "print(f\"Accuracy en el conjunto de prueba: {acc:.4f}\\n\")\n",
    "print(\"Reporte de Clasificación:\")\n",
    "print(report)\n",
    "\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder_eval.classes_, yticklabels=label_encoder_eval.classes_)\n",
    "plt.ylabel('Etiqueta Real')\n",
    "plt.xlabel('Etiqueta Predicha')\n",
    "plt.title('Matriz de Confusión del Modelo Ensemble (Prueba)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Evaluación Completada ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_train_md_6",
   "metadata": {},
   "source": [
    "## 7. Guardado de Artefactos del Modelo\n",
    "\n",
    "Guardamos el modelo ensemble final, los modelos individuales, el escalador y los resultados de la optimización para su uso futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_train_save",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Guardando artefactos en {MODEL_OUTPUT_DIR} ---\")\n",
    "\n",
    "# 1. Guardar el modelo ensemble completo\n",
    "ensemble_path = os.path.join(MODEL_OUTPUT_DIR, \"ensemble_model.pkl\")\n",
    "with open(ensemble_path, 'wb') as f:\n",
    "    pickle.dump(ensemble_model, f)\n",
    "print(f\"✓ Modelo ensemble guardado en: {ensemble_path}\")\n",
    "\n",
    "# 2. Guardar los modelos individuales\n",
    "# Para PyTorch, guardamos el state_dict, que es más robusto\n",
    "mlp_path = os.path.join(MODEL_OUTPUT_DIR, \"mlp_pytorch.pth\")\n",
    "torch.save(trained_models['MLP_PyTorch'].state_dict(), mlp_path)\n",
    "print(f\"✓ Modelo MLP (PyTorch) guardado en: {mlp_path}\")\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    if name != 'MLP_PyTorch':\n",
    "        model_path = os.path.join(MODEL_OUTPUT_DIR, f\"{name.lower()}_model.pkl\")\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"✓ Modelo {name} guardado en: {model_path}\")\n",
    "\n",
    "# 3. Guardar el escalador\n",
    "scaler_path = os.path.join(MODEL_OUTPUT_DIR, \"scaler.pkl\")\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"✓ Scaler guardado en: {scaler_path}\")\n",
    "\n",
    "# 4. Guardar los resultados de Optuna\n",
    "results_path = os.path.join(MODEL_OUTPUT_DIR, \"optuna_results.pkl\")\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump(model_results, f)\n",
    "print(f\"✓ Resultados de Optuna guardados en: {results_path}\")\n",
    "\n",
    "print(\"\\n🎉 Pipeline completado y todos los artefactos han sido guardados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
