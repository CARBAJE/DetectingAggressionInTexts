{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# Modelo de Clasificaci√≥n Local de Extremo a Extremo\n",
    "\n",
    "Este notebook implementa un pipeline completo de Machine Learning de forma local:\n",
    "\n",
    "1.  **Carga de Datos**: Usa un dataset de texto real (`20 Newsgroups`) de forma local.\n",
    "2.  **An√°lisis y Tokenizaci√≥n**: Analiza la longitud de los textos usando un tokenizador de BERT.\n",
    "3.  **Generaci√≥n de Embeddings**: Convierte el texto en vectores num√©ricos (embeddings) usando un modelo BERT pre-entrenado.\n",
    "4.  **Entrenamiento Multi-Modelo**: Entrena y optimiza tres modelos (XGBoost, MLP con PyTorch, Regresi√≥n Log√≠stica) usando **Optuna**.\n",
    "5.  **Creaci√≥n de Ensemble**: Combina los tres modelos en un **ensemble ponderado** para mejorar la precisi√≥n.\n",
    "6.  **Evaluaci√≥n**: Eval√∫a el rendimiento del modelo ensemble final.\n",
    "\n",
    "Todo el proceso se ejecuta localmente sin dependencias de la nube, aprovechando la GPU si est√° disponible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_md",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n y Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch datasets scikit-learn xgboost pandas seaborn matplotlib tqdm optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa9ab72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64bit', 'WindowsPE')\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.architecture())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURACI√ìN GENERAL ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import unicodedata\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss, f1_score\n",
    "\n",
    "# --- Par√°metros de Configuraci√≥n ---\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_SAMPLES = 2500 # Limitar el n√∫mero de muestras para que la ejecuci√≥n sea m√°s r√°pida. Poner a None para usar el dataset completo.\n",
    "MAX_TOKEN_LENGTH = 128 # Max longitud para truncar/rellenar tokens.\n",
    "\n",
    "# --- Configuraci√≥n de Dispositivo (GPU o CPU) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# --- Definici√≥n de Rutas Locales ---\n",
    "job_id = f\"local-ensemble-job-{int(time.time())}\"\n",
    "BASE_DIR = \"datos_locales\"\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"input\")\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, \"processed\", job_id)\n",
    "MODEL_OUTPUT_DIR = os.path.join(BASE_DIR, \"model_output\", job_id)\n",
    "\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "INPUT_EMBEDDINGS_FILENAME = \"text_embeddings.csv\"\n",
    "LOCAL_EMBEDDINGS_PATH = os.path.join(INPUT_DIR, INPUT_EMBEDDINGS_FILENAME)\n",
    "\n",
    "print(f\"\\nID de trabajo para esta ejecuci√≥n: {job_id}\")\n",
    "print(f\"Ruta para embeddings generados: {LOCAL_EMBEDDINGS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_load_md",
   "metadata": {},
   "source": [
    "## 2. Carga, An√°lisis y Tokenizaci√≥n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_load_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Cargando dataset '20 Newsgroups' desde Scikit-learn (100% offline)...\")\n",
    "# NOTA: La primera vez que se ejecute, scikit-learn puede descargar y guardar los datos en cach√©.\n",
    "# Despu√©s de eso, siempre se cargar√° localmente.\n",
    "\n",
    "# Cargamos tanto el conjunto de entrenamiento como el de prueba para tener m√°s datos\n",
    "try:\n",
    "    # El par√°metro 'remove' limpia los metadatos para que el modelo se centre en el contenido del texto.\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "    newsgroups_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: No se pudo cargar el dataset '20 Newsgroups'. Causa: {e}\")\n",
    "    print(\"Si es un error de red, ejecuta este notebook una vez en una m√°quina con internet para que se descargue y guarde en cach√©.\")\n",
    "    raise\n",
    "\n",
    "# Combinamos los datos en un solo DataFrame de pandas\n",
    "all_text = newsgroups_train.data + newsgroups_test.data\n",
    "all_targets = list(newsgroups_train.target) + list(newsgroups_test.target)\n",
    "label_names = newsgroups_train.target_names\n",
    "all_label_names = [label_names[i] for i in all_targets]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'text': all_text,\n",
    "    'label_name': all_label_names\n",
    "})\n",
    "\n",
    "# Tomar una muestra si se especific√≥ para acelerar el proceso\n",
    "if 'MAX_SAMPLES' in locals() and MAX_SAMPLES is not None:\n",
    "    print(f\"Tomando una muestra aleatoria de {MAX_SAMPLES} registros.\")\n",
    "    df = df.sample(n=MAX_SAMPLES, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset cargado con {len(df)} filas.\")\n",
    "print(\"\\nPrimeras 5 filas:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDistribuci√≥n de clases (en la muestra):\")\n",
    "print(df['label_name'].value_counts())\n",
    "\n",
    "# Cargar tokenizador de BERT (esto todav√≠a necesita internet la primera vez que se ejecuta)\n",
    "print(f\"\\nCargando tokenizador: {BERT_MODEL_NAME}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "except (ConnectionError, OSError) as e:\n",
    "    print(f\"\\nERROR: No se pudo descargar el tokenizador. Causa: {e}\")\n",
    "    print(\"Si el problema es de red, descarga la carpeta del modelo 'bert-base-uncased' manualmente desde el Hub y c√°rgalo desde la ruta local.\")\n",
    "    raise\n",
    "\n",
    "# Medir longitud de tokens\n",
    "print(\"Analizando longitud de los textos en tokens...\")\n",
    "# Usamos tqdm para ver una barra de progreso, ya que puede tardar un poco\n",
    "df['token_length'] = [len(tokenizer.encode(text, max_length=512, truncation=True)) for text in tqdm(df['text'])]\n",
    "\n",
    "# Visualizar la distribuci√≥n de la longitud de tokens\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['token_length'], bins=50, kde=True)\n",
    "plt.title('Distribuci√≥n de la Longitud de Tokens por Texto')\n",
    "plt.xlabel('Longitud de Tokens')\n",
    "plt.ylabel('Frecuencia')\n",
    "# Usamos MAX_TOKEN_LENGTH definido en la primera celda\n",
    "plt.axvline(x=MAX_TOKEN_LENGTH, color='r', linestyle='--', label=f'Max Length = {MAX_TOKEN_LENGTH}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Longitud promedio de tokens: {df['token_length'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding_md",
   "metadata": {},
   "source": [
    "## 3. Generaci√≥n de Embeddings con BERT\n",
    "Este es el paso m√°s intensivo computacionalmente. Se recomienda usar una GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedding_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cargando modelo pre-entrenado: {BERT_MODEL_NAME}\")\n",
    "model = AutoModel.from_pretrained(BERT_MODEL_NAME).to(device)\n",
    "model.eval() # Poner el modelo en modo de evaluaci√≥n\n",
    "\n",
    "def get_bert_embeddings(batch_text):\n",
    "    \"\"\"Tokeniza un lote de texto y obtiene el embedding [CLS] de BERT.\"\"\"\n",
    "    inputs = tokenizer(batch_text, padding=True, truncation=True, \n",
    "                       max_length=MAX_TOKEN_LENGTH, return_tensors='pt')\n",
    "    \n",
    "    # Mover tensores al dispositivo (GPU/CPU)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Usamos el embedding del token [CLS] (√≠ndice 0)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return cls_embeddings\n",
    "\n",
    "print(\"\\nGenerando embeddings... Esto puede tardar varios minutos.\")\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "\n",
    "# tqdm ofrece una barra de progreso\n",
    "for i in tqdm(range(0, len(df), batch_size)):\n",
    "    batch_df = df.iloc[i:i+batch_size]\n",
    "    batch_text = batch_df['text'].tolist()\n",
    "    embeddings = get_bert_embeddings(batch_text)\n",
    "    all_embeddings.append(embeddings)\n",
    "\n",
    "# Combinar todos los embeddings de los lotes\n",
    "final_embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "# Crear un DataFrame con los embeddings\n",
    "embedding_cols = [f'dim_{i}' for i in range(final_embeddings.shape[1])]\n",
    "df_embeddings = pd.DataFrame(final_embeddings, columns=embedding_cols)\n",
    "\n",
    "# Unir los embeddings con las etiquetas originales\n",
    "# Usamos 'label_name' como la etiqueta de texto que queremos predecir\n",
    "df_final = pd.concat([df[['label_name']].rename(columns={'label_name': 'label'}), df_embeddings], axis=1)\n",
    "\n",
    "# Guardar el resultado para los siguientes pasos\n",
    "df_final.to_csv(LOCAL_EMBEDDINGS_PATH, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Embeddings generados y guardados en: {LOCAL_EMBEDDINGS_PATH}\")\n",
    "print(\"Dimensiones del DataFrame final:\", df_final.shape)\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split_md",
   "metadata": {},
   "source": [
    "## 4. Divisi√≥n y Codificaci√≥n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Ejecutando L√≥gica de Divisi√≥n y Codificaci√≥n ---\")\n",
    "\n",
    "# 1. Cargar los datos de entrada (embeddings generados)\n",
    "print(f\"Cargando datos desde {LOCAL_EMBEDDINGS_PATH}\")\n",
    "df = pd.read_csv(LOCAL_EMBEDDINGS_PATH)\n",
    "print(f\"Datos cargados. {len(df)} filas.\")\n",
    "\n",
    "# 2. Codificar las etiquetas\n",
    "label_col_name = 'label'\n",
    "encoded_label_col = f\"{label_col_name}_encoded\"\n",
    "embedding_cols = [col for col in df.columns if col.startswith('dim_')]\n",
    "\n",
    "print(f\"Columnas de embedding detectadas: {len(embedding_cols)}\")\n",
    "print(f\"Columna de etiquetas: {label_col_name}\")\n",
    "\n",
    "# La l√≥gica de filtrar clases con <10 instancias se mantiene por robustez\n",
    "label_counts = df[label_col_name].value_counts()\n",
    "valid_labels = label_counts[label_counts > 9].index\n",
    "df = df[df[label_col_name].isin(valid_labels)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Filtrado completado. Se conservaron {len(valid_labels)} clases con m√°s de 9 muestras.\")\n",
    "\n",
    "print(\"Codificando etiquetas...\")\n",
    "label_encoder = LabelEncoder()\n",
    "df[encoded_label_col] = label_encoder.fit_transform(df[label_col_name])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Se detectaron y codificaron {num_classes} clases.\")\n",
    "print(\"Mapeo de etiquetas:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "X = df[embedding_cols].values\n",
    "y = df[encoded_label_col].values\n",
    "\n",
    "# 3. Dividir los datos\n",
    "print(\"Dividiendo los datos en conjuntos de entrenamiento, validaci√≥n y prueba...\")\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.25, random_state=42, stratify=y_trainval)\n",
    "print(f\"Tama√±o Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "\n",
    "# Guardar el codificador y los datos de prueba para la evaluaci√≥n final\n",
    "label_encoder_path = os.path.join(MODEL_OUTPUT_DIR, \"label_encoder.pkl\")\n",
    "with open(label_encoder_path, \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "test_features_path = os.path.join(PROCESSED_DIR, \"test_features.pkl\")\n",
    "test_labels_path = os.path.join(PROCESSED_DIR, \"test_labels.pkl\")\n",
    "with open(test_features_path, 'wb') as f: pickle.dump(X_test, f)\n",
    "with open(test_labels_path, 'wb') as f: pickle.dump(y_test, f)\n",
    "\n",
    "print(f\"\\n‚úì Procesamiento completado exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_train_md_1",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento Multi-Modelo con Optuna\n",
    "\n",
    "En esta secci√≥n, optimizaremos tres modelos diferentes usando Optuna para encontrar los mejores hiperpar√°metros. Luego, los combinaremos en un ensemble ponderado para maximizar el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_train_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"--- Preparando datos para el entrenamiento ---\")\n",
    "# Algunos modelos (MLP, Regresi√≥n Log√≠stica) se benefician del escalado de caracter√≠sticas.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Convertimos los datos de validaci√≥n a tensores para PyTorch una sola vez\n",
    "X_val_torch = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "y_val_torch = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "print(\"Datos escalados y tensores de PyTorch listos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_train_md_2",
   "metadata": {},
   "source": [
    "### 5.1 Definici√≥n de las Funciones Objetivo para Optuna\n",
    "\n",
    "Cada funci√≥n `objective` define c√≥mo Optuna debe entrenar y evaluar un modelo para un conjunto de hiperpar√°metros (`trial`). El objetivo es minimizar la m√©trica `log_loss` en el conjunto de validaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_train_objectives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Objetivo para MLP con PyTorch ---\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size, activation_fn, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(current_size, hidden_size))\n",
    "            layers.append(activation_fn())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            current_size = hidden_size\n",
    "        layers.append(nn.Linear(current_size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def objective_mlp(trial):\n",
    "    # Hiperpar√°metros a optimizar\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    hidden_layers = [trial.suggest_int(f'n_units_l{i}', 64, 512) for i in range(n_layers)]\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    activation_name = trial.suggest_categorical('activation', ['ReLU', 'Tanh'])\n",
    "    activation_fn = getattr(nn, activation_name)\n",
    "    \n",
    "    # Modelo, datos y l√≥gica se mueven al dispositivo (GPU/CPU)\n",
    "    model = MLP(X_train_scaled.shape[1], hidden_layers, num_classes, activation_fn, dropout_rate).to(device)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience, patience_counter = 5, 0\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            # Mover cada lote al dispositivo\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # El tensor de validaci√≥n ya est√° en el dispositivo\n",
    "            val_outputs = model(X_val_torch)\n",
    "            val_loss = criterion(val_outputs, y_val_torch).item()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience: break\n",
    "        \n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# --- 2. Objetivo para XGBoost (GPU Habilitado) ---\n",
    "def objective_xgboost(trial):\n",
    "    params = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': num_classes,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'verbosity': 0,\n",
    "        'seed': 42,\n",
    "        # Configuraci√≥n expl√≠cita para usar GPU si est√° disponible\n",
    "        'tree_method': 'hist', \n",
    "        'device': 'cuda' if device.type == 'cuda' else 'cpu',\n",
    "        # Hiperpar√°metros\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10, step=1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params, early_stopping_rounds=15, use_label_encoder=False)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, y_pred_proba)\n",
    "\n",
    "# --- 3. Objetivo para Regresi√≥n Log√≠stica (CPU Paralelizado) ---\n",
    "def objective_logistic(trial):\n",
    "    # Nota: Este modelo se ejecuta en CPU. 'n_jobs=-1' usa todos los n√∫cleos.\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 1e-4, 1e2, log=True),\n",
    "        'solver': 'saga',\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet']),\n",
    "        'max_iter': 1000,\n",
    "        'random_state': 42,\n",
    "        'multi_class': 'multinomial',\n",
    "        'n_jobs': -1 # Usar todos los n√∫cleos de CPU disponibles\n",
    "    }\n",
    "    if params['penalty'] == 'elasticnet':\n",
    "        params['l1_ratio'] = trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "    \n",
    "    model = LogisticRegression(**params)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_val_scaled)\n",
    "    return log_loss(y_val, y_pred_proba)\n",
    "\n",
    "print(f\"Funciones objetivo de Optuna definidas. Dispositivo de c√≥mputo principal: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_train_md_3",
   "metadata": {},
   "source": [
    "### 5.2 Ejecuci√≥n de la B√∫squeda de Hiperpar√°metros\n",
    "Lanzamos los estudios de Optuna para cada modelo. Esto puede tardar un tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_train_hpo_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    'XGBoost': {'objective_func': objective_xgboost, 'n_trials': 40},\n",
    "    'MLP_PyTorch': {'objective_func': objective_mlp, 'n_trials': 50},\n",
    "    'LogisticRegression': {'objective_func': objective_logistic, 'n_trials': 30}\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model_name, config in models_config.items():\n",
    "    print(f\"\\n--- Optimizando {model_name} ---\")\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "    )\n",
    "    study.optimize(config['objective_func'], n_trials=config['n_trials'], show_progress_bar=True)\n",
    "    \n",
    "    model_results[model_name] = {\n",
    "        'best_params': study.best_params,\n",
    "        'best_score': study.best_value\n",
    "    }\n",
    "    print(f\"‚úì {model_name} completado. Mejor LogLoss: {study.best_value:.4f}\")\n",
    "    print(f\"  Mejores par√°metros: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_train_md_4",
   "metadata": {},
   "source": [
    "### 5.3 Entrenamiento de los Modelos Finales\n",
    "\n",
    "Ahora entrenamos cada modelo una √∫ltima vez usando el conjunto completo de entrenamiento (`X_train`, `y_train`) y los mejores hiperpar√°metros encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_train_final_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = {}\n",
    "\n",
    "print(f\"--- Entrenando modelos finales con los mejores hiperpar√°metros en el dispositivo: {device} ---\")\n",
    "\n",
    "# 1. XGBoost (Entrenamiento final en GPU)\n",
    "print(\"Entrenando XGBoost final en GPU...\")\n",
    "xgb_params = model_results['XGBoost']['best_params']\n",
    "final_xgb = xgb.XGBClassifier(\n",
    "    objective='multi:softprob', num_class=num_classes, eval_metric='mlogloss',\n",
    "    seed=42, use_label_encoder=False,\n",
    "    # Configuraci√≥n expl√≠cita para GPU\n",
    "    tree_method='hist',\n",
    "    device='cuda' if device.type == 'cuda' else 'cpu',\n",
    "    early_stopping_rounds=15,\n",
    "    **xgb_params\n",
    ")\n",
    "final_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "trained_models['XGBoost'] = final_xgb\n",
    "print(\"‚úì Modelo XGBoost final entrenado.\")\n",
    "\n",
    "# 2. Regresi√≥n Log√≠stica (Entrenamiento final en CPU)\n",
    "print(\"\\nEntrenando Regresi√≥n Log√≠stica final en CPU (paralelizado)...\")\n",
    "log_params = model_results['LogisticRegression']['best_params']\n",
    "if 'l1_ratio' not in log_params and log_params.get('penalty') == 'elasticnet':\n",
    "    log_params['l1_ratio'] = 0.5\n",
    "final_log = LogisticRegression(\n",
    "    solver='saga', max_iter=2000, random_state=42, multi_class='multinomial', \n",
    "    n_jobs=-1, # Usar todos los n√∫cleos de CPU\n",
    "    **log_params\n",
    ")\n",
    "final_log.fit(X_train_scaled, y_train)\n",
    "trained_models['LogisticRegression'] = final_log\n",
    "print(\"‚úì Modelo de Regresi√≥n Log√≠stica final entrenado.\")\n",
    "\n",
    "\n",
    "# 3. MLP con PyTorch (Entrenamiento final en GPU)\n",
    "print(\"\\nEntrenando MLP (PyTorch) final en GPU...\")\n",
    "mlp_params = model_results['MLP_PyTorch']['best_params']\n",
    "hidden_layers = [mlp_params[f'n_units_l{i}'] for i in range(mlp_params['n_layers'])]\n",
    "# Mover el modelo al dispositivo (GPU/CPU)\n",
    "final_mlp = MLP(\n",
    "    input_size=X_train_scaled.shape[1],\n",
    "    hidden_layers=hidden_layers,\n",
    "    output_size=num_classes,\n",
    "    activation_fn=getattr(nn, mlp_params['activation']),\n",
    "    dropout_rate=mlp_params['dropout_rate']\n",
    ").to(device)\n",
    "\n",
    "optimizer = getattr(optim, mlp_params['optimizer'])(final_mlp.parameters(), lr=mlp_params['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "for epoch in tqdm(range(60), desc=\"Epochs MLP final\"):\n",
    "    final_mlp.train()\n",
    "    for data, target in train_loader:\n",
    "        # Mover cada lote a la GPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = final_mlp(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "trained_models['MLP_PyTorch'] = final_mlp\n",
    "print(\"‚úì Modelo MLP (PyTorch) final entrenado.\")\n",
    "\n",
    "print(\"\\n‚úì Todos los modelos finales han sido entrenados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_train_md_5",
   "metadata": {},
   "source": [
    "### 5.4 Creaci√≥n y Evaluaci√≥n del Ensemble Ponderado\n",
    "\n",
    "Definimos un clasificador `Ensemble` que combina las predicciones de los modelos. Los pesos se calculan en base al rendimiento de cada modelo en el conjunto de validaci√≥n: un menor `log_loss` resulta en un mayor peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_train_ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedEnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, models, weights, scaler=None, device_str='cpu'):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "        self.scaler = scaler\n",
    "        self.device = torch.device(device_str)\n",
    "        self.classes_ = None\n",
    "        \n",
    "        # Verificar si CuPy est√° disponible para el uso de la GPU en XGBoost\n",
    "        self.gpu_available = False\n",
    "        if self.device.type == 'cuda':\n",
    "            try:\n",
    "                import cupy as cp\n",
    "                self.gpu_available = True\n",
    "            except ImportError:\n",
    "                self.gpu_available = False\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        ensemble_proba = np.zeros((X.shape[0], len(self.classes_)))\n",
    "        X_scaled = self.scaler.transform(X) if self.scaler else X\n",
    "\n",
    "        for name, model in self.models.items():\n",
    "            if name == 'XGBoost':\n",
    "                if self.gpu_available:\n",
    "                    import cupy as cp\n",
    "                    X_gpu = cp.asarray(X)\n",
    "                    proba = cp.asnumpy(model.predict_proba(X_gpu))\n",
    "                else:\n",
    "                    proba = model.predict_proba(X)\n",
    "            elif name in ['MLP_Scikit', 'LogisticRegression']:\n",
    "                proba = model.predict_proba(X_scaled)\n",
    "            \n",
    "            ensemble_proba += proba * self.weights[name]\n",
    "        return ensemble_proba\n",
    "\n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probas, axis=1)]\n",
    "\n",
    "# --- C√ìDIGO CORREGIDO ---\n",
    "\n",
    "# CORRECCI√ìN: Volver a verificar la disponibilidad de GPU al inicio de la celda\n",
    "gpu_available = False\n",
    "if device.type == 'cuda':\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        gpu_available = True\n",
    "    except ImportError:\n",
    "        gpu_available = False\n",
    "\n",
    "\n",
    "print(\"--- Evaluando modelos individuales en el conjunto de validaci√≥n ---\")\n",
    "individual_metrics = {}\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    if name == 'XGBoost':\n",
    "        if gpu_available:\n",
    "            import cupy as cp\n",
    "            X_val_gpu = cp.asarray(X_val)\n",
    "            proba = cp.asnumpy(model.predict_proba(X_val_gpu))\n",
    "            pred = cp.asnumpy(model.predict(X_val_gpu))\n",
    "        else:\n",
    "            proba = model.predict_proba(X_val)\n",
    "            pred = model.predict(X_val)\n",
    "    elif name in ['MLP_Scikit', 'LogisticRegression']:\n",
    "        proba = model.predict_proba(X_val_scaled)\n",
    "        pred = model.predict(X_val_scaled)\n",
    "    \n",
    "    loss = log_loss(y_val, proba)\n",
    "    acc = accuracy_score(y_val, pred)\n",
    "    f1 = f1_score(y_val, pred, average='weighted')\n",
    "    \n",
    "    individual_metrics[name] = {'log_loss': loss, 'accuracy': acc, 'f1_score': f1}\n",
    "    print(f\"Modelo: {name:<20} | LogLoss: {loss:.4f} | Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Calcular pesos\n",
    "losses = {name: metrics['log_loss'] for name, metrics in individual_metrics.items()}\n",
    "scores = {name: 1.0 / (loss + 1e-9) for name, loss in losses.items()}\n",
    "total_score = sum(scores.values())\n",
    "weights = {name: score / total_score for name, score in scores.items()}\n",
    "\n",
    "print(\"\\n--- Pesos del Ensemble Calculados ---\")\n",
    "for name, w in weights.items(): print(f\"{name:<20} | Peso: {w:.3f} | LogLoss (Val): {losses[name]:.4f}\")\n",
    "\n",
    "# Crear el ensemble final\n",
    "ensemble_model = WeightedEnsembleClassifier(trained_models, weights, scaler, device.type)\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el ensemble\n",
    "ensemble_proba = ensemble_model.predict_proba(X_val)\n",
    "ensemble_pred = ensemble_model.predict(X_val)\n",
    "ensemble_log_loss = log_loss(y_val, ensemble_proba)\n",
    "ensemble_accuracy = accuracy_score(y_val, ensemble_pred)\n",
    "ensemble_f1 = f1_score(y_val, ensemble_pred, average='weighted')\n",
    "\n",
    "print(\"\\n--- Comparaci√≥n de Rendimiento (Validaci√≥n) ---\")\n",
    "print(f\"{'Modelo':<20} | {'Log Loss':<10} | {'Accuracy':<10} | {'F1-Score':<10}\")\n",
    "print(\"=\"*60)\n",
    "for name, metrics in individual_metrics.items():\n",
    "    print(f\"{name:<20} | {metrics['log_loss']:<10.4f} | {metrics['accuracy']:<10.4f} | {metrics['f1_score']:<10.4f}\")\n",
    "print(\"‚Äî\"*60)\n",
    "print(f\"{'üèÜ ENSEMBLE':<20} | {ensemble_log_loss:<10.4f} | {ensemble_accuracy:<10.4f} | {ensemble_f1:<10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_md",
   "metadata": {},
   "source": [
    "## 6. Evaluaci√≥n Final del Modelo Ensemble (en Conjunto de Prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Ejecutando Evaluaci√≥n Final del Ensemble en el Conjunto de Prueba ---\")\n",
    "\n",
    "# 1. Cargar los datos de prueba y el codificador de etiquetas\n",
    "with open(test_features_path, 'rb') as f: X_test_eval = pickle.load(f)\n",
    "with open(test_labels_path, 'rb') as f: y_test_eval = pickle.load(f)\n",
    "with open(label_encoder_path, 'rb') as f: label_encoder_eval = pickle.load(f)\n",
    "print(f\"Datos de prueba cargados: {X_test_eval.shape[0]} muestras.\")\n",
    "\n",
    "# 2. Obtener predicciones del modelo ensemble\n",
    "y_pred_eval = ensemble_model.predict(X_test_eval)\n",
    "\n",
    "# 3. Calcular y mostrar m√©tricas\n",
    "print(\"\\n--- Resultados de la Evaluaci√≥n Final ---\")\n",
    "acc = accuracy_score(y_test_eval, y_pred_eval)\n",
    "report = classification_report(y_test_eval, y_pred_eval, target_names=label_encoder_eval.classes_)\n",
    "cm = confusion_matrix(y_test_eval, y_pred_eval)\n",
    "\n",
    "print(f\"Accuracy en el conjunto de prueba: {acc:.4f}\\n\")\n",
    "print(\"Reporte de Clasificaci√≥n:\")\n",
    "print(report)\n",
    "\n",
    "print(\"\\nMatriz de Confusi√≥n:\")\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder_eval.classes_, yticklabels=label_encoder_eval.classes_)\n",
    "plt.ylabel('Etiqueta Real')\n",
    "plt.xlabel('Etiqueta Predicha')\n",
    "plt.title('Matriz de Confusi√≥n del Modelo Ensemble (Prueba)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Evaluaci√≥n Completada ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_train_md_6",
   "metadata": {},
   "source": [
    "## 7. Guardado de Artefactos del Modelo\n",
    "\n",
    "Guardamos el modelo ensemble final, los modelos individuales, el escalador y los resultados de la optimizaci√≥n para su uso futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_train_save",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Guardando artefactos en {MODEL_OUTPUT_DIR} ---\")\n",
    "\n",
    "# 1. Guardar el modelo ensemble completo\n",
    "ensemble_path = os.path.join(MODEL_OUTPUT_DIR, \"ensemble_model.pkl\")\n",
    "with open(ensemble_path, 'wb') as f:\n",
    "    pickle.dump(ensemble_model, f)\n",
    "print(f\"‚úì Modelo ensemble guardado en: {ensemble_path}\")\n",
    "\n",
    "# 2. Guardar los modelos individuales\n",
    "# Para PyTorch, guardamos el state_dict, que es m√°s robusto\n",
    "mlp_path = os.path.join(MODEL_OUTPUT_DIR, \"mlp_pytorch.pth\")\n",
    "torch.save(trained_models['MLP_PyTorch'].state_dict(), mlp_path)\n",
    "print(f\"‚úì Modelo MLP (PyTorch) guardado en: {mlp_path}\")\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    if name != 'MLP_PyTorch':\n",
    "        model_path = os.path.join(MODEL_OUTPUT_DIR, f\"{name.lower()}_model.pkl\")\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"‚úì Modelo {name} guardado en: {model_path}\")\n",
    "\n",
    "# 3. Guardar el escalador\n",
    "scaler_path = os.path.join(MODEL_OUTPUT_DIR, \"scaler.pkl\")\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"‚úì Scaler guardado en: {scaler_path}\")\n",
    "\n",
    "# 4. Guardar los resultados de Optuna\n",
    "results_path = os.path.join(MODEL_OUTPUT_DIR, \"optuna_results.pkl\")\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump(model_results, f)\n",
    "print(f\"‚úì Resultados de Optuna guardados en: {results_path}\")\n",
    "\n",
    "print(\"\\nüéâ Pipeline completado y todos los artefactos han sido guardados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
