{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# Modelo de Clasificación Jerárquica con Aumento de Datos v4.0\n",
    "\n",
    "Este notebook implementa un pipeline avanzado de clasificación jerárquica con características combinadas:\n",
    "\n",
    "1.  **Carga y Preprocesamiento**: Usa `hate_speech_twitter` y realiza limpieza de texto (tokenización, stemming, etc.).\n",
    "2.  **Generación de Características Dual**: Crea embeddings de BERT y vectores TF-IDF.\n",
    "3.  **Aumento de Datos Sintéticos**: Utiliza **CTGAN** de la librería `sdv` para generar datos sintéticos y **balancear las sub-categorías** de discurso de odio en el conjunto de entrenamiento, mejorando la robustez del modelo.\n",
    "4.  **Entrenamiento de Clasificador Principal (Nivel 1)**: Entrena y optimiza un **ensemble extendido de seis modelos**. Incluye modelos que usan solo embeddings, solo TF-IDF, y una **combinación de ambos** para una detección más robusta de `odio` vs. `no-odio`.\n",
    "5.  **Entrenamiento de Clasificador de Sub-categorías (Nivel 2)**: Entrena un ensemble de tres modelos XGBoost, MLP y Regresión Logística para clasificar el **tipo de odio** (ej. sexismo, racismo), utilizando también **características combinadas de embeddings y TF-IDF**.\n",
    "6.  **Evaluación Jerárquica**: Evalúa el rendimiento del pipeline completo en dos niveles, reportando la precisión tanto en la detección de odio como en la clasificación de su tipo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_md",
   "metadata": {},
   "source": [
    "## 1. Instalación y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch datasets scikit-learn xgboost pandas seaborn matplotlib tqdm optuna nltk scipy sdv nltk ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "config_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "\n",
      "ID de trabajo: hierarchical-job-1751010650\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import xgboost as xgb\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix, vstack\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from transformer_text import build_vocab\n",
    "from transformer_text import HateSpeechDataset\n",
    "\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_SAMPLES = 10000 # Aumentar para un mejor entrenamiento de sub-categorías\n",
    "MAX_TOKEN_LENGTH = 128\n",
    "\n",
    "# --- Configuración de Dispositivo (GPU o CPU) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# --- Definición de Rutas Locales ---\n",
    "job_id = f\"hierarchical-job-{int(time.time())}\"\n",
    "BASE_DIR = \"./datos_locales\"\n",
    "# PROCESSED_DIR = os.path.join(BASE_DIR, \"processed\", job_id)\n",
    "MODEL_OUTPUT_DIR = os.path.join(BASE_DIR, \"model_output\", job_id)\n",
    "# os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "# PROCESSED_DATA_PATH = os.path.join(PROCESSED_DIR, \"processed_data_with_embeddings.csv\")\n",
    "print(f\"\\nID de trabajo: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cc320ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transformer para clasificación de odio ---\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N, query_len, _ = query.shape\n",
    "        value_len, key_len = values.shape[1], keys.shape[1]\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        attention = torch.softmax(energy / (self.embed_size ** 0.5), dim=3)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values])\n",
    "        out = out.reshape(N, query_len, self.heads * self.head_dim)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attn = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attn + query))\n",
    "        ff = self.feed_forward(x)\n",
    "        return self.dropout(self.norm2(ff + x))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).unsqueeze(\n",
    "            0).expand(N, seq_length).to(self.device)\n",
    "        out = self.dropout(self.word_embedding(\n",
    "            x) + self.position_embedding(positions))\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, num_classes, max_length, device):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size, embed_size, num_layers,\n",
    "                               heads, forward_expansion, dropout, max_length, device)\n",
    "        self.fc_out = nn.Linear(embed_size, num_classes)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x != 0).unsqueeze(1).unsqueeze(2).to(self.device)\n",
    "        enc = self.encoder(x, mask)\n",
    "        pooled = enc.mean(dim=1)\n",
    "        return self.fc_out(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_load_md",
   "metadata": {},
   "source": [
    "## 2. Carga, Análisis y Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "data_load_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset 'thefrankhsu/hate_speech_twitter'...\n",
      "Tomando una muestra aleatoria de 5679 registros (de un total de 5679).\n",
      "Distribución de etiquetas principales:\n",
      "main_label\n",
      "0    4163\n",
      "1    1516\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución de sub-etiquetas (solo para 'odio'):\n",
      "sub_label_str\n",
      "Race                   523\n",
      "Sexual Orientation     429\n",
      "Gender                 279\n",
      "Physical Appearance     73\n",
      "Religion                52\n",
      "Behavior                40\n",
      "Class                   40\n",
      "Ethnicity               40\n",
      "Disability              40\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Mapeo de Sub-etiquetas: {'Behavior': np.int64(0), 'Class': np.int64(1), 'Disability': np.int64(2), 'Ethnicity': np.int64(3), 'Gender': np.int64(4), 'Physical Appearance': np.int64(5), 'Race': np.int64(6), 'Religion': np.int64(7), 'Sexual Orientation': np.int64(8), 'not-hate': np.int64(9)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a53a95a84f343dc8c581a5f0e9152e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Limpiando Texto para Embeddings:   0%|          | 0/5679 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106a384efa594c9cabe99f66daa1dd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aplicando Stemming para TF-IDF:   0%|          | 0/5679 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Cargando dataset 'thefrankhsu/hate_speech_twitter'...\")\n",
    "dataset = load_dataset(\"thefrankhsu/hate_speech_twitter\")\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Renombrar columnas y manejar nulos en 'categories'\n",
    "df = df.rename(columns={'tweet': 'text_raw', 'label': 'main_label', 'categories': 'sub_label_str'})\n",
    "df['sub_label_str'] = df['sub_label_str'].fillna('not-hate')\n",
    "\n",
    "\n",
    "if MAX_SAMPLES is not None:\n",
    "    # Asegurarse de que el tamaño de la muestra no sea mayor que la población\n",
    "    sample_size = min(MAX_SAMPLES, len(df))\n",
    "    print(f\"Tomando una muestra aleatoria de {sample_size} registros (de un total de {len(df)}).\")\n",
    "    df = df.sample(n=sample_size, random_state=42, replace=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Distribución de etiquetas principales:\")\n",
    "print(df['main_label'].value_counts())\n",
    "\n",
    "print(\"\\nDistribución de sub-etiquetas (solo para 'odio'):\")\n",
    "print(df[df['main_label'] == 1]['sub_label_str'].value_counts())\n",
    "\n",
    "# Codificar sub-etiquetas\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "sub_label_encoder = LabelEncoder()\n",
    "df['sub_label_encoded'] = sub_label_encoder.fit_transform(df['sub_label_str'])\n",
    "sub_label_mapping = dict(zip(sub_label_encoder.classes_, sub_label_encoder.transform(sub_label_encoder.classes_)))\n",
    "print(\"\\nMapeo de Sub-etiquetas:\", sub_label_mapping)\n",
    "\n",
    "# Preprocesamiento de texto\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "def clean_text(text, apply_stemming=False):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|#','', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    words = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    if apply_stemming: words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "tqdm.pandas(desc=\"Limpiando Texto para Embeddings\")\n",
    "df['text_cleaned'] = df['text_raw'].progress_apply(lambda x: clean_text(x, apply_stemming=False))\n",
    "tqdm.pandas(desc=\"Aplicando Stemming para TF-IDF\")\n",
    "df['text_stemmed'] = df['text_cleaned'].progress_apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding_md",
   "metadata": {},
   "source": [
    "## 3. Generación de Embeddings y División de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "embedding_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo y tokenizador BERT: bert-base-uncased\n",
      "Generando embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c9b65672324a1d9735ad504bf5385c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dividiendo Datos ---\n",
      "Tamaño Train: 3407, Val: 1136, Test: 1136\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cargando modelo y tokenizador BERT: {BERT_MODEL_NAME}\")\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "model_bert = AutoModel.from_pretrained(BERT_MODEL_NAME).to(device)\n",
    "model_bert.eval()\n",
    "\n",
    "def get_bert_embeddings(batch_text):\n",
    "    inputs = tokenizer_bert(batch_text, padding=True, truncation=True, max_length=MAX_TOKEN_LENGTH, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "print(\"Generando embeddings...\")\n",
    "batch_size = 32\n",
    "all_embeddings = np.vstack([get_bert_embeddings(df.iloc[i:i+batch_size]['text_cleaned'].tolist()) for i in tqdm(range(0, len(df), batch_size))])\n",
    "\n",
    "embedding_cols = [f'dim_{i}' for i in range(all_embeddings.shape[1])]\n",
    "df_embeddings = pd.DataFrame(all_embeddings, columns=embedding_cols, index=df.index)\n",
    "df_processed = pd.concat([df, df_embeddings], axis=1)\n",
    "\n",
    "print(\"\\n--- Dividiendo Datos ---\")\n",
    "y_main = df_processed['main_label'].values\n",
    "df_trainval, df_test = train_test_split(df_processed, test_size=0.2, random_state=42, stratify=y_main)\n",
    "y_trainval_main = df_trainval['main_label'].values\n",
    "df_train, df_val = train_test_split(df_trainval, test_size=0.25, random_state=42, stratify=y_trainval_main)\n",
    "\n",
    "print(f\"Tamaño Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "augmentation_md",
   "metadata": {},
   "source": [
    "## 4. Aumento de Datos Sintéticos para Sub-categorías (CTGAN)\n",
    "Nos enfocamos en el desbalance de las sub-categorías de 'odio'. Usaremos CTGAN para generar nuevos datos de entrenamiento para las clases minoritarias, basándonos en sus embeddings. **Importante**: CTGAN solo genera embeddings sintéticos; no puede generar texto. La parte de TF-IDF para estos datos se manejará más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "augmentation_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparando datos para aumento ---\n",
      "Distribución de sub-categorías ANTES del aumento:\n",
      "sub_label_str\n",
      "Race                   321\n",
      "Sexual Orientation     271\n",
      "Gender                 158\n",
      "Physical Appearance     43\n",
      "Religion                27\n",
      "Class                   25\n",
      "Ethnicity               23\n",
      "Disability              22\n",
      "Behavior                20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Nuevo encoder para Nivel 2 creado. Clases: ['Behavior' 'Class' 'Disability' 'Ethnicity' 'Gender'\n",
      " 'Physical Appearance' 'Race' 'Religion' 'Sexual Orientation']\n",
      "\n",
      "Actualizando metadatos para tratar embeddings como numéricos continuos...\n",
      "Usando GPU: True\n",
      "\n",
      "Entrenando CTGAN para generar datos sintéticos... (Usando GPU: True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Kevin\\Escritorio\\Kevin\\DetectingAggressionInTexts\\Env\\Lib\\site-packages\\sdv\\single_table\\base.py:162: FutureWarning:\n",
      "\n",
      "The 'SingleTableMetadata' is deprecated. Please use the new 'Metadata' class for synthesizers.\n",
      "\n",
      "h:\\Kevin\\Escritorio\\Kevin\\DetectingAggressionInTexts\\Env\\Lib\\site-packages\\sdv\\single_table\\base.py:128: UserWarning:\n",
      "\n",
      "We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerformanceAlert: Using the CTGANSynthesizer on this data is not recommended. To model this data, CTGAN will generate a large number of columns.\n",
      "\n",
      "Original Column Name   Est # of Columns (CTGAN)\n",
      "sub_label_str          9\n",
      "dim_0                  11\n",
      "dim_1                  11\n",
      "dim_2                  11\n",
      "dim_3                  11\n",
      "dim_4                  11\n",
      "dim_5                  11\n",
      "dim_6                  11\n",
      "dim_7                  11\n",
      "dim_8                  11\n",
      "dim_9                  11\n",
      "dim_10                 11\n",
      "dim_11                 11\n",
      "dim_12                 11\n",
      "dim_13                 11\n",
      "dim_14                 11\n",
      "dim_15                 11\n",
      "dim_16                 11\n",
      "dim_17                 11\n",
      "dim_18                 11\n",
      "dim_19                 11\n",
      "dim_20                 11\n",
      "dim_21                 11\n",
      "dim_22                 11\n",
      "dim_23                 11\n",
      "dim_24                 11\n",
      "dim_25                 11\n",
      "dim_26                 11\n",
      "dim_27                 11\n",
      "dim_28                 11\n",
      "dim_29                 11\n",
      "dim_30                 11\n",
      "dim_31                 11\n",
      "dim_32                 11\n",
      "dim_33                 11\n",
      "dim_34                 11\n",
      "dim_35                 11\n",
      "dim_36                 11\n",
      "dim_37                 11\n",
      "dim_38                 11\n",
      "dim_39                 11\n",
      "dim_40                 11\n",
      "dim_41                 11\n",
      "dim_42                 11\n",
      "dim_43                 11\n",
      "dim_44                 11\n",
      "dim_45                 11\n",
      "dim_46                 11\n",
      "dim_47                 11\n",
      "dim_48                 11\n",
      "dim_49                 11\n",
      "dim_50                 11\n",
      "dim_51                 11\n",
      "dim_52                 11\n",
      "dim_53                 11\n",
      "dim_54                 11\n",
      "dim_55                 11\n",
      "dim_56                 11\n",
      "dim_57                 11\n",
      "dim_58                 11\n",
      "dim_59                 11\n",
      "dim_60                 11\n",
      "dim_61                 11\n",
      "dim_62                 11\n",
      "dim_63                 11\n",
      "dim_64                 11\n",
      "dim_65                 11\n",
      "dim_66                 11\n",
      "dim_67                 11\n",
      "dim_68                 11\n",
      "dim_69                 11\n",
      "dim_70                 11\n",
      "dim_71                 11\n",
      "dim_72                 11\n",
      "dim_73                 11\n",
      "dim_74                 11\n",
      "dim_75                 11\n",
      "dim_76                 11\n",
      "dim_77                 11\n",
      "dim_78                 11\n",
      "dim_79                 11\n",
      "dim_80                 11\n",
      "dim_81                 11\n",
      "dim_82                 11\n",
      "dim_83                 11\n",
      "dim_84                 11\n",
      "dim_85                 11\n",
      "dim_86                 11\n",
      "dim_87                 11\n",
      "dim_88                 11\n",
      "dim_89                 11\n",
      "dim_90                 11\n",
      "dim_91                 11\n",
      "dim_92                 11\n",
      "dim_93                 11\n",
      "dim_94                 11\n",
      "dim_95                 11\n",
      "dim_96                 11\n",
      "dim_97                 11\n",
      "dim_98                 11\n",
      "dim_99                 11\n",
      "dim_100                11\n",
      "dim_101                11\n",
      "dim_102                11\n",
      "dim_103                11\n",
      "dim_104                11\n",
      "dim_105                11\n",
      "dim_106                11\n",
      "dim_107                11\n",
      "dim_108                11\n",
      "dim_109                11\n",
      "dim_110                11\n",
      "dim_111                11\n",
      "dim_112                11\n",
      "dim_113                11\n",
      "dim_114                11\n",
      "dim_115                11\n",
      "dim_116                11\n",
      "dim_117                11\n",
      "dim_118                11\n",
      "dim_119                11\n",
      "dim_120                11\n",
      "dim_121                11\n",
      "dim_122                11\n",
      "dim_123                11\n",
      "dim_124                11\n",
      "dim_125                11\n",
      "dim_126                11\n",
      "dim_127                11\n",
      "dim_128                11\n",
      "dim_129                11\n",
      "dim_130                11\n",
      "dim_131                11\n",
      "dim_132                11\n",
      "dim_133                11\n",
      "dim_134                11\n",
      "dim_135                11\n",
      "dim_136                11\n",
      "dim_137                11\n",
      "dim_138                11\n",
      "dim_139                11\n",
      "dim_140                11\n",
      "dim_141                11\n",
      "dim_142                11\n",
      "dim_143                11\n",
      "dim_144                11\n",
      "dim_145                11\n",
      "dim_146                11\n",
      "dim_147                11\n",
      "dim_148                11\n",
      "dim_149                11\n",
      "dim_150                11\n",
      "dim_151                11\n",
      "dim_152                11\n",
      "dim_153                11\n",
      "dim_154                11\n",
      "dim_155                11\n",
      "dim_156                11\n",
      "dim_157                11\n",
      "dim_158                11\n",
      "dim_159                11\n",
      "dim_160                11\n",
      "dim_161                11\n",
      "dim_162                11\n",
      "dim_163                11\n",
      "dim_164                11\n",
      "dim_165                11\n",
      "dim_166                11\n",
      "dim_167                11\n",
      "dim_168                11\n",
      "dim_169                11\n",
      "dim_170                11\n",
      "dim_171                11\n",
      "dim_172                11\n",
      "dim_173                11\n",
      "dim_174                11\n",
      "dim_175                11\n",
      "dim_176                11\n",
      "dim_177                11\n",
      "dim_178                11\n",
      "dim_179                11\n",
      "dim_180                11\n",
      "dim_181                11\n",
      "dim_182                11\n",
      "dim_183                11\n",
      "dim_184                11\n",
      "dim_185                11\n",
      "dim_186                11\n",
      "dim_187                11\n",
      "dim_188                11\n",
      "dim_189                11\n",
      "dim_190                11\n",
      "dim_191                11\n",
      "dim_192                11\n",
      "dim_193                11\n",
      "dim_194                11\n",
      "dim_195                11\n",
      "dim_196                11\n",
      "dim_197                11\n",
      "dim_198                11\n",
      "dim_199                11\n",
      "dim_200                11\n",
      "dim_201                11\n",
      "dim_202                11\n",
      "dim_203                11\n",
      "dim_204                11\n",
      "dim_205                11\n",
      "dim_206                11\n",
      "dim_207                11\n",
      "dim_208                11\n",
      "dim_209                11\n",
      "dim_210                11\n",
      "dim_211                11\n",
      "dim_212                11\n",
      "dim_213                11\n",
      "dim_214                11\n",
      "dim_215                11\n",
      "dim_216                11\n",
      "dim_217                11\n",
      "dim_218                11\n",
      "dim_219                11\n",
      "dim_220                11\n",
      "dim_221                11\n",
      "dim_222                11\n",
      "dim_223                11\n",
      "dim_224                11\n",
      "dim_225                11\n",
      "dim_226                11\n",
      "dim_227                11\n",
      "dim_228                11\n",
      "dim_229                11\n",
      "dim_230                11\n",
      "dim_231                11\n",
      "dim_232                11\n",
      "dim_233                11\n",
      "dim_234                11\n",
      "dim_235                11\n",
      "dim_236                11\n",
      "dim_237                11\n",
      "dim_238                11\n",
      "dim_239                11\n",
      "dim_240                11\n",
      "dim_241                11\n",
      "dim_242                11\n",
      "dim_243                11\n",
      "dim_244                11\n",
      "dim_245                11\n",
      "dim_246                11\n",
      "dim_247                11\n",
      "dim_248                11\n",
      "dim_249                11\n",
      "dim_250                11\n",
      "dim_251                11\n",
      "dim_252                11\n",
      "dim_253                11\n",
      "dim_254                11\n",
      "dim_255                11\n",
      "dim_256                11\n",
      "dim_257                11\n",
      "dim_258                11\n",
      "dim_259                11\n",
      "dim_260                11\n",
      "dim_261                11\n",
      "dim_262                11\n",
      "dim_263                11\n",
      "dim_264                11\n",
      "dim_265                11\n",
      "dim_266                11\n",
      "dim_267                11\n",
      "dim_268                11\n",
      "dim_269                11\n",
      "dim_270                11\n",
      "dim_271                11\n",
      "dim_272                11\n",
      "dim_273                11\n",
      "dim_274                11\n",
      "dim_275                11\n",
      "dim_276                11\n",
      "dim_277                11\n",
      "dim_278                11\n",
      "dim_279                11\n",
      "dim_280                11\n",
      "dim_281                11\n",
      "dim_282                11\n",
      "dim_283                11\n",
      "dim_284                11\n",
      "dim_285                11\n",
      "dim_286                11\n",
      "dim_287                11\n",
      "dim_288                11\n",
      "dim_289                11\n",
      "dim_290                11\n",
      "dim_291                11\n",
      "dim_292                11\n",
      "dim_293                11\n",
      "dim_294                11\n",
      "dim_295                11\n",
      "dim_296                11\n",
      "dim_297                11\n",
      "dim_298                11\n",
      "dim_299                11\n",
      "dim_300                11\n",
      "dim_301                11\n",
      "dim_302                11\n",
      "dim_303                11\n",
      "dim_304                11\n",
      "dim_305                11\n",
      "dim_306                11\n",
      "dim_307                11\n",
      "dim_308                11\n",
      "dim_309                11\n",
      "dim_310                11\n",
      "dim_311                11\n",
      "dim_312                11\n",
      "dim_313                11\n",
      "dim_314                11\n",
      "dim_315                11\n",
      "dim_316                11\n",
      "dim_317                11\n",
      "dim_318                11\n",
      "dim_319                11\n",
      "dim_320                11\n",
      "dim_321                11\n",
      "dim_322                11\n",
      "dim_323                11\n",
      "dim_324                11\n",
      "dim_325                11\n",
      "dim_326                11\n",
      "dim_327                11\n",
      "dim_328                11\n",
      "dim_329                11\n",
      "dim_330                11\n",
      "dim_331                11\n",
      "dim_332                11\n",
      "dim_333                11\n",
      "dim_334                11\n",
      "dim_335                11\n",
      "dim_336                11\n",
      "dim_337                11\n",
      "dim_338                11\n",
      "dim_339                11\n",
      "dim_340                11\n",
      "dim_341                11\n",
      "dim_342                11\n",
      "dim_343                11\n",
      "dim_344                11\n",
      "dim_345                11\n",
      "dim_346                11\n",
      "dim_347                11\n",
      "dim_348                11\n",
      "dim_349                11\n",
      "dim_350                11\n",
      "dim_351                11\n",
      "dim_352                11\n",
      "dim_353                11\n",
      "dim_354                11\n",
      "dim_355                11\n",
      "dim_356                11\n",
      "dim_357                11\n",
      "dim_358                11\n",
      "dim_359                11\n",
      "dim_360                11\n",
      "dim_361                11\n",
      "dim_362                11\n",
      "dim_363                11\n",
      "dim_364                11\n",
      "dim_365                11\n",
      "dim_366                11\n",
      "dim_367                11\n",
      "dim_368                11\n",
      "dim_369                11\n",
      "dim_370                11\n",
      "dim_371                11\n",
      "dim_372                11\n",
      "dim_373                11\n",
      "dim_374                11\n",
      "dim_375                11\n",
      "dim_376                11\n",
      "dim_377                11\n",
      "dim_378                11\n",
      "dim_379                11\n",
      "dim_380                11\n",
      "dim_381                11\n",
      "dim_382                11\n",
      "dim_383                11\n",
      "dim_384                11\n",
      "dim_385                11\n",
      "dim_386                11\n",
      "dim_387                11\n",
      "dim_388                11\n",
      "dim_389                11\n",
      "dim_390                11\n",
      "dim_391                11\n",
      "dim_392                11\n",
      "dim_393                11\n",
      "dim_394                11\n",
      "dim_395                11\n",
      "dim_396                11\n",
      "dim_397                11\n",
      "dim_398                11\n",
      "dim_399                11\n",
      "dim_400                11\n",
      "dim_401                11\n",
      "dim_402                11\n",
      "dim_403                11\n",
      "dim_404                11\n",
      "dim_405                11\n",
      "dim_406                11\n",
      "dim_407                11\n",
      "dim_408                11\n",
      "dim_409                11\n",
      "dim_410                11\n",
      "dim_411                11\n",
      "dim_412                11\n",
      "dim_413                11\n",
      "dim_414                11\n",
      "dim_415                11\n",
      "dim_416                11\n",
      "dim_417                11\n",
      "dim_418                11\n",
      "dim_419                11\n",
      "dim_420                11\n",
      "dim_421                11\n",
      "dim_422                11\n",
      "dim_423                11\n",
      "dim_424                11\n",
      "dim_425                11\n",
      "dim_426                11\n",
      "dim_427                11\n",
      "dim_428                11\n",
      "dim_429                11\n",
      "dim_430                11\n",
      "dim_431                11\n",
      "dim_432                11\n",
      "dim_433                11\n",
      "dim_434                11\n",
      "dim_435                11\n",
      "dim_436                11\n",
      "dim_437                11\n",
      "dim_438                11\n",
      "dim_439                11\n",
      "dim_440                11\n",
      "dim_441                11\n",
      "dim_442                11\n",
      "dim_443                11\n",
      "dim_444                11\n",
      "dim_445                11\n",
      "dim_446                11\n",
      "dim_447                11\n",
      "dim_448                11\n",
      "dim_449                11\n",
      "dim_450                11\n",
      "dim_451                11\n",
      "dim_452                11\n",
      "dim_453                11\n",
      "dim_454                11\n",
      "dim_455                11\n",
      "dim_456                11\n",
      "dim_457                11\n",
      "dim_458                11\n",
      "dim_459                11\n",
      "dim_460                11\n",
      "dim_461                11\n",
      "dim_462                11\n",
      "dim_463                11\n",
      "dim_464                11\n",
      "dim_465                11\n",
      "dim_466                11\n",
      "dim_467                11\n",
      "dim_468                11\n",
      "dim_469                11\n",
      "dim_470                11\n",
      "dim_471                11\n",
      "dim_472                11\n",
      "dim_473                11\n",
      "dim_474                11\n",
      "dim_475                11\n",
      "dim_476                11\n",
      "dim_477                11\n",
      "dim_478                11\n",
      "dim_479                11\n",
      "dim_480                11\n",
      "dim_481                11\n",
      "dim_482                11\n",
      "dim_483                11\n",
      "dim_484                11\n",
      "dim_485                11\n",
      "dim_486                11\n",
      "dim_487                11\n",
      "dim_488                11\n",
      "dim_489                11\n",
      "dim_490                11\n",
      "dim_491                11\n",
      "dim_492                11\n",
      "dim_493                11\n",
      "dim_494                11\n",
      "dim_495                11\n",
      "dim_496                11\n",
      "dim_497                11\n",
      "dim_498                11\n",
      "dim_499                11\n",
      "dim_500                11\n",
      "dim_501                11\n",
      "dim_502                11\n",
      "dim_503                11\n",
      "dim_504                11\n",
      "dim_505                11\n",
      "dim_506                11\n",
      "dim_507                11\n",
      "dim_508                11\n",
      "dim_509                11\n",
      "dim_510                11\n",
      "dim_511                11\n",
      "dim_512                11\n",
      "dim_513                11\n",
      "dim_514                11\n",
      "dim_515                11\n",
      "dim_516                11\n",
      "dim_517                11\n",
      "dim_518                11\n",
      "dim_519                11\n",
      "dim_520                11\n",
      "dim_521                11\n",
      "dim_522                11\n",
      "dim_523                11\n",
      "dim_524                11\n",
      "dim_525                11\n",
      "dim_526                11\n",
      "dim_527                11\n",
      "dim_528                11\n",
      "dim_529                11\n",
      "dim_530                11\n",
      "dim_531                11\n",
      "dim_532                11\n",
      "dim_533                11\n",
      "dim_534                11\n",
      "dim_535                11\n",
      "dim_536                11\n",
      "dim_537                11\n",
      "dim_538                11\n",
      "dim_539                11\n",
      "dim_540                11\n",
      "dim_541                11\n",
      "dim_542                11\n",
      "dim_543                11\n",
      "dim_544                11\n",
      "dim_545                11\n",
      "dim_546                11\n",
      "dim_547                11\n",
      "dim_548                11\n",
      "dim_549                11\n",
      "dim_550                11\n",
      "dim_551                11\n",
      "dim_552                11\n",
      "dim_553                11\n",
      "dim_554                11\n",
      "dim_555                11\n",
      "dim_556                11\n",
      "dim_557                11\n",
      "dim_558                11\n",
      "dim_559                11\n",
      "dim_560                11\n",
      "dim_561                11\n",
      "dim_562                11\n",
      "dim_563                11\n",
      "dim_564                11\n",
      "dim_565                11\n",
      "dim_566                11\n",
      "dim_567                11\n",
      "dim_568                11\n",
      "dim_569                11\n",
      "dim_570                11\n",
      "dim_571                11\n",
      "dim_572                11\n",
      "dim_573                11\n",
      "dim_574                11\n",
      "dim_575                11\n",
      "dim_576                11\n",
      "dim_577                11\n",
      "dim_578                11\n",
      "dim_579                11\n",
      "dim_580                11\n",
      "dim_581                11\n",
      "dim_582                11\n",
      "dim_583                11\n",
      "dim_584                11\n",
      "dim_585                11\n",
      "dim_586                11\n",
      "dim_587                11\n",
      "dim_588                11\n",
      "dim_589                11\n",
      "dim_590                11\n",
      "dim_591                11\n",
      "dim_592                11\n",
      "dim_593                11\n",
      "dim_594                11\n",
      "dim_595                11\n",
      "dim_596                11\n",
      "dim_597                11\n",
      "dim_598                11\n",
      "dim_599                11\n",
      "dim_600                11\n",
      "dim_601                11\n",
      "dim_602                11\n",
      "dim_603                11\n",
      "dim_604                11\n",
      "dim_605                11\n",
      "dim_606                11\n",
      "dim_607                11\n",
      "dim_608                11\n",
      "dim_609                11\n",
      "dim_610                11\n",
      "dim_611                11\n",
      "dim_612                11\n",
      "dim_613                11\n",
      "dim_614                11\n",
      "dim_615                11\n",
      "dim_616                11\n",
      "dim_617                11\n",
      "dim_618                11\n",
      "dim_619                11\n",
      "dim_620                11\n",
      "dim_621                11\n",
      "dim_622                11\n",
      "dim_623                11\n",
      "dim_624                11\n",
      "dim_625                11\n",
      "dim_626                11\n",
      "dim_627                11\n",
      "dim_628                11\n",
      "dim_629                11\n",
      "dim_630                11\n",
      "dim_631                11\n",
      "dim_632                11\n",
      "dim_633                11\n",
      "dim_634                11\n",
      "dim_635                11\n",
      "dim_636                11\n",
      "dim_637                11\n",
      "dim_638                11\n",
      "dim_639                11\n",
      "dim_640                11\n",
      "dim_641                11\n",
      "dim_642                11\n",
      "dim_643                11\n",
      "dim_644                11\n",
      "dim_645                11\n",
      "dim_646                11\n",
      "dim_647                11\n",
      "dim_648                11\n",
      "dim_649                11\n",
      "dim_650                11\n",
      "dim_651                11\n",
      "dim_652                11\n",
      "dim_653                11\n",
      "dim_654                11\n",
      "dim_655                11\n",
      "dim_656                11\n",
      "dim_657                11\n",
      "dim_658                11\n",
      "dim_659                11\n",
      "dim_660                11\n",
      "dim_661                11\n",
      "dim_662                11\n",
      "dim_663                11\n",
      "dim_664                11\n",
      "dim_665                11\n",
      "dim_666                11\n",
      "dim_667                11\n",
      "dim_668                11\n",
      "dim_669                11\n",
      "dim_670                11\n",
      "dim_671                11\n",
      "dim_672                11\n",
      "dim_673                11\n",
      "dim_674                11\n",
      "dim_675                11\n",
      "dim_676                11\n",
      "dim_677                11\n",
      "dim_678                11\n",
      "dim_679                11\n",
      "dim_680                11\n",
      "dim_681                11\n",
      "dim_682                11\n",
      "dim_683                11\n",
      "dim_684                11\n",
      "dim_685                11\n",
      "dim_686                11\n",
      "dim_687                11\n",
      "dim_688                11\n",
      "dim_689                11\n",
      "dim_690                11\n",
      "dim_691                11\n",
      "dim_692                11\n",
      "dim_693                11\n",
      "dim_694                11\n",
      "dim_695                11\n",
      "dim_696                11\n",
      "dim_697                11\n",
      "dim_698                11\n",
      "dim_699                11\n",
      "dim_700                11\n",
      "dim_701                11\n",
      "dim_702                11\n",
      "dim_703                11\n",
      "dim_704                11\n",
      "dim_705                11\n",
      "dim_706                11\n",
      "dim_707                11\n",
      "dim_708                11\n",
      "dim_709                11\n",
      "dim_710                11\n",
      "dim_711                11\n",
      "dim_712                11\n",
      "dim_713                11\n",
      "dim_714                11\n",
      "dim_715                11\n",
      "dim_716                11\n",
      "dim_717                11\n",
      "dim_718                11\n",
      "dim_719                11\n",
      "dim_720                11\n",
      "dim_721                11\n",
      "dim_722                11\n",
      "dim_723                11\n",
      "dim_724                11\n",
      "dim_725                11\n",
      "dim_726                11\n",
      "dim_727                11\n",
      "dim_728                11\n",
      "dim_729                11\n",
      "dim_730                11\n",
      "dim_731                11\n",
      "dim_732                11\n",
      "dim_733                11\n",
      "dim_734                11\n",
      "dim_735                11\n",
      "dim_736                11\n",
      "dim_737                11\n",
      "dim_738                11\n",
      "dim_739                11\n",
      "dim_740                11\n",
      "dim_741                11\n",
      "dim_742                11\n",
      "dim_743                11\n",
      "dim_744                11\n",
      "dim_745                11\n",
      "dim_746                11\n",
      "dim_747                11\n",
      "dim_748                11\n",
      "dim_749                11\n",
      "dim_750                11\n",
      "dim_751                11\n",
      "dim_752                11\n",
      "dim_753                11\n",
      "dim_754                11\n",
      "dim_755                11\n",
      "dim_756                11\n",
      "dim_757                11\n",
      "dim_758                11\n",
      "dim_759                11\n",
      "dim_760                11\n",
      "dim_761                11\n",
      "dim_762                11\n",
      "dim_763                11\n",
      "dim_764                11\n",
      "dim_765                11\n",
      "dim_766                11\n",
      "dim_767                11\n",
      "\n",
      "We recommend preprocessing discrete columns that can have many values, using 'update_transformers'. Or you may drop columns that are not necessary to model. (Exit this script using ctrl-C)\n",
      "\n",
      "Generando 1979 muestras sintéticas...\n",
      "\n",
      "Distribución de sub-categorías DESPUÉS del aumento:\n",
      "sub_label_str\n",
      "Race                   638\n",
      "Sexual Orientation     591\n",
      "Gender                 434\n",
      "Class                  199\n",
      "Behavior               187\n",
      "Ethnicity              195\n",
      "Physical Appearance    245\n",
      "Disability             200\n",
      "Religion               200\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"--- Preparando datos para aumento ---\")\n",
    "# 1. Aislar los datos de entrenamiento que son 'odio'\n",
    "df_train_hate = df_train[df_train['main_label'] == 1].copy()\n",
    "features_to_augment = ['sub_label_str'] + embedding_cols\n",
    "df_to_augment = df_train_hate[features_to_augment]\n",
    "\n",
    "print(\"Distribución de sub-categorías ANTES del aumento:\")\n",
    "hate_counts = df_to_augment['sub_label_str'].value_counts()\n",
    "print(hate_counts)\n",
    "\n",
    "# Crear un nuevo encoder dedicado SOLO para las sub-categorías de odio.\n",
    "sub_hate_only_encoder = LabelEncoder()\n",
    "df_synthetic = pd.DataFrame() # Inicializar como dataframe vacío\n",
    "\n",
    "if len(hate_counts) > 1 and not df_to_augment.empty:\n",
    "    # Ajustar el nuevo encoder solo con las etiquetas de odio\n",
    "    sub_hate_only_encoder.fit(df_to_augment['sub_label_str'])\n",
    "    print(\"\\nNuevo encoder para Nivel 2 creado. Clases:\", sub_hate_only_encoder.classes_)\n",
    "\n",
    "    # 2. Configurar metadatos\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(data=df_to_augment)\n",
    "\n",
    "    print(\"\\nActualizando metadatos para tratar embeddings como numéricos continuos...\")\n",
    "    for col in embedding_cols:\n",
    "        metadata.update_column(column_name=col, sdtype='numerical')\n",
    "    metadata.update_column(column_name='sub_label_str', sdtype='categorical')\n",
    "\n",
    "    # 3. Configurar y entrenar el sintetizador CTGAN\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    print(f\"Usando GPU: {use_gpu}\")\n",
    "    synthesizer = CTGANSynthesizer(\n",
    "        metadata,\n",
    "        epochs=150,\n",
    "        embedding_dim=64,\n",
    "        verbose=False,\n",
    "        cuda=use_gpu\n",
    "    )\n",
    "\n",
    "    print(f\"\\nEntrenando CTGAN para generar datos sintéticos... (Usando GPU: {use_gpu})\")\n",
    "    synthesizer.fit(df_to_augment)\n",
    "\n",
    "    # 4. Determinar cuántas muestras generar\n",
    "    max_class_size = hate_counts.max()\n",
    "    num_to_generate = max_class_size * len(hate_counts) - hate_counts.sum()\n",
    "\n",
    "    # 5. Generar y combinar datos\n",
    "    if num_to_generate > 0:\n",
    "        print(f\"\\nGenerando {num_to_generate} muestras sintéticas...\")\n",
    "        df_synthetic = synthesizer.sample(num_rows=num_to_generate)\n",
    "        df_train_hate_balanced = pd.concat([df_to_augment, df_synthetic], ignore_index=True)\n",
    "    else:\n",
    "        df_train_hate_balanced = df_to_augment\n",
    "else:\n",
    "    print(\"\\nSolo una sub-categoría presente o no hay datos de odio, no se requiere aumento.\")\n",
    "    df_train_hate_balanced = df_to_augment\n",
    "    if not df_to_augment.empty:\n",
    "        # Ajustar el encoder si solo hay una clase\n",
    "        sub_hate_only_encoder.fit(df_to_augment['sub_label_str'])\n",
    "\n",
    "print(\"\\nDistribución de sub-categorías DESPUÉS del aumento:\")\n",
    "all_sub_labels = df_to_augment['sub_label_str'].unique()\n",
    "print(df_train_hate_balanced['sub_label_str'].value_counts().reindex(all_sub_labels, fill_value=0))\n",
    "\n",
    "# Preparar datos de entrenamiento para el clasificador de sub-categorías\n",
    "if not df_train_hate_balanced.empty:\n",
    "    X_train_sub_emb = df_train_hate_balanced[embedding_cols].values\n",
    "    # Usar el NUEVO encoder para transformar las etiquetas\n",
    "    y_train_sub = sub_hate_only_encoder.transform(df_train_hate_balanced['sub_label_str'])\n",
    "else:\n",
    "    # Crear arrays vacíos si no hay datos para evitar errores posteriores\n",
    "    X_train_sub_emb = np.array([]).reshape(0, len(embedding_cols))\n",
    "    y_train_sub = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main_classifier_md",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento del Clasificador Principal (Nivel 1) con Optuna y Ensemble Extendido\n",
    "\n",
    "Aquí es donde integramos el pipeline de entrenamiento robusto. Entrenaremos y optimizaremos un **ensemble de seis modelos** (XGBoost y MLP usando solo embeddings, los mismos dos usando embeddings + TF-IDF, y dos Regresiones Logísticas usando cada tipo de característica por separado). Este proceso no utiliza los datos aumentados, solo el conjunto de entrenamiento original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "main_classifier_train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparando datos para el entrenamiento del Clasificador Principal ---\n",
      "TF-IDF: 10000 características generadas.\n",
      "\n",
      "✓ Datos escalados, vectorizados y tensores de PyTorch listos para el Nivel 1.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"--- Preparando datos para el entrenamiento del Clasificador Principal ---\")\n",
    "\n",
    "# Usar las variables correctas del split jerárquico\n",
    "y_train = df_train['main_label'].values\n",
    "y_val = df_val['main_label'].values\n",
    "num_classes = len(np.unique(y_train)) # Será 2 en este caso\n",
    "\n",
    "# 1. Escalar características de embeddings\n",
    "scaler_L1_emb = StandardScaler()\n",
    "X_train_emb = df_train[embedding_cols].values\n",
    "X_train_emb_scaled = scaler_L1_emb.fit_transform(X_train_emb)\n",
    "X_val_emb = df_val[embedding_cols].values\n",
    "X_val_emb_scaled = scaler_L1_emb.transform(X_val_emb)\n",
    "\n",
    "# 2. Vectorizar características de texto con TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "X_train_text = df_train['text_stemmed'].values\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_val_text = df_val['text_stemmed'].values\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val_text)\n",
    "print(f\"TF-IDF: {X_train_tfidf.shape[1]} características generadas.\")\n",
    "\n",
    "# 3. Crear características combinadas (Embeddings + TF-IDF)\n",
    "# Para XGBoost y modelos de Scikit-learn, usamos la matriz sparse combinada\n",
    "X_train_combined = hstack([X_train_emb, X_train_tfidf]).tocsr()\n",
    "X_val_combined = hstack([X_val_emb, X_val_tfidf]).tocsr()\n",
    "\n",
    "# Para MLP, necesitamos una matriz densa. Combinamos embeddings escalados y TF-IDF denso.\n",
    "X_train_combined_dense = np.hstack([X_train_emb_scaled, X_train_tfidf.toarray()])\n",
    "X_val_combined_dense = np.hstack([X_val_emb_scaled, X_val_tfidf.toarray()])\n",
    "\n",
    "# 4. Convertir datos a tensores para PyTorch\n",
    "X_val_torch_emb = torch.tensor(X_val_emb_scaled, dtype=torch.float32).to(device)\n",
    "X_val_torch_combined = torch.tensor(X_val_combined_dense, dtype=torch.float32).to(device)\n",
    "y_val_torch = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "print(\"\\n✓ Datos escalados, vectorizados y tensores de PyTorch listos para el Nivel 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5533809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones objetivo de Optuna para el Nivel 1 definidas.\n"
     ]
    }
   ],
   "source": [
    "# --- Clase genérica para MLP ---\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size, activation_fn, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(current_size, hidden_size))\n",
    "            layers.append(activation_fn())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            current_size = hidden_size\n",
    "        layers.append(nn.Linear(current_size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Función genérica para entrenar y evaluar MLP en Optuna ---\n",
    "def train_eval_mlp_objective(trial, X_train_data, y_train_data, X_val_tensor, y_val_tensor, input_dim):\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    hidden_layers = [trial.suggest_int(f'n_units_l{i}', 32, 256) for i in range(n_layers)]\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop'])\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    activation_fn = getattr(nn, trial.suggest_categorical('activation', ['ReLU', 'Tanh']))\n",
    "\n",
    "    model = MLP(input_dim, hidden_layers, num_classes, activation_fn, dropout_rate).to(device)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train_data, dtype=torch.float32), torch.tensor(y_train_data, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    for epoch in range(25):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(data), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = criterion(model(X_val_tensor), y_val_tensor).item()\n",
    "\n",
    "    trial.report(val_loss, epoch)\n",
    "    if trial.should_prune(): raise optuna.exceptions.TrialPruned()\n",
    "    return val_loss\n",
    "\n",
    "# --- 1. Objetivo para MLP (Usa solo Embeddings) ---\n",
    "def objective_mlp_embeddings(trial):\n",
    "    return train_eval_mlp_objective(trial, X_train_emb_scaled, y_train, X_val_torch_emb, y_val_torch, X_train_emb_scaled.shape[1])\n",
    "\n",
    "# --- 2. Objetivo para MLP (Usa Embeddings + TF-IDF) ---\n",
    "def objective_mlp_combined(trial):\n",
    "    return train_eval_mlp_objective(trial, X_train_combined_dense, y_train, X_val_torch_combined, y_val_torch, X_train_combined_dense.shape[1])\n",
    "\n",
    "# --- 3. Objetivo para XGBoost (Usa solo Embeddings) ---\n",
    "def objective_xgboost_embeddings(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic', 'eval_metric': 'logloss',\n",
    "        'device': 'cuda' if device.type == 'cuda' else 'cpu',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**params, early_stopping_rounds=10)\n",
    "    model.fit(X_train_emb, y_train, eval_set=[(X_val_emb, y_val)], verbose=False)\n",
    "    return log_loss(y_val, model.predict_proba(X_val_emb))\n",
    "\n",
    "# --- 4. Objetivo para XGBoost (Usa Embeddings + TF-IDF) ---\n",
    "def objective_xgboost_combined(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic', 'eval_metric': 'logloss',\n",
    "        'device': 'cuda' if device.type == 'cuda' else 'cpu',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**params, early_stopping_rounds=10)\n",
    "    model.fit(X_train_combined, y_train, eval_set=[(X_val_combined, y_val)], verbose=False)\n",
    "    return log_loss(y_val, model.predict_proba(X_val_combined))\n",
    "\n",
    "# --- 5. Objetivo para Regresión Logística (Usa solo Embeddings) ---\n",
    "def objective_logistic_embeddings(trial):\n",
    "    params = {'C': trial.suggest_float('C', 1e-4, 1e2, log=True), 'solver': 'liblinear', 'max_iter': 1000}\n",
    "    model = LogisticRegression(**params, random_state=42)\n",
    "    model.fit(X_train_emb_scaled, y_train)\n",
    "    return log_loss(y_val, model.predict_proba(X_val_emb_scaled))\n",
    "\n",
    "# --- 6. Objetivo para Regresión Logística (Usa solo TF-IDF) ---\n",
    "def objective_logistic_tfidf(trial):\n",
    "    params = {'C': trial.suggest_float('C', 1e-2, 1e2, log=True), 'solver': 'liblinear', 'max_iter': 1000}\n",
    "    model = LogisticRegression(**params, random_state=42)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    return log_loss(y_val, model.predict_proba(X_val_tfidf))\n",
    "\n",
    "\n",
    "def objective_transformer_text(trial):\n",
    "    params = {\n",
    "        'embed_size': trial.suggest_categorical('embed_size', [64, 128]),\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 2),\n",
    "        'heads': trial.suggest_categorical('heads', [2, 4]),\n",
    "        'forward_expansion': trial.suggest_int('forward_expansion', 1, 2),\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.3),\n",
    "        'lr': trial.suggest_loguniform('lr', 1e-5, 1e-3),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64])\n",
    "    }\n",
    "    vocab = build_vocab(df_train['text'], min_freq=5)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = TransformerClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_size=params['embed_size'],\n",
    "        num_layers=params['num_layers'],\n",
    "        heads=params['heads'],\n",
    "        forward_expansion=params['forward_expansion'],\n",
    "        dropout=params['dropout'],\n",
    "        num_classes=len(np.unique(y_train)),\n",
    "        max_length=100,\n",
    "        device=device\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    ds_train = HateSpeechDataset(\n",
    "        df_train['text'].tolist(), y_train, None, vocab, max_len=100)\n",
    "    ds_val = HateSpeechDataset(\n",
    "        df_val['text'].tolist(), y_val, None, vocab, max_len=100)\n",
    "    loader_train = DataLoader(\n",
    "        ds_train, batch_size=params['batch_size'], shuffle=True)\n",
    "    loader_val = DataLoader(ds_val, batch_size=params['batch_size'])\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for x_batch, y_batch, _ in loader_train:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch, _ in loader_val:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            logits = model(x_batch)\n",
    "            total_loss += criterion(logits, y_batch).item() * x_batch.size(0)\n",
    "    return total_loss / len(ds_val)\n",
    "\n",
    "\n",
    "print(f\"Funciones objetivo de Optuna para el Nivel 1 definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b520e155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:08:09,864] A new study created in memory with name: no-name-bad67628-38e1-4827-8a2f-4a730bb9f495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optimizando XGBoost_Embeddings (Nivel 1) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c18e0b5c78b4907bb7491ff2778e733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:08:11,761] Trial 0 finished with value: 0.38387531361845034 and parameters: {'n_estimators': 437, 'learning_rate': 0.2536999076681772, 'max_depth': 8}. Best is trial 0 with value: 0.38387531361845034.\n",
      "[I 2025-06-27 02:08:16,976] Trial 1 finished with value: 0.33203744800196816 and parameters: {'n_estimators': 639, 'learning_rate': 0.01700037298921102, 'max_depth': 4}. Best is trial 1 with value: 0.33203744800196816.\n",
      "[I 2025-06-27 02:08:18,733] Trial 2 finished with value: 0.34653860977545825 and parameters: {'n_estimators': 152, 'learning_rate': 0.19030368381735815, 'max_depth': 7}. Best is trial 1 with value: 0.33203744800196816.\n",
      "[I 2025-06-27 02:08:52,533] Trial 3 finished with value: 0.36257044383563813 and parameters: {'n_estimators': 737, 'learning_rate': 0.010725209743171996, 'max_depth': 9}. Best is trial 1 with value: 0.33203744800196816.\n",
      "[I 2025-06-27 02:08:57,602] Trial 4 finished with value: 0.3305336990990282 and parameters: {'n_estimators': 850, 'learning_rate': 0.020589728197687916, 'max_depth': 4}. Best is trial 4 with value: 0.3305336990990282.\n",
      "[I 2025-06-27 02:09:03,747] Trial 5 finished with value: 0.34424432159254986 and parameters: {'n_estimators': 265, 'learning_rate': 0.028145092716060652, 'max_depth': 6}. Best is trial 4 with value: 0.3305336990990282.\n",
      "[I 2025-06-27 02:09:11,808] Trial 6 finished with value: 0.3548912509267969 and parameters: {'n_estimators': 489, 'learning_rate': 0.02692655251486473, 'max_depth': 7}. Best is trial 4 with value: 0.3305336990990282.\n",
      "[I 2025-06-27 02:09:15,161] Trial 7 finished with value: 0.3422113195717719 and parameters: {'n_estimators': 225, 'learning_rate': 0.027010527749605478, 'max_depth': 5}. Best is trial 4 with value: 0.3305336990990282.\n",
      "[I 2025-06-27 02:09:16,119] Trial 8 finished with value: 0.346012060924221 and parameters: {'n_estimators': 510, 'learning_rate': 0.14447746112718687, 'max_depth': 4}. Best is trial 4 with value: 0.3305336990990282.\n",
      "[I 2025-06-27 02:09:17,406] Trial 9 finished with value: 0.3360646012623698 and parameters: {'n_estimators': 563, 'learning_rate': 0.07500118950416987, 'max_depth': 3}. Best is trial 4 with value: 0.3305336990990282.\n",
      "[I 2025-06-27 02:09:18,727] Trial 10 finished with value: 0.33487597927636664 and parameters: {'n_estimators': 981, 'learning_rate': 0.06690992453172917, 'max_depth': 3}. Best is trial 4 with value: 0.3305336990990282.\n",
      "[I 2025-06-27 02:09:26,780] Trial 11 finished with value: 0.34039046437551573 and parameters: {'n_estimators': 847, 'learning_rate': 0.010464817979692459, 'max_depth': 5}. Best is trial 4 with value: 0.3305336990990282.\n",
      "[I 2025-06-27 02:09:32,248] Trial 12 finished with value: 0.3290499727246669 and parameters: {'n_estimators': 710, 'learning_rate': 0.018042210081692857, 'max_depth': 4}. Best is trial 12 with value: 0.3290499727246669.\n",
      "[I 2025-06-27 02:09:34,162] Trial 13 finished with value: 0.3311531594446315 and parameters: {'n_estimators': 814, 'learning_rate': 0.04616820325874795, 'max_depth': 4}. Best is trial 12 with value: 0.3290499727246669.\n",
      "[I 2025-06-27 02:09:40,109] Trial 14 finished with value: 0.336406758133346 and parameters: {'n_estimators': 939, 'learning_rate': 0.016847807817382127, 'max_depth': 5}. Best is trial 12 with value: 0.3290499727246669.\n",
      "[I 2025-06-27 02:09:41,688] Trial 15 finished with value: 0.3419567990384101 and parameters: {'n_estimators': 699, 'learning_rate': 0.042746116476006964, 'max_depth': 3}. Best is trial 12 with value: 0.3290499727246669.\n",
      "[I 2025-06-27 02:09:49,576] Trial 16 finished with value: 0.34478171392966345 and parameters: {'n_estimators': 841, 'learning_rate': 0.017776876104979536, 'max_depth': 6}. Best is trial 12 with value: 0.3290499727246669.\n",
      "[I 2025-06-27 02:09:50,815] Trial 17 finished with value: 0.33325761838754325 and parameters: {'n_estimators': 389, 'learning_rate': 0.10598541606403505, 'max_depth': 4}. Best is trial 12 with value: 0.3290499727246669.\n",
      "[I 2025-06-27 02:09:54,656] Trial 18 finished with value: 0.3295894344346594 and parameters: {'n_estimators': 665, 'learning_rate': 0.035416303050094244, 'max_depth': 5}. Best is trial 12 with value: 0.3290499727246669.\n",
      "[I 2025-06-27 02:09:59,557] Trial 19 finished with value: 0.3442638886463166 and parameters: {'n_estimators': 598, 'learning_rate': 0.033982808664331884, 'max_depth': 6}. Best is trial 12 with value: 0.3290499727246669.\n",
      "[I 2025-06-27 02:10:01,536] Trial 20 finished with value: 0.3351838216208133 and parameters: {'n_estimators': 706, 'learning_rate': 0.05983499452183108, 'max_depth': 5}. Best is trial 12 with value: 0.3290499727246669.\n",
      "[I 2025-06-27 02:10:06,610] Trial 21 finished with value: 0.32813981549457233 and parameters: {'n_estimators': 781, 'learning_rate': 0.019559150916562313, 'max_depth': 4}. Best is trial 21 with value: 0.32813981549457233.\n",
      "[I 2025-06-27 02:10:14,064] Trial 22 finished with value: 0.3383630359239654 and parameters: {'n_estimators': 757, 'learning_rate': 0.013903110258609493, 'max_depth': 5}. Best is trial 21 with value: 0.32813981549457233.\n",
      "[I 2025-06-27 02:10:16,624] Trial 23 finished with value: 0.3362170903256609 and parameters: {'n_estimators': 620, 'learning_rate': 0.03672682314795206, 'max_depth': 3}. Best is trial 21 with value: 0.32813981549457233.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:10:21,337] A new study created in memory with name: no-name-f78b918b-2f60-485b-9d3c-04e34160ada1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:10:21,331] Trial 24 finished with value: 0.32716216259561587 and parameters: {'n_estimators': 677, 'learning_rate': 0.02384310119963715, 'max_depth': 4}. Best is trial 24 with value: 0.32716216259561587.\n",
      "✓ XGBoost_Embeddings completado. Mejor LogLoss: 0.3272\n",
      "\n",
      "--- Optimizando MLP_PyTorch_Embeddings (Nivel 1) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230cb08b34174d9e9aae4289c0fbb5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:10:23,245] Trial 0 finished with value: 0.3736172616481781 and parameters: {'n_layers': 2, 'n_units_l0': 245, 'n_units_l1': 196, 'dropout_rate': 0.3394633936788146, 'optimizer': 'Adam', 'lr': 1.493656855461762e-05, 'activation': 'ReLU'}. Best is trial 0 with value: 0.3736172616481781.\n",
      "[I 2025-06-27 02:10:25,304] Trial 1 finished with value: 0.37159648537635803 and parameters: {'n_layers': 3, 'n_units_l0': 36, 'n_units_l1': 250, 'n_units_l2': 219, 'dropout_rate': 0.18493564427131048, 'optimizer': 'RMSprop', 'lr': 8.17949947521167e-05, 'activation': 'ReLU'}. Best is trial 1 with value: 0.37159648537635803.\n",
      "[I 2025-06-27 02:10:26,857] Trial 2 finished with value: 0.3755982518196106 and parameters: {'n_layers': 1, 'n_units_l0': 169, 'dropout_rate': 0.15579754426081674, 'optimizer': 'RMSprop', 'lr': 0.00023345864076016249, 'activation': 'ReLU'}. Best is trial 1 with value: 0.37159648537635803.\n",
      "[I 2025-06-27 02:10:28,570] Trial 3 finished with value: 1.0257515907287598 and parameters: {'n_layers': 2, 'n_units_l0': 165, 'n_units_l1': 42, 'dropout_rate': 0.34301794076057535, 'optimizer': 'Adam', 'lr': 0.007025166339242158, 'activation': 'ReLU'}. Best is trial 1 with value: 0.37159648537635803.\n",
      "[I 2025-06-27 02:10:30,263] Trial 4 finished with value: 0.32342520356178284 and parameters: {'n_layers': 1, 'n_units_l0': 53, 'dropout_rate': 0.3736932106048628, 'optimizer': 'Adam', 'lr': 0.0003058656666978527, 'activation': 'Tanh'}. Best is trial 4 with value: 0.32342520356178284.\n",
      "[I 2025-06-27 02:10:31,817] Trial 5 finished with value: 0.31547442078590393 and parameters: {'n_layers': 1, 'n_units_l0': 181, 'dropout_rate': 0.2246844304357644, 'optimizer': 'RMSprop', 'lr': 3.585612610345396e-05, 'activation': 'ReLU'}. Best is trial 5 with value: 0.31547442078590393.\n",
      "[I 2025-06-27 02:10:33,918] Trial 6 pruned. \n",
      "[I 2025-06-27 02:10:36,140] Trial 7 pruned. \n",
      "[I 2025-06-27 02:10:37,726] Trial 8 finished with value: 0.34817591309547424 and parameters: {'n_layers': 1, 'n_units_l0': 215, 'dropout_rate': 0.38274293753904687, 'optimizer': 'RMSprop', 'lr': 1.667761543019792e-05, 'activation': 'ReLU'}. Best is trial 5 with value: 0.31547442078590393.\n",
      "[I 2025-06-27 02:10:40,021] Trial 9 pruned. \n",
      "[I 2025-06-27 02:10:41,658] Trial 10 pruned. \n",
      "[I 2025-06-27 02:10:43,706] Trial 11 finished with value: 0.3089737594127655 and parameters: {'n_layers': 2, 'n_units_l0': 34, 'n_units_l1': 245, 'dropout_rate': 0.25184169736808953, 'optimizer': 'Adam', 'lr': 4.988267746250603e-05, 'activation': 'Tanh'}. Best is trial 11 with value: 0.3089737594127655.\n",
      "[I 2025-06-27 02:10:45,831] Trial 12 finished with value: 0.3189548850059509 and parameters: {'n_layers': 2, 'n_units_l0': 91, 'n_units_l1': 243, 'dropout_rate': 0.2604822409543778, 'optimizer': 'Adam', 'lr': 4.536300733735301e-05, 'activation': 'Tanh'}. Best is trial 11 with value: 0.3089737594127655.\n",
      "[I 2025-06-27 02:10:47,677] Trial 13 finished with value: 0.3061191439628601 and parameters: {'n_layers': 2, 'n_units_l0': 204, 'n_units_l1': 203, 'dropout_rate': 0.2800867957510187, 'optimizer': 'RMSprop', 'lr': 4.18792498067155e-05, 'activation': 'Tanh'}. Best is trial 13 with value: 0.3061191439628601.\n",
      "[I 2025-06-27 02:10:49,459] Trial 14 finished with value: 0.3265221118927002 and parameters: {'n_layers': 2, 'n_units_l0': 74, 'n_units_l1': 206, 'dropout_rate': 0.2752520669964538, 'optimizer': 'Adam', 'lr': 0.00012367316948011653, 'activation': 'Tanh'}. Best is trial 13 with value: 0.3061191439628601.\n",
      "[I 2025-06-27 02:10:51,149] Trial 15 pruned. \n",
      "[I 2025-06-27 02:10:53,132] Trial 16 finished with value: 0.32538217306137085 and parameters: {'n_layers': 2, 'n_units_l0': 135, 'n_units_l1': 159, 'dropout_rate': 0.42818890127467685, 'optimizer': 'RMSprop', 'lr': 3.798895757932489e-05, 'activation': 'Tanh'}. Best is trial 13 with value: 0.3061191439628601.\n",
      "[I 2025-06-27 02:10:55,151] Trial 17 pruned. \n",
      "[I 2025-06-27 02:10:57,289] Trial 18 pruned. \n",
      "[I 2025-06-27 02:10:59,037] Trial 19 pruned. \n",
      "[I 2025-06-27 02:11:00,842] Trial 20 finished with value: 0.31732308864593506 and parameters: {'n_layers': 2, 'n_units_l0': 252, 'n_units_l1': 128, 'dropout_rate': 0.2555004456601164, 'optimizer': 'Adam', 'lr': 6.839906343579257e-05, 'activation': 'Tanh'}. Best is trial 13 with value: 0.3061191439628601.\n",
      "[I 2025-06-27 02:11:02,316] Trial 21 pruned. \n",
      "[I 2025-06-27 02:11:03,909] Trial 22 pruned. \n",
      "[I 2025-06-27 02:11:05,487] Trial 23 finished with value: 0.29869791865348816 and parameters: {'n_layers': 1, 'n_units_l0': 228, 'dropout_rate': 0.10259335298061568, 'optimizer': 'RMSprop', 'lr': 5.727585249331011e-05, 'activation': 'ReLU'}. Best is trial 23 with value: 0.29869791865348816.\n",
      "[I 2025-06-27 02:11:07,047] Trial 24 finished with value: 0.31251248717308044 and parameters: {'n_layers': 1, 'n_units_l0': 226, 'dropout_rate': 0.1046483846510213, 'optimizer': 'RMSprop', 'lr': 6.43745230754415e-05, 'activation': 'Tanh'}. Best is trial 23 with value: 0.29869791865348816.\n",
      "[I 2025-06-27 02:11:08,880] Trial 25 pruned. \n",
      "[I 2025-06-27 02:11:11,021] Trial 26 pruned. \n",
      "[I 2025-06-27 02:11:12,765] Trial 27 pruned. \n",
      "[I 2025-06-27 02:11:14,413] Trial 28 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:11:16,359] A new study created in memory with name: no-name-d3e86c9b-2e0d-4f1e-9d25-af80c24be61e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:11:16,355] Trial 29 pruned. \n",
      "✓ MLP_PyTorch_Embeddings completado. Mejor LogLoss: 0.2987\n",
      "\n",
      "--- Optimizando LogisticRegression_Embeddings (Nivel 1) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d642eff7209c47ccb02051a524258929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:11:16,852] Trial 0 finished with value: 0.31922370667745825 and parameters: {'C': 0.017670169402947963}. Best is trial 0 with value: 0.31922370667745825.\n",
      "[I 2025-06-27 02:11:19,663] Trial 1 finished with value: 1.5954983262409372 and parameters: {'C': 50.61576888752309}. Best is trial 0 with value: 0.31922370667745825.\n",
      "[I 2025-06-27 02:11:21,134] Trial 2 finished with value: 0.8099008380066611 and parameters: {'C': 2.465832945854912}. Best is trial 0 with value: 0.31922370667745825.\n",
      "[I 2025-06-27 02:11:22,208] Trial 3 finished with value: 0.5044080702350761 and parameters: {'C': 0.39079671568228835}. Best is trial 0 with value: 0.31922370667745825.\n",
      "[I 2025-06-27 02:11:22,415] Trial 4 finished with value: 0.4062108728314638 and parameters: {'C': 0.0008632008168602544}. Best is trial 0 with value: 0.31922370667745825.\n",
      "[I 2025-06-27 02:11:22,623] Trial 5 finished with value: 0.4062311002079151 and parameters: {'C': 0.0008629132190071859}. Best is trial 0 with value: 0.31922370667745825.\n",
      "[I 2025-06-27 02:11:22,757] Trial 6 finished with value: 0.4917986622376586 and parameters: {'C': 0.00022310108018679258}. Best is trial 0 with value: 0.31922370667745825.\n",
      "[I 2025-06-27 02:11:24,794] Trial 7 finished with value: 1.2866203383146544 and parameters: {'C': 15.741890047456648}. Best is trial 0 with value: 0.31922370667745825.\n",
      "[I 2025-06-27 02:11:25,800] Trial 8 finished with value: 0.5085935032217438 and parameters: {'C': 0.4042872735027334}. Best is trial 0 with value: 0.31922370667745825.\n",
      "[I 2025-06-27 02:11:27,186] Trial 9 finished with value: 0.7405204084879199 and parameters: {'C': 1.7718847354806828}. Best is trial 0 with value: 0.31922370667745825.\n",
      "[I 2025-06-27 02:11:27,614] Trial 10 finished with value: 0.31921507319868386 and parameters: {'C': 0.017654677164766052}. Best is trial 10 with value: 0.31921507319868386.\n",
      "[I 2025-06-27 02:11:28,030] Trial 11 finished with value: 0.3196379773365638 and parameters: {'C': 0.018388382022356858}. Best is trial 10 with value: 0.31921507319868386.\n",
      "[I 2025-06-27 02:11:28,414] Trial 12 finished with value: 0.3174502195707856 and parameters: {'C': 0.012798666702408415}. Best is trial 12 with value: 0.3174502195707856.\n",
      "[I 2025-06-27 02:11:28,819] Trial 13 finished with value: 0.31742841863663046 and parameters: {'C': 0.012053282859112415}. Best is trial 13 with value: 0.31742841863663046.\n",
      "[I 2025-06-27 02:11:29,111] Trial 14 finished with value: 0.3451372311768702 and parameters: {'C': 0.0028718250465162133}. Best is trial 13 with value: 0.31742841863663046.\n",
      "[I 2025-06-27 02:11:29,748] Trial 15 finished with value: 0.3535272136274817 and parameters: {'C': 0.061296196372412286}. Best is trial 13 with value: 0.31742841863663046.\n",
      "[I 2025-06-27 02:11:30,022] Trial 16 finished with value: 0.3389995445399849 and parameters: {'C': 0.003409264942037956}. Best is trial 13 with value: 0.31742841863663046.\n",
      "[I 2025-06-27 02:11:30,713] Trial 17 finished with value: 0.3731333431281197 and parameters: {'C': 0.08901581332229032}. Best is trial 13 with value: 0.31742841863663046.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:11:31,147] A new study created in memory with name: no-name-52c68b13-001c-47b3-8642-ec46a4e7fe15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:11:31,035] Trial 18 finished with value: 0.3328653443537924 and parameters: {'C': 0.00415403421342495}. Best is trial 13 with value: 0.31742841863663046.\n",
      "[I 2025-06-27 02:11:31,145] Trial 19 finished with value: 0.5296032495019642 and parameters: {'C': 0.00011996661220636725}. Best is trial 13 with value: 0.31742841863663046.\n",
      "✓ LogisticRegression_Embeddings completado. Mejor LogLoss: 0.3174\n",
      "\n",
      "--- Optimizando LogisticRegression_TFIDF (Nivel 1) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5017288f97941a091fe0955daa8b1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:11:31,173] Trial 0 finished with value: 0.3550133493487559 and parameters: {'C': 0.31489116479568624}. Best is trial 0 with value: 0.3550133493487559.\n",
      "[I 2025-06-27 02:11:31,209] Trial 1 finished with value: 0.18741625382705585 and parameters: {'C': 63.512210106407046}. Best is trial 1 with value: 0.18741625382705585.\n",
      "[I 2025-06-27 02:11:31,238] Trial 2 finished with value: 0.18731845507443304 and parameters: {'C': 8.471801418819979}. Best is trial 2 with value: 0.18731845507443304.\n",
      "[I 2025-06-27 02:11:31,270] Trial 3 finished with value: 0.21905261515471147 and parameters: {'C': 2.481040974867813}. Best is trial 2 with value: 0.18731845507443304.\n",
      "[I 2025-06-27 02:11:31,288] Trial 4 finished with value: 0.5142493051315309 and parameters: {'C': 0.04207988669606638}. Best is trial 2 with value: 0.18731845507443304.\n",
      "[I 2025-06-27 02:11:31,301] Trial 5 finished with value: 0.5142613164145062 and parameters: {'C': 0.042070539502879395}. Best is trial 2 with value: 0.18731845507443304.\n",
      "[I 2025-06-27 02:11:31,314] Trial 6 finished with value: 0.5508268234668355 and parameters: {'C': 0.017073967431528128}. Best is trial 2 with value: 0.18731845507443304.\n",
      "[I 2025-06-27 02:11:31,346] Trial 7 finished with value: 0.1818984343093288 and parameters: {'C': 29.154431891537552}. Best is trial 7 with value: 0.1818984343093288.\n",
      "[I 2025-06-27 02:11:31,369] Trial 8 finished with value: 0.21816153176948755 and parameters: {'C': 2.5378155082656657}. Best is trial 7 with value: 0.1818984343093288.\n",
      "[I 2025-06-27 02:11:31,399] Trial 9 finished with value: 0.19073711802511392 and parameters: {'C': 6.79657809075816}. Best is trial 7 with value: 0.1818984343093288.\n",
      "[I 2025-06-27 02:11:31,435] Trial 10 finished with value: 0.190332909190852 and parameters: {'C': 82.29631658321766}. Best is trial 7 with value: 0.1818984343093288.\n",
      "[I 2025-06-27 02:11:31,464] Trial 11 finished with value: 0.18139114851066793 and parameters: {'C': 20.996451336399733}. Best is trial 11 with value: 0.18139114851066793.\n",
      "[I 2025-06-27 02:11:31,500] Trial 12 finished with value: 0.1825460400789433 and parameters: {'C': 34.043053954671954}. Best is trial 11 with value: 0.18139114851066793.\n",
      "[I 2025-06-27 02:11:31,531] Trial 13 finished with value: 0.18169887193794626 and parameters: {'C': 17.14446414712941}. Best is trial 11 with value: 0.18139114851066793.\n",
      "[I 2025-06-27 02:11:31,550] Trial 14 finished with value: 0.3369985031439088 and parameters: {'C': 0.3880040916855655}. Best is trial 11 with value: 0.18139114851066793.\n",
      "[I 2025-06-27 02:11:31,581] Trial 15 finished with value: 0.1846401510272633 and parameters: {'C': 10.66261287572915}. Best is trial 11 with value: 0.18139114851066793.\n",
      "[I 2025-06-27 02:11:31,601] Trial 16 finished with value: 0.2709323414313157 and parameters: {'C': 0.9253254956089687}. Best is trial 11 with value: 0.18139114851066793.\n",
      "[I 2025-06-27 02:11:31,632] Trial 17 finished with value: 0.18150383823340172 and parameters: {'C': 18.63934843098346}. Best is trial 11 with value: 0.18139114851066793.\n",
      "[I 2025-06-27 02:11:31,662] Trial 18 finished with value: 0.2095475298690466 and parameters: {'C': 3.217618717445632}. Best is trial 11 with value: 0.18139114851066793.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:11:31,701] A new study created in memory with name: no-name-30f06e11-4b78-4194-bd60-18d7d34595c2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:11:31,698] Trial 19 finished with value: 0.19226438776594595 and parameters: {'C': 95.77799307217616}. Best is trial 11 with value: 0.18139114851066793.\n",
      "✓ LogisticRegression_TFIDF completado. Mejor LogLoss: 0.1814\n",
      "\n",
      "--- Optimizando XGBoost_Combined (Nivel 1) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdbb62d60e07498ba5338dfa1a12cc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:11:33,954] Trial 0 finished with value: 0.23442146335891542 and parameters: {'n_estimators': 437, 'learning_rate': 0.2536999076681772, 'max_depth': 8}. Best is trial 0 with value: 0.23442146335891542.\n",
      "[I 2025-06-27 02:11:41,093] Trial 1 finished with value: 0.21428953882813034 and parameters: {'n_estimators': 639, 'learning_rate': 0.01700037298921102, 'max_depth': 4}. Best is trial 1 with value: 0.21428953882813034.\n",
      "[I 2025-06-27 02:11:43,772] Trial 2 finished with value: 0.2262995167069346 and parameters: {'n_estimators': 152, 'learning_rate': 0.19030368381735815, 'max_depth': 7}. Best is trial 1 with value: 0.21428953882813034.\n",
      "[I 2025-06-27 02:12:22,931] Trial 3 finished with value: 0.21774815123160726 and parameters: {'n_estimators': 737, 'learning_rate': 0.010725209743171996, 'max_depth': 9}. Best is trial 1 with value: 0.21428953882813034.\n",
      "[I 2025-06-27 02:12:32,530] Trial 4 finished with value: 0.19919569516514132 and parameters: {'n_estimators': 850, 'learning_rate': 0.020589728197687916, 'max_depth': 4}. Best is trial 4 with value: 0.19919569516514132.\n",
      "[I 2025-06-27 02:12:39,248] Trial 5 finished with value: 0.21948883177492456 and parameters: {'n_estimators': 265, 'learning_rate': 0.028145092716060652, 'max_depth': 6}. Best is trial 4 with value: 0.19919569516514132.\n",
      "[I 2025-06-27 02:12:53,074] Trial 6 finished with value: 0.2120974959062768 and parameters: {'n_estimators': 489, 'learning_rate': 0.02692655251486473, 'max_depth': 7}. Best is trial 4 with value: 0.19919569516514132.\n",
      "[I 2025-06-27 02:12:57,166] Trial 7 finished with value: 0.2331094535514253 and parameters: {'n_estimators': 225, 'learning_rate': 0.027010527749605478, 'max_depth': 5}. Best is trial 4 with value: 0.19919569516514132.\n",
      "[I 2025-06-27 02:12:59,256] Trial 8 finished with value: 0.20625327213400255 and parameters: {'n_estimators': 510, 'learning_rate': 0.14447746112718687, 'max_depth': 4}. Best is trial 4 with value: 0.19919569516514132.\n",
      "[I 2025-06-27 02:13:02,362] Trial 9 finished with value: 0.1942350327066001 and parameters: {'n_estimators': 563, 'learning_rate': 0.07500118950416987, 'max_depth': 3}. Best is trial 9 with value: 0.1942350327066001.\n",
      "[I 2025-06-27 02:13:05,320] Trial 10 finished with value: 0.18927643032788272 and parameters: {'n_estimators': 951, 'learning_rate': 0.09121222976475842, 'max_depth': 3}. Best is trial 10 with value: 0.18927643032788272.\n",
      "[I 2025-06-27 02:13:07,606] Trial 11 finished with value: 0.20096791043939308 and parameters: {'n_estimators': 939, 'learning_rate': 0.08362963273390475, 'max_depth': 3}. Best is trial 10 with value: 0.18927643032788272.\n",
      "[I 2025-06-27 02:13:10,660] Trial 12 finished with value: 0.1964983348825202 and parameters: {'n_estimators': 981, 'learning_rate': 0.06691212209556527, 'max_depth': 3}. Best is trial 10 with value: 0.18927643032788272.\n",
      "[I 2025-06-27 02:13:13,156] Trial 13 finished with value: 0.19132481156653797 and parameters: {'n_estimators': 742, 'learning_rate': 0.10107637867112979, 'max_depth': 3}. Best is trial 10 with value: 0.18927643032788272.\n",
      "[I 2025-06-27 02:13:15,931] Trial 14 finished with value: 0.21133656826135278 and parameters: {'n_estimators': 775, 'learning_rate': 0.11200669617300979, 'max_depth': 5}. Best is trial 10 with value: 0.18927643032788272.\n",
      "[I 2025-06-27 02:13:21,295] Trial 15 finished with value: 0.20367401685949302 and parameters: {'n_estimators': 855, 'learning_rate': 0.042746116476006964, 'max_depth': 5}. Best is trial 10 with value: 0.18927643032788272.\n",
      "[I 2025-06-27 02:13:26,556] Trial 16 finished with value: 0.19100220693605102 and parameters: {'n_estimators': 687, 'learning_rate': 0.043461789946250766, 'max_depth': 3}. Best is trial 10 with value: 0.18927643032788272.\n",
      "[I 2025-06-27 02:13:31,106] Trial 17 finished with value: 0.19946479213402693 and parameters: {'n_estimators': 374, 'learning_rate': 0.04671156170810353, 'max_depth': 4}. Best is trial 10 with value: 0.18927643032788272.\n",
      "[I 2025-06-27 02:13:38,112] Trial 18 finished with value: 0.20703109034611034 and parameters: {'n_estimators': 665, 'learning_rate': 0.05018489632683118, 'max_depth': 6}. Best is trial 10 with value: 0.18927643032788272.\n",
      "[I 2025-06-27 02:13:44,857] Trial 19 finished with value: 0.19188258368126668 and parameters: {'n_estimators': 883, 'learning_rate': 0.035984135108248556, 'max_depth': 3}. Best is trial 10 with value: 0.18927643032788272.\n",
      "[I 2025-06-27 02:13:47,833] Trial 20 finished with value: 0.2121546604893177 and parameters: {'n_estimators': 1000, 'learning_rate': 0.14906874001324674, 'max_depth': 6}. Best is trial 10 with value: 0.18927643032788272.\n",
      "[I 2025-06-27 02:13:50,474] Trial 21 finished with value: 0.19308753459567413 and parameters: {'n_estimators': 731, 'learning_rate': 0.10389590734703241, 'max_depth': 3}. Best is trial 10 with value: 0.18927643032788272.\n",
      "[I 2025-06-27 02:13:54,835] Trial 22 finished with value: 0.199888340984361 and parameters: {'n_estimators': 635, 'learning_rate': 0.06839620407063442, 'max_depth': 4}. Best is trial 10 with value: 0.18927643032788272.\n",
      "[I 2025-06-27 02:13:57,515] Trial 23 finished with value: 0.19551462984608264 and parameters: {'n_estimators': 790, 'learning_rate': 0.09921948141026087, 'max_depth': 3}. Best is trial 10 with value: 0.18927643032788272.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:13:59,625] A new study created in memory with name: no-name-7ca82479-0465-4580-9d7d-8aa54070edf3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:13:59,618] Trial 24 finished with value: 0.20667310710623252 and parameters: {'n_estimators': 578, 'learning_rate': 0.1426931006740446, 'max_depth': 5}. Best is trial 10 with value: 0.18927643032788272.\n",
      "✓ XGBoost_Combined completado. Mejor LogLoss: 0.1893\n",
      "\n",
      "--- Optimizando MLP_PyTorch_Combined (Nivel 1) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380824ed7e164b7dab5dc2a056f52934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:14:02,098] Trial 0 finished with value: 0.3485220968723297 and parameters: {'n_layers': 2, 'n_units_l0': 245, 'n_units_l1': 196, 'dropout_rate': 0.3394633936788146, 'optimizer': 'Adam', 'lr': 1.493656855461762e-05, 'activation': 'ReLU'}. Best is trial 0 with value: 0.3485220968723297.\n",
      "[I 2025-06-27 02:14:04,911] Trial 1 finished with value: 0.38684558868408203 and parameters: {'n_layers': 3, 'n_units_l0': 36, 'n_units_l1': 250, 'n_units_l2': 219, 'dropout_rate': 0.18493564427131048, 'optimizer': 'RMSprop', 'lr': 8.17949947521167e-05, 'activation': 'ReLU'}. Best is trial 0 with value: 0.3485220968723297.\n",
      "[I 2025-06-27 02:14:07,227] Trial 2 finished with value: 0.3438335955142975 and parameters: {'n_layers': 1, 'n_units_l0': 169, 'dropout_rate': 0.15579754426081674, 'optimizer': 'RMSprop', 'lr': 0.00023345864076016249, 'activation': 'ReLU'}. Best is trial 2 with value: 0.3438335955142975.\n",
      "[I 2025-06-27 02:14:09,614] Trial 3 finished with value: 0.9470146298408508 and parameters: {'n_layers': 2, 'n_units_l0': 165, 'n_units_l1': 42, 'dropout_rate': 0.34301794076057535, 'optimizer': 'Adam', 'lr': 0.007025166339242158, 'activation': 'ReLU'}. Best is trial 2 with value: 0.3438335955142975.\n",
      "[I 2025-06-27 02:14:11,407] Trial 4 finished with value: 0.30484655499458313 and parameters: {'n_layers': 1, 'n_units_l0': 53, 'dropout_rate': 0.3736932106048628, 'optimizer': 'Adam', 'lr': 0.0003058656666978527, 'activation': 'Tanh'}. Best is trial 4 with value: 0.30484655499458313.\n",
      "[I 2025-06-27 02:14:13,148] Trial 5 finished with value: 0.2979767620563507 and parameters: {'n_layers': 1, 'n_units_l0': 181, 'dropout_rate': 0.2246844304357644, 'optimizer': 'RMSprop', 'lr': 3.585612610345396e-05, 'activation': 'ReLU'}. Best is trial 5 with value: 0.2979767620563507.\n",
      "[I 2025-06-27 02:14:15,385] Trial 6 pruned. \n",
      "[I 2025-06-27 02:14:17,865] Trial 7 pruned. \n",
      "[I 2025-06-27 02:14:19,675] Trial 8 finished with value: 0.3442705571651459 and parameters: {'n_layers': 1, 'n_units_l0': 215, 'dropout_rate': 0.38274293753904687, 'optimizer': 'RMSprop', 'lr': 1.667761543019792e-05, 'activation': 'ReLU'}. Best is trial 5 with value: 0.2979767620563507.\n",
      "[I 2025-06-27 02:14:22,065] Trial 9 pruned. \n",
      "[I 2025-06-27 02:14:24,045] Trial 10 pruned. \n",
      "[I 2025-06-27 02:14:26,323] Trial 11 finished with value: 0.29148733615875244 and parameters: {'n_layers': 2, 'n_units_l0': 34, 'n_units_l1': 245, 'dropout_rate': 0.25184169736808953, 'optimizer': 'Adam', 'lr': 4.988267746250603e-05, 'activation': 'Tanh'}. Best is trial 11 with value: 0.29148733615875244.\n",
      "[I 2025-06-27 02:14:28,678] Trial 12 finished with value: 0.2894899249076843 and parameters: {'n_layers': 2, 'n_units_l0': 91, 'n_units_l1': 243, 'dropout_rate': 0.2604822409543778, 'optimizer': 'Adam', 'lr': 4.536300733735301e-05, 'activation': 'Tanh'}. Best is trial 12 with value: 0.2894899249076843.\n",
      "[I 2025-06-27 02:14:31,084] Trial 13 finished with value: 0.28981682658195496 and parameters: {'n_layers': 2, 'n_units_l0': 75, 'n_units_l1': 255, 'dropout_rate': 0.2746020260935557, 'optimizer': 'Adam', 'lr': 5.285411192760535e-05, 'activation': 'Tanh'}. Best is trial 12 with value: 0.2894899249076843.\n",
      "[I 2025-06-27 02:14:33,365] Trial 14 pruned. \n",
      "[I 2025-06-27 02:14:35,637] Trial 15 pruned. \n",
      "[I 2025-06-27 02:14:38,085] Trial 16 finished with value: 0.2927795946598053 and parameters: {'n_layers': 2, 'n_units_l0': 125, 'n_units_l1': 199, 'dropout_rate': 0.42818890127467685, 'optimizer': 'Adam', 'lr': 3.798895757932489e-05, 'activation': 'Tanh'}. Best is trial 12 with value: 0.2894899249076843.\n",
      "[I 2025-06-27 02:14:40,445] Trial 17 pruned. \n",
      "[I 2025-06-27 02:14:43,133] Trial 18 pruned. \n",
      "[I 2025-06-27 02:14:45,488] Trial 19 pruned. \n",
      "[I 2025-06-27 02:14:47,986] Trial 20 finished with value: 0.29275020956993103 and parameters: {'n_layers': 2, 'n_units_l0': 102, 'n_units_l1': 127, 'dropout_rate': 0.19353635832015834, 'optimizer': 'Adam', 'lr': 6.087511034880959e-05, 'activation': 'Tanh'}. Best is trial 12 with value: 0.2894899249076843.\n",
      "[I 2025-06-27 02:14:50,428] Trial 21 pruned. \n",
      "[I 2025-06-27 02:14:53,099] Trial 22 finished with value: 0.29190197587013245 and parameters: {'n_layers': 2, 'n_units_l0': 56, 'n_units_l1': 256, 'dropout_rate': 0.3147601917269767, 'optimizer': 'Adam', 'lr': 5.747217557042981e-05, 'activation': 'Tanh'}. Best is trial 12 with value: 0.2894899249076843.\n",
      "[I 2025-06-27 02:14:55,470] Trial 23 pruned. \n",
      "[I 2025-06-27 02:14:57,754] Trial 24 pruned. \n",
      "[I 2025-06-27 02:15:00,117] Trial 25 pruned. \n",
      "[I 2025-06-27 02:15:02,992] Trial 26 pruned. \n",
      "[I 2025-06-27 02:15:05,109] Trial 27 pruned. \n",
      "[I 2025-06-27 02:15:07,492] Trial 28 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:15:09,772] A new study created in memory with name: no-name-0e5f11b1-f9d1-43f6-91fe-71cf81b9405e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 02:15:09,768] Trial 29 pruned. \n",
      "✓ MLP_PyTorch_Combined completado. Mejor LogLoss: 0.2895\n",
      "\n",
      "--- Optimizando Transformer_Text (Nivel 1) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fb0d1b20f24216af80d9693fec854d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-06-27 02:15:09,780] Trial 0 failed with parameters: {'embed_size': 128, 'num_layers': 2, 'heads': 2, 'forward_expansion': 1, 'dropout': 0.1116167224336399, 'lr': 0.0005399484409787432, 'batch_size': 64} because of the following error: KeyError('text').\n",
      "Traceback (most recent call last):\n",
      "  File \"h:\\Kevin\\Escritorio\\Kevin\\DetectingAggressionInTexts\\Env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3812, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pandas/_libs/index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7096, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'text'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"h:\\Kevin\\Escritorio\\Kevin\\DetectingAggressionInTexts\\Env\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\loboa\\AppData\\Local\\Temp\\ipykernel_12544\\636081721.py\", line 109, in objective_transformer_text\n",
      "    vocab = build_vocab(df_train['text'], min_freq=5)\n",
      "                        ~~~~~~~~^^^^^^^^\n",
      "  File \"h:\\Kevin\\Escritorio\\Kevin\\DetectingAggressionInTexts\\Env\\Lib\\site-packages\\pandas\\core\\frame.py\", line 4107, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"h:\\Kevin\\Escritorio\\Kevin\\DetectingAggressionInTexts\\Env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3819, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'text'\n",
      "[W 2025-06-27 02:15:09,854] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Kevin\\Escritorio\\Kevin\\DetectingAggressionInTexts\\Env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Optimizando \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (Nivel 1) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m study = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m, sampler=optuna.samplers.TPESampler(seed=\u001b[32m42\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mobjective_func\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mn_trials\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m model_results[model_name] = {\n\u001b[32m     19\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbest_params\u001b[39m\u001b[33m'\u001b[39m: study.best_params,\n\u001b[32m     20\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbest_score\u001b[39m\u001b[33m'\u001b[39m: study.best_value\n\u001b[32m     21\u001b[39m }\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m completado. Mejor LogLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Kevin\\Escritorio\\Kevin\\DetectingAggressionInTexts\\Env\\Lib\\site-packages\\optuna\\study\\study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Kevin\\Escritorio\\Kevin\\DetectingAggressionInTexts\\Env\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Kevin\\Escritorio\\Kevin\\DetectingAggressionInTexts\\Env\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Kevin\\Escritorio\\Kevin\\DetectingAggressionInTexts\\Env\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Kevin\\Escritorio\\Kevin\\DetectingAggressionInTexts\\Env\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mobjective_transformer_text\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobjective_transformer_text\u001b[39m(trial):\n\u001b[32m    100\u001b[39m     params = {\n\u001b[32m    101\u001b[39m         \u001b[33m'\u001b[39m\u001b[33membed_size\u001b[39m\u001b[33m'\u001b[39m: trial.suggest_categorical(\u001b[33m'\u001b[39m\u001b[33membed_size\u001b[39m\u001b[33m'\u001b[39m, [\u001b[32m64\u001b[39m, \u001b[32m128\u001b[39m]),\n\u001b[32m    102\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mnum_layers\u001b[39m\u001b[33m'\u001b[39m: trial.suggest_int(\u001b[33m'\u001b[39m\u001b[33mnum_layers\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m: trial.suggest_categorical(\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m, [\u001b[32m32\u001b[39m, \u001b[32m64\u001b[39m])\n\u001b[32m    108\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     vocab = build_vocab(\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m, min_freq=\u001b[32m5\u001b[39m)\n\u001b[32m    110\u001b[39m     device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    111\u001b[39m     model = TransformerClassifier(\n\u001b[32m    112\u001b[39m         vocab_size=\u001b[38;5;28mlen\u001b[39m(vocab),\n\u001b[32m    113\u001b[39m         embed_size=params[\u001b[33m'\u001b[39m\u001b[33membed_size\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    120\u001b[39m         device=device\n\u001b[32m    121\u001b[39m     ).to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Kevin\\Escritorio\\Kevin\\DetectingAggressionInTexts\\Env\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Kevin\\Escritorio\\Kevin\\DetectingAggressionInTexts\\Env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'text'"
     ]
    }
   ],
   "source": [
    "models_config = {\n",
    "    'XGBoost_Embeddings': {'objective_func': objective_xgboost_embeddings, 'n_trials': 25},\n",
    "    'MLP_PyTorch_Embeddings': {'objective_func': objective_mlp_embeddings, 'n_trials': 30},\n",
    "    'LogisticRegression_Embeddings': {'objective_func': objective_logistic_embeddings, 'n_trials': 20},\n",
    "    'LogisticRegression_TFIDF': {'objective_func': objective_logistic_tfidf, 'n_trials': 20},\n",
    "    'XGBoost_Combined': {'objective_func': objective_xgboost_combined, 'n_trials': 25},\n",
    "    'MLP_PyTorch_Combined': {'objective_func': objective_mlp_combined, 'n_trials': 30},\n",
    "    'Transformer_Text': {'objective_func': objective_transformer_text, 'n_trials': 20},\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model_name, config in models_config.items():\n",
    "    print(f\"\\n--- Optimizando {model_name} (Nivel 1) ---\")\n",
    "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(config['objective_func'], n_trials=config['n_trials'], show_progress_bar=True)\n",
    "\n",
    "    model_results[model_name] = {\n",
    "        'best_params': study.best_params,\n",
    "        'best_score': study.best_value\n",
    "    }\n",
    "    print(f\"✓ {model_name} completado. Mejor LogLoss: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classifier_models = {}\n",
    "print(\"--- Entrenando modelos finales con los mejores hiperparámetros ---\\n\")\n",
    "\n",
    "# 1. XGBoost (Embeddings)\n",
    "params = model_results['XGBoost_Embeddings']['best_params']\n",
    "final_xgb_emb = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss',\n",
    "                                  device='cuda' if device.type == 'cuda' else 'cpu', **params)\n",
    "final_xgb_emb.fit(X_train_emb, y_train)\n",
    "main_classifier_models['XGBoost_Embeddings'] = final_xgb_emb\n",
    "print(\"✓ Modelo XGBoost (Embeddings) final entrenado.\")\n",
    "\n",
    "# 2. XGBoost (Combined)\n",
    "params = model_results['XGBoost_Combined']['best_params']\n",
    "final_xgb_comb = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss',\n",
    "                                   device='cuda' if device.type == 'cuda' else 'cpu', **params)\n",
    "final_xgb_comb.fit(X_train_combined, y_train)\n",
    "main_classifier_models['XGBoost_Combined'] = final_xgb_comb\n",
    "print(\"✓ Modelo XGBoost (Combined) final entrenado.\")\n",
    "\n",
    "# 3. Regresión Logística (Embeddings)\n",
    "params = model_results['LogisticRegression_Embeddings']['best_params']\n",
    "final_log_emb = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000, **params)\n",
    "final_log_emb.fit(X_train_emb_scaled, y_train)\n",
    "main_classifier_models['LogisticRegression_Embeddings'] = final_log_emb\n",
    "print(\"✓ Modelo Regresión Logística (Embeddings) final entrenado.\")\n",
    "\n",
    "# 4. Regresión Logística (TF-IDF)\n",
    "params = model_results['LogisticRegression_TFIDF']['best_params']\n",
    "final_log_tfidf = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000, **params)\n",
    "final_log_tfidf.fit(X_train_tfidf, y_train)\n",
    "main_classifier_models['LogisticRegression_TFIDF'] = final_log_tfidf\n",
    "print(\"✓ Modelo Regresión Logística (TF-IDF) final entrenado.\")\n",
    "\n",
    "# 5. MLP (Embeddings)\n",
    "params = model_results['MLP_PyTorch_Embeddings']['best_params']\n",
    "hidden_layers = [params[f'n_units_l{i}'] for i in range(params['n_layers'])]\n",
    "final_mlp_emb = MLP(X_train_emb_scaled.shape[1], hidden_layers, num_classes,\n",
    "                    getattr(nn, params['activation']), params['dropout_rate']).to(device)\n",
    "optimizer = getattr(optim, params['optimizer'])(final_mlp_emb.parameters(), lr=params['lr'])\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_train_emb_scaled, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)), batch_size=128, shuffle=True)\n",
    "for epoch in tqdm(range(30), desc=\"Epochs MLP (Embeddings) final\"):\n",
    "    final_mlp_emb.train()\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = nn.CrossEntropyLoss()(final_mlp_emb(data), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "main_classifier_models['MLP_PyTorch_Embeddings'] = final_mlp_emb.eval()\n",
    "print(\"✓ Modelo MLP (Embeddings) final entrenado.\")\n",
    "\n",
    "# 6. MLP (Combined)\n",
    "params = model_results['MLP_PyTorch_Combined']['best_params']\n",
    "hidden_layers = [params[f'n_units_l{i}'] for i in range(params['n_layers'])]\n",
    "final_mlp_comb = MLP(X_train_combined_dense.shape[1], hidden_layers, num_classes,\n",
    "                     getattr(nn, params['activation']), params['dropout_rate']).to(device)\n",
    "optimizer = getattr(optim, params['optimizer'])(final_mlp_comb.parameters(), lr=params['lr'])\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_train_combined_dense, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)), batch_size=128, shuffle=True)\n",
    "for epoch in tqdm(range(30), desc=\"Epochs MLP (Combined) final\"):\n",
    "    final_mlp_comb.train()\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = nn.CrossEntropyLoss()(final_mlp_comb(data), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "main_classifier_models['MLP_PyTorch_Combined'] = final_mlp_comb.eval()\n",
    "print(\"✓ Modelo MLP (Combined) final entrenado.\")\n",
    "\n",
    "print(\"\\n✓ Todos los modelos finales han sido entrenados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Calculando pesos para el Ensemble del Clasificador Principal (Nivel 1) ---\")\n",
    "val_probas = {}\n",
    "\n",
    "# Obtener predicciones de cada modelo en el set de validación\n",
    "val_probas['XGBoost_Embeddings'] = main_classifier_models['XGBoost_Embeddings'].predict_proba(X_val_emb)\n",
    "val_probas['XGBoost_Combined'] = main_classifier_models['XGBoost_Combined'].predict_proba(X_val_combined)\n",
    "val_probas['LogisticRegression_Embeddings'] = main_classifier_models['LogisticRegression_Embeddings'].predict_proba(X_val_emb_scaled)\n",
    "val_probas['LogisticRegression_TFIDF'] = main_classifier_models['LogisticRegression_TFIDF'].predict_proba(X_val_tfidf)\n",
    "with torch.no_grad():\n",
    "    # MLP Embeddings\n",
    "    mlp_outputs_emb = main_classifier_models['MLP_PyTorch_Embeddings'](X_val_torch_emb)\n",
    "    val_probas['MLP_PyTorch_Embeddings'] = torch.softmax(mlp_outputs_emb, dim=1).cpu().numpy()\n",
    "    # MLP Combined\n",
    "    mlp_outputs_comb = main_classifier_models['MLP_PyTorch_Combined'](X_val_torch_combined)\n",
    "    val_probas['MLP_PyTorch_Combined'] = torch.softmax(mlp_outputs_comb, dim=1).cpu().numpy()\n",
    "\n",
    "# Calcular métricas y pesos del ensemble (mayor peso a menor log_loss)\n",
    "losses = {name: log_loss(y_val, proba) for name, proba in val_probas.items()}\n",
    "scores = {name: 1.0 / (loss + 1e-9) for name, loss in losses.items()}\n",
    "total_score = sum(scores.values())\n",
    "ensemble_weights = {name: score / total_score for name, score in scores.items()}\n",
    "\n",
    "print(\"\\n--- Pesos del Ensemble de Nivel 1 Calculados ---\")\n",
    "for name, w in sorted(ensemble_weights.items(), key=lambda item: item[1], reverse=True):\n",
    "    print(f\"{name:<30} | Peso: {w:.3f} | LogLoss (Val): {losses[name]:.4f}\")\n",
    "\n",
    "# Evaluar el rendimiento del ensemble en el set de validación\n",
    "ensemble_proba_val = np.zeros_like(val_probas['XGBoost_Embeddings'])\n",
    "for name, proba in val_probas.items():\n",
    "    ensemble_proba_val += proba * ensemble_weights[name]\n",
    "\n",
    "ensemble_log_loss_val = log_loss(y_val, ensemble_proba_val)\n",
    "print(f\"\\nLogLoss del Ensemble L1 en Validación: {ensemble_log_loss_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sub_classifier_md",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento del Clasificador de Sub-categorías (Nivel 2) con Optuna y Ensemble\n",
    "\n",
    "Ahora, aplicamos la misma metodología robusta al clasificador de Nivel 2. Este se entrenará **únicamente con los datos de 'odio' balanceados sintéticamente**. Crearemos un ensemble de tres modelos (XGBoost, MLP, Regresión Logística) usando **características combinadas de embeddings y TF-IDF**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sub_classifier_train",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Preparando datos y definiendo objetivos para el Clasificador de Sub-categorías (Nivel 2) ---\")\n",
    "\n",
    "if X_train_sub_emb.shape[0] > 0:\n",
    "    # 1. Preparar características TF-IDF para datos de Nivel 2\n",
    "    # Datos reales de 'odio'\n",
    "    real_hate_texts_train = df_train_hate['text_stemmed']\n",
    "    X_train_sub_tfidf_real = tfidf_vectorizer.transform(real_hate_texts_train)\n",
    "    # Datos sintéticos (vector de ceros, ya que no tienen texto)\n",
    "    num_synthetic = len(df_synthetic)\n",
    "    X_train_sub_tfidf_synthetic = csr_matrix((num_synthetic, X_train_sub_tfidf_real.shape[1]), dtype=np.float64)\n",
    "    # Combinar TF-IDF de datos reales y sintéticos\n",
    "    X_train_sub_tfidf = vstack([X_train_sub_tfidf_real, X_train_sub_tfidf_synthetic])\n",
    "\n",
    "    # 2. Combinar Embeddings y TF-IDF para Nivel 2\n",
    "    X_train_sub_combined = hstack([X_train_sub_emb, X_train_sub_tfidf]).tocsr()\n",
    "    num_sub_classes = len(np.unique(y_train_sub))\n",
    "    print(f\"Datos combinados para Nivel 2 listos. Shape: {X_train_sub_combined.shape}, {num_sub_classes} sub-clases detectadas.\")\n",
    "\n",
    "    # 3. Dividir los datos COMBINADOS para HPO\n",
    "    X_sub_train_comb, X_sub_val_comb, y_sub_train, y_sub_val = train_test_split(\n",
    "        X_train_sub_combined, y_train_sub, test_size=0.25, random_state=42, stratify=y_train_sub\n",
    "    )\n",
    "\n",
    "    # 4. Escalar la parte de embeddings de los datos combinados para MLP y LogReg\n",
    "    scaler_L2_emb = StandardScaler()\n",
    "    # Extraer y escalar la parte de embeddings\n",
    "    X_sub_train_emb_part = X_sub_train_comb[:, :X_train_sub_emb.shape[1]].toarray()\n",
    "    X_sub_train_emb_part_scaled = scaler_L2_emb.fit_transform(X_sub_train_emb_part)\n",
    "    X_sub_val_emb_part = X_sub_val_comb[:, :X_train_sub_emb.shape[1]].toarray()\n",
    "    X_sub_val_emb_part_scaled = scaler_L2_emb.transform(X_sub_val_emb_part)\n",
    "    # Re-combinar con la parte de TF-IDF (que no se escala)\n",
    "    X_sub_train_scaled_comb_dense = np.hstack([X_sub_train_emb_part_scaled, X_sub_train_comb[:, X_train_sub_emb.shape[1]:].toarray()])\n",
    "    X_sub_val_scaled_comb_dense = np.hstack([X_sub_val_emb_part_scaled, X_sub_val_comb[:, X_train_sub_emb.shape[1]:].toarray()])\n",
    "\n",
    "    # 5. Convertir a tensores de PyTorch\n",
    "    X_sub_val_torch = torch.tensor(X_sub_val_scaled_comb_dense, dtype=torch.float32).to(device)\n",
    "    y_sub_val_torch = torch.tensor(y_sub_val, dtype=torch.long).to(device)\n",
    "\n",
    "    # --- Funciones Objetivo para Optuna (Nivel 2, con datos combinados) ---\n",
    "    def objective_xgboost_L2(trial):\n",
    "        params = {'objective': 'multi:softprob', 'num_class': num_sub_classes, 'eval_metric': 'mlogloss', 'device': 'cuda',\n",
    "                  'n_estimators': trial.suggest_int('n_estimators', 100, 800), 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True), 'max_depth': trial.suggest_int('max_depth', 3, 8)}\n",
    "        model = xgb.XGBClassifier(**params, early_stopping_rounds=10)\n",
    "        model.fit(X_sub_train_comb, y_sub_train, eval_set=[(X_sub_val_comb, y_sub_val)], verbose=False)\n",
    "        return log_loss(y_sub_val, model.predict_proba(X_sub_val_comb))\n",
    "\n",
    "    def objective_logistic_L2(trial):\n",
    "        params = {'C': trial.suggest_float('C', 1e-3, 1e2, log=True), 'solver': 'liblinear', 'max_iter': 1000, 'multi_class': 'ovr'}\n",
    "        model = LogisticRegression(**params, random_state=42)\n",
    "        # Se entrena con los datos densos (escalados en parte)\n",
    "        model.fit(X_sub_train_scaled_comb_dense, y_sub_train)\n",
    "        return log_loss(y_sub_val, model.predict_proba(X_sub_val_scaled_comb_dense))\n",
    "\n",
    "    def objective_mlp_L2(trial):\n",
    "        n_layers = trial.suggest_int('n_layers', 1, 2)\n",
    "        hidden_layers = [trial.suggest_int(f'n_units_l{i}', 32, 128) for i in range(n_layers)]\n",
    "        lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "\n",
    "        model = MLP(X_sub_train_scaled_comb_dense.shape[1], hidden_layers, num_sub_classes, nn.ReLU, 0.3).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        train_dataset = TensorDataset(torch.tensor(X_sub_train_scaled_comb_dense, dtype=torch.float32), torch.tensor(y_sub_train, dtype=torch.long))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        for epoch in range(20):\n",
    "            for data, target in train_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(data), target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = criterion(model(X_sub_val_torch), y_sub_val_torch).item()\n",
    "        return val_loss\n",
    "    print(\"Funciones objetivo para Nivel 2 definidas.\")\n",
    "else:\n",
    "    print(\"No hay datos para preparar el Nivel 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train_sub_emb.shape[0] > 0:\n",
    "    # --- 1. Búsqueda de Hiperparámetros (HPO) para Nivel 2 ---\n",
    "    models_config_L2 = {\n",
    "        'XGBoost_L2': {'objective_func': objective_xgboost_L2, 'n_trials': 20},\n",
    "        'MLP_PyTorch_L2': {'objective_func': objective_mlp_L2, 'n_trials': 25},\n",
    "        'LogisticRegression_L2': {'objective_func': objective_logistic_L2, 'n_trials': 15}\n",
    "    }\n",
    "    model_results_L2 = {}\n",
    "    for model_name, config in models_config_L2.items():\n",
    "        print(f\"\\n--- Optimizando {model_name} (Nivel 2) ---\")\n",
    "        study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        study.optimize(config['objective_func'], n_trials=config['n_trials'], show_progress_bar=True)\n",
    "        model_results_L2[model_name] = {'best_params': study.best_params}\n",
    "\n",
    "    # --- 2. Entrenamiento de los modelos finales del ensemble de Nivel 2 ---\n",
    "    print(\"\\n--- Entrenando modelos finales del Ensemble (Nivel 2) ---\")\n",
    "    sub_classifier_models = {}\n",
    "\n",
    "    # Preparar datos completos de entrenamiento L2 (escalados y densos para MLP/LogReg)\n",
    "    X_train_sub_emb_part_full = X_train_sub_combined[:, :X_train_sub_emb.shape[1]].toarray()\n",
    "    X_train_sub_emb_part_full_scaled = scaler_L2_emb.transform(X_train_sub_emb_part_full)\n",
    "    X_train_sub_full_scaled_dense = np.hstack([X_train_sub_emb_part_full_scaled, X_train_sub_combined[:, X_train_sub_emb.shape[1]:].toarray()])\n",
    "\n",
    "    # XGBoost L2\n",
    "    params = model_results_L2['XGBoost_L2']['best_params']\n",
    "    final_xgb_L2 = xgb.XGBClassifier(objective='multi:softprob', num_class=num_sub_classes, eval_metric='mlogloss',\n",
    "                                     device='cuda' if device.type == 'cuda' else 'cpu', **params)\n",
    "    final_xgb_L2.fit(X_train_sub_combined, y_train_sub) # Entrenar con sparse\n",
    "    sub_classifier_models['XGBoost_L2'] = final_xgb_L2\n",
    "\n",
    "    # Logistic Regression L2\n",
    "    params = model_results_L2['LogisticRegression_L2']['best_params']\n",
    "    final_log_L2 = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000, **params)\n",
    "    final_log_L2.fit(X_train_sub_full_scaled_dense, y_train_sub) # Entrenar con denso escalado\n",
    "    sub_classifier_models['LogisticRegression_L2'] = final_log_L2\n",
    "\n",
    "    # MLP L2\n",
    "    params = model_results_L2['MLP_PyTorch_L2']['best_params']\n",
    "    hidden_layers = [params[f'n_units_l{i}'] for i in range(params['n_layers'])]\n",
    "    final_mlp_L2 = MLP(X_train_sub_full_scaled_dense.shape[1], hidden_layers, num_sub_classes, nn.ReLU, 0.3).to(device)\n",
    "    optimizer = optim.Adam(final_mlp_L2.parameters(), lr=params['lr'])\n",
    "    train_dataset_L2 = TensorDataset(torch.tensor(X_train_sub_full_scaled_dense, dtype=torch.float32), torch.tensor(y_train_sub, dtype=torch.long))\n",
    "    train_loader_L2 = DataLoader(train_dataset_L2, batch_size=64, shuffle=True)\n",
    "    for epoch in tqdm(range(30), desc=\"Epochs MLP L2 final\"):\n",
    "        for data, target in train_loader_L2:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = nn.CrossEntropyLoss()(final_mlp_L2(data), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    sub_classifier_models['MLP_PyTorch_L2'] = final_mlp_L2.eval()\n",
    "    print(\"✓ Todos los modelos del ensemble de Nivel 2 han sido entrenados.\")\n",
    "\n",
    "    # --- 3. Cálculo de pesos para el ensemble de Nivel 2 ---\n",
    "    print(\"\\n--- Calculando pesos para el Ensemble de Nivel 2 ---\")\n",
    "    val_probas_L2 = {}\n",
    "    val_probas_L2['XGBoost_L2'] = sub_classifier_models['XGBoost_L2'].predict_proba(X_sub_val_comb)\n",
    "    val_probas_L2['LogisticRegression_L2'] = sub_classifier_models['LogisticRegression_L2'].predict_proba(X_sub_val_scaled_comb_dense)\n",
    "    with torch.no_grad():\n",
    "        mlp_outputs = sub_classifier_models['MLP_PyTorch_L2'](X_sub_val_torch)\n",
    "        val_probas_L2['MLP_PyTorch_L2'] = torch.softmax(mlp_outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    losses_L2 = {name: log_loss(y_sub_val, proba, labels=np.unique(y_train_sub)) for name, proba in val_probas_L2.items()}\n",
    "    scores_L2 = {name: 1.0 / (loss + 1e-9) for name, loss in losses_L2.items()}\n",
    "    total_score_L2 = sum(scores_L2.values())\n",
    "    ensemble_weights_L2 = {name: score / total_score_L2 for name, score in scores_L2.items()}\n",
    "\n",
    "    print(\"\\n--- Pesos del Ensemble de Nivel 2 Calculados ---\")\n",
    "    for name, w in sorted(ensemble_weights_L2.items(), key=lambda item: item[1], reverse=True):\n",
    "        print(f\"{name:<25} | Peso: {w:.3f} | LogLoss (Val): {losses_L2[name]:.4f}\")\n",
    "else:\n",
    "    print(\"No hay datos para entrenar el clasificador de Nivel 2.\")\n",
    "    sub_classifier_models = None\n",
    "    ensemble_weights_L2 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_eval_md",
   "metadata": {},
   "source": [
    "## 7. Evaluación Final del Pipeline Jerárquico Robusto\n",
    "\n",
    "Evaluamos el pipeline completo. Primero, usamos el **ensemble ponderado de Nivel 1** para la predicción de \"Odio vs. No-Odio\". Luego, para las predicciones de \"odio\", usamos el **ensemble ponderado de Nivel 2** para predecir la sub-categoría, ambos usando las configuraciones de características correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_eval_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Evaluación del pipeline jerárquico en el conjunto de prueba ---\")\n",
    "\n",
    "# 1. Preparar todas las características de prueba\n",
    "X_test_emb_eval = df_test[embedding_cols].values\n",
    "X_test_emb_scaled_eval = scaler_L1_emb.transform(X_test_emb_eval)\n",
    "X_test_text_eval = df_test['text_stemmed'].values\n",
    "X_test_tfidf_eval = tfidf_vectorizer.transform(X_test_text_eval)\n",
    "X_test_combined_eval = hstack([X_test_emb_eval, X_test_tfidf_eval]).tocsr()\n",
    "X_test_combined_dense_eval = np.hstack([X_test_emb_scaled_eval, X_test_tfidf_eval.toarray()])\n",
    "X_test_torch_emb_eval = torch.tensor(X_test_emb_scaled_eval, dtype=torch.float32).to(device)\n",
    "X_test_torch_combined_eval = torch.tensor(X_test_combined_dense_eval, dtype=torch.float32).to(device)\n",
    "y_main_true = df_test['main_label'].values\n",
    "\n",
    "# 2. Obtener predicciones del ENSEMBLE de Nivel 1\n",
    "test_probas_L1 = {}\n",
    "test_probas_L1['XGBoost_Embeddings'] = main_classifier_models['XGBoost_Embeddings'].predict_proba(X_test_emb_eval)\n",
    "test_probas_L1['XGBoost_Combined'] = main_classifier_models['XGBoost_Combined'].predict_proba(X_test_combined_eval)\n",
    "test_probas_L1['LogisticRegression_Embeddings'] = main_classifier_models['LogisticRegression_Embeddings'].predict_proba(X_test_emb_scaled_eval)\n",
    "test_probas_L1['LogisticRegression_TFIDF'] = main_classifier_models['LogisticRegression_TFIDF'].predict_proba(X_test_tfidf_eval)\n",
    "with torch.no_grad():\n",
    "    test_probas_L1['MLP_PyTorch_Embeddings'] = torch.softmax(main_classifier_models['MLP_PyTorch_Embeddings'](X_test_torch_emb_eval), dim=1).cpu().numpy()\n",
    "    test_probas_L1['MLP_PyTorch_Combined'] = torch.softmax(main_classifier_models['MLP_PyTorch_Combined'](X_test_torch_combined_eval), dim=1).cpu().numpy()\n",
    "\n",
    "final_ensemble_proba_L1 = np.zeros_like(test_probas_L1['XGBoost_Embeddings'])\n",
    "for name, proba in test_probas_L1.items():\n",
    "    final_ensemble_proba_L1 += proba * ensemble_weights[name]\n",
    "y_main_pred = np.argmax(final_ensemble_proba_L1, axis=1)\n",
    "\n",
    "# 3. Evaluar Nivel 1\n",
    "print(\"\\n--- [Nivel 1] Rendimiento del Ensemble Principal (Prueba) ---\")\n",
    "print(classification_report(y_main_true, y_main_pred, target_names=['not-hate', 'hate']))\n",
    "\n",
    "# 4. Obtener y evaluar predicciones del ENSEMBLE de Nivel 2\n",
    "if sub_classifier_models is not None:\n",
    "    df_test_true_hate = df_test[df_test['main_label'] == 1].copy()\n",
    "    if not df_test_true_hate.empty:\n",
    "        y_sub_true = sub_hate_only_encoder.transform(df_test_true_hate['sub_label_str'])\n",
    "\n",
    "        # Preparar datos combinados para L2 en el conjunto de prueba\n",
    "        X_test_hate_emb = df_test_true_hate[embedding_cols].values\n",
    "        X_test_hate_tfidf = tfidf_vectorizer.transform(df_test_true_hate['text_stemmed'])\n",
    "        X_test_hate_combined = hstack([X_test_hate_emb, X_test_hate_tfidf]).tocsr()\n",
    "\n",
    "        # Preparar versión densa y escalada para MLP/LogReg\n",
    "        X_test_hate_emb_scaled = scaler_L2_emb.transform(X_test_hate_emb)\n",
    "        X_test_hate_combined_dense = np.hstack([X_test_hate_emb_scaled, X_test_hate_tfidf.toarray()])\n",
    "        X_test_hate_torch = torch.tensor(X_test_hate_combined_dense, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Obtener y combinar probabilidades L2\n",
    "        true_hate_probas_L2 = {}\n",
    "        true_hate_probas_L2['XGBoost_L2'] = sub_classifier_models['XGBoost_L2'].predict_proba(X_test_hate_combined)\n",
    "        true_hate_probas_L2['LogisticRegression_L2'] = sub_classifier_models['LogisticRegression_L2'].predict_proba(X_test_hate_combined_dense)\n",
    "        with torch.no_grad():\n",
    "            mlp_outputs_L2 = sub_classifier_models['MLP_PyTorch_L2'](X_test_hate_torch)\n",
    "            true_hate_probas_L2['MLP_PyTorch_L2'] = torch.softmax(mlp_outputs_L2, dim=1).cpu().numpy()\n",
    "\n",
    "        final_true_hate_proba_L2 = np.zeros_like(true_hate_probas_L2['XGBoost_L2'])\n",
    "        for name, proba in true_hate_probas_L2.items():\n",
    "            final_true_hate_proba_L2 += proba * ensemble_weights_L2[name]\n",
    "        y_sub_pred_for_eval = np.argmax(final_true_hate_proba_L2, axis=1)\n",
    "\n",
    "        print(\"\\n--- [Nivel 2] Rendimiento del Ensemble de Sub-categorías (Prueba) ---\")\n",
    "        print(classification_report(y_sub_true, y_sub_pred_for_eval, target_names=sub_hate_only_encoder.classes_, zero_division=0))\n",
    "else:\n",
    "    print(\"\\nEl clasificador de sub-categorías no fue entrenado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_artifacts_md",
   "metadata": {},
   "source": [
    "## 8. Guardado de Artefactos\n",
    "\n",
    "Guardamos todos los componentes del pipeline jerárquico: los modelos de ambos ensembles, sus respectivos pesos, transformadores y codificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_artifacts_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Guardando artefactos en {MODEL_OUTPUT_DIR} ---\")\n",
    "\n",
    "# 1. Guardar modelos y pesos del ensemble de Nivel 1\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, \"main_classifier_models_L1.pkl\"), 'wb') as f: pickle.dump(main_classifier_models, f)\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, \"ensemble_weights_L1.pkl\"), 'wb') as f: pickle.dump(ensemble_weights, f)\n",
    "print(\"✓ Modelos y pesos de Nivel 1 guardados.\")\n",
    "\n",
    "# 2. Guardar modelos y pesos del ensemble de Nivel 2\n",
    "if sub_classifier_models is not None:\n",
    "    with open(os.path.join(MODEL_OUTPUT_DIR, \"sub_classifier_models_L2.pkl\"), 'wb') as f: pickle.dump(sub_classifier_models, f)\n",
    "    with open(os.path.join(MODEL_OUTPUT_DIR, \"ensemble_weights_L2.pkl\"), 'wb') as f: pickle.dump(ensemble_weights_L2, f)\n",
    "    print(\"✓ Modelos y pesos de Nivel 2 guardados.\")\n",
    "\n",
    "# 3. Guardar transformadores y codificadores\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, \"scaler_L1_emb.pkl\"), 'wb') as f: pickle.dump(scaler_L1_emb, f)\n",
    "if 'scaler_L2_emb' in locals():\n",
    "    with open(os.path.join(MODEL_OUTPUT_DIR, \"scaler_L2_emb.pkl\"), 'wb') as f: pickle.dump(scaler_L2_emb, f)\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, \"tfidf_vectorizer.pkl\"), 'wb') as f: pickle.dump(tfidf_vectorizer, f)\n",
    "if 'sub_hate_only_encoder' in locals():\n",
    "    with open(os.path.join(MODEL_OUTPUT_DIR, \"sub_hate_only_encoder.pkl\"), 'wb') as f: pickle.dump(sub_hate_only_encoder, f)\n",
    "print(\"✓ Scalers, TF-IDF Vectorizer y codificador de sub-etiquetas guardados.\")\n",
    "\n",
    "# 4. Guardar resultados de Optuna\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, \"optuna_results.pkl\"), 'wb') as f:\n",
    "    pickle.dump({'L1': model_results, 'L2': model_results_L2 if 'model_results_L2' in locals() else {}}, f)\n",
    "print(\"✓ Resultados de Optuna guardados.\")\n",
    "\n",
    "print(\"\\n🎉 Pipeline jerárquico robusto completado y todos los artefactos han sido guardados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
